{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle\n",
    "## Competição DSA de Machine Learning - Dezembro 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versão 1.0.0: LB = 0.48866 CV = 0.463102\n",
    "- modelo: LightGBM (com algumas otimizações)\n",
    "- features engineering: gerado através do Auto_ViML\n",
    "\n",
    "Versão 1.0.1: LB = 0.48991 CV = 0.462946\n",
    "- modelo: LightGBM (com algumas otimizações)\n",
    "- features engineering: gerado através do Auto_ViML (com novas features)\n",
    "\n",
    "Versão 1.0.2: LB = 0.48915 CV = 0.464442\n",
    "- modelo: LightGBM (com algumas otimizações)\n",
    "- features engineering: gerado através do Auto_ViML (com agrupamento pela coluna v2)\n",
    "\n",
    "Versão 1.0.3: LB = ???? CV = \n",
    "- modelo: LightGBM (com algumas otimizações)\n",
    "- features engineering: gerado através do Auto_ViML (com agrupamento pela coluna v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar os principais pacotes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from scipy.stats import mstats\n",
    "\n",
    "# Evitar que aparece os warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Seta algumas opções no Jupyter para exibição dos datasets\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# Variavel para controlar o treinamento no Kaggle\n",
    "TRAIN_OFFLINE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importa os pacotes de algoritmos\n",
    "import lightgbm as lgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "# Importa pacotes do sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando os dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao de leitura dos dados\n",
    "def read_data():\n",
    "    \n",
    "    if TRAIN_OFFLINE:\n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        train = pd.read_csv('../dataset/dataset_treino.csv')\n",
    "        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n",
    "        \n",
    "        print('Carregando arquivo dataset_teste.csv....')\n",
    "        test = pd.read_csv('../dataset/dataset_teste.csv')\n",
    "        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n",
    "        \n",
    "    else:\n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        train = pd.read_csv('/kaggle/input/competicao-dsa-machine-learning-dec-2019/dataset_treino.csv')\n",
    "        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n",
    "        \n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        test = pd.read_csv('/kaggle/input/competicao-dsa-machine-learning-dec-2019/dataset_teste.csv')\n",
    "        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando arquivo dataset_treino.csv....\n",
      "dataset_treino.csv tem 114321 linhas and 133 colunas\n",
      "Carregando arquivo dataset_teste.csv....\n",
      "dataset_teste.csv tem 114393 linhas and 132 colunas\n"
     ]
    }
   ],
   "source": [
    "# Leitura dos dados\n",
    "train, test = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228714, 133)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Juntando os dois dataset (treino e teste)\n",
    "df = train.append(test)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Numerical features:  114\n",
      "Number of Categorical features:  19\n"
     ]
    }
   ],
   "source": [
    "# Verificar a quantidade de features numericas e categoricas\n",
    "\n",
    "numerical_feats = df.dtypes[df.dtypes != \"object\"].index\n",
    "print(\"Number of Numerical features: \", len(numerical_feats))\n",
    "\n",
    "categorical_feats = df.dtypes[df.dtypes == \"object\"].index\n",
    "print(\"Number of Categorical features: \", len(categorical_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratando os dados outliers com Winsorize\n",
    "for col in df.columns:\n",
    "    if df[col].dtype !='object':\n",
    "        if col in ['ID','target']:\n",
    "            continue\n",
    "        df[col] = mstats.winsorize(df[col], limits=[0.05, 0.05])[0]\n",
    "        df[col].fillna(df[col].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratando os dados outliers com Winsorize e missing com media (numerico) e XXX (categorica)\n",
    "for col in df.columns:\n",
    "    if df[col].dtype !='object':\n",
    "        if col in ['ID','target']:\n",
    "            continue\n",
    "        df[col] = mstats.winsorize(df[col], limits=[0.05, 0.05])[0]\n",
    "        df[col].fillna(df[col].mean(),inplace=True)\n",
    "    else:\n",
    "        df[col].fillna('XXX',inplace=True)\n",
    "        \n",
    "# Função para criação de novas features, agrupando por colunas\n",
    "'''def ft(data):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    numerical_cols = data.dtypes[data.dtypes != \"object\"].index\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        if col in ['ID','target','v2']:\n",
    "            continue\n",
    "        \n",
    "        df[col + '_mean']   = data.groupby(['v2'])[col].mean()\n",
    "        df[col + '_median'] = data.groupby(['v2'])[col].median()\n",
    "        df[col + '_max']    = data.groupby(['v2'])[col].max()\n",
    "        df[col + '_min']    = data.groupby(['v2'])[col].min()\n",
    "        df[col + '_std'] = data.groupby(['v2'])[col].std()\n",
    "\n",
    "    return df'''\n",
    "    \n",
    "#new_df = ft(df)\n",
    "#new_df = new_df.reset_index()\n",
    "#new_df = pd.merge(df, new_df, on='v2',how='left')\n",
    "new_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratando as features categoricas com LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in categorical_feats:\n",
    "    le.fit(np.unique(list(new_df[col].values)))\n",
    "    new_df[col] = le.transform(new_df[col])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = new_df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228714, 133)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop features \n",
    "#new_df = new_df.drop(new_df[to_drop], axis=1)\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>v1</th>\n",
       "      <th>v10</th>\n",
       "      <th>v100</th>\n",
       "      <th>v101</th>\n",
       "      <th>v102</th>\n",
       "      <th>v103</th>\n",
       "      <th>v104</th>\n",
       "      <th>v105</th>\n",
       "      <th>v106</th>\n",
       "      <th>v107</th>\n",
       "      <th>v108</th>\n",
       "      <th>v109</th>\n",
       "      <th>v11</th>\n",
       "      <th>v110</th>\n",
       "      <th>v111</th>\n",
       "      <th>v112</th>\n",
       "      <th>v113</th>\n",
       "      <th>v114</th>\n",
       "      <th>v115</th>\n",
       "      <th>v116</th>\n",
       "      <th>v117</th>\n",
       "      <th>v118</th>\n",
       "      <th>v119</th>\n",
       "      <th>v12</th>\n",
       "      <th>v120</th>\n",
       "      <th>v121</th>\n",
       "      <th>v122</th>\n",
       "      <th>v123</th>\n",
       "      <th>v124</th>\n",
       "      <th>v125</th>\n",
       "      <th>v126</th>\n",
       "      <th>v127</th>\n",
       "      <th>v128</th>\n",
       "      <th>v129</th>\n",
       "      <th>v13</th>\n",
       "      <th>v130</th>\n",
       "      <th>v131</th>\n",
       "      <th>v14</th>\n",
       "      <th>v15</th>\n",
       "      <th>v16</th>\n",
       "      <th>v17</th>\n",
       "      <th>v18</th>\n",
       "      <th>v19</th>\n",
       "      <th>v2</th>\n",
       "      <th>v20</th>\n",
       "      <th>v21</th>\n",
       "      <th>v22</th>\n",
       "      <th>v23</th>\n",
       "      <th>v24</th>\n",
       "      <th>v25</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>v29</th>\n",
       "      <th>v3</th>\n",
       "      <th>v30</th>\n",
       "      <th>v31</th>\n",
       "      <th>v32</th>\n",
       "      <th>v33</th>\n",
       "      <th>v34</th>\n",
       "      <th>v35</th>\n",
       "      <th>v36</th>\n",
       "      <th>v37</th>\n",
       "      <th>v38</th>\n",
       "      <th>v39</th>\n",
       "      <th>v4</th>\n",
       "      <th>v40</th>\n",
       "      <th>v41</th>\n",
       "      <th>v42</th>\n",
       "      <th>v43</th>\n",
       "      <th>v44</th>\n",
       "      <th>v45</th>\n",
       "      <th>v46</th>\n",
       "      <th>v47</th>\n",
       "      <th>v48</th>\n",
       "      <th>v49</th>\n",
       "      <th>v5</th>\n",
       "      <th>v50</th>\n",
       "      <th>v51</th>\n",
       "      <th>v52</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>v55</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57</th>\n",
       "      <th>v58</th>\n",
       "      <th>v59</th>\n",
       "      <th>v6</th>\n",
       "      <th>v60</th>\n",
       "      <th>v61</th>\n",
       "      <th>v62</th>\n",
       "      <th>v63</th>\n",
       "      <th>v64</th>\n",
       "      <th>v65</th>\n",
       "      <th>v66</th>\n",
       "      <th>v67</th>\n",
       "      <th>v68</th>\n",
       "      <th>v69</th>\n",
       "      <th>v7</th>\n",
       "      <th>v70</th>\n",
       "      <th>v71</th>\n",
       "      <th>v72</th>\n",
       "      <th>v73</th>\n",
       "      <th>v74</th>\n",
       "      <th>v75</th>\n",
       "      <th>v76</th>\n",
       "      <th>v77</th>\n",
       "      <th>v78</th>\n",
       "      <th>v79</th>\n",
       "      <th>v8</th>\n",
       "      <th>v80</th>\n",
       "      <th>v81</th>\n",
       "      <th>v82</th>\n",
       "      <th>v83</th>\n",
       "      <th>v84</th>\n",
       "      <th>v85</th>\n",
       "      <th>v86</th>\n",
       "      <th>v87</th>\n",
       "      <th>v88</th>\n",
       "      <th>v89</th>\n",
       "      <th>v9</th>\n",
       "      <th>v90</th>\n",
       "      <th>v91</th>\n",
       "      <th>v92</th>\n",
       "      <th>v93</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "      <th>v96</th>\n",
       "      <th>v97</th>\n",
       "      <th>v98</th>\n",
       "      <th>v99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.335739</td>\n",
       "      <td>0.525165</td>\n",
       "      <td>19.470199</td>\n",
       "      <td>8.389237</td>\n",
       "      <td>2.757375</td>\n",
       "      <td>4.374296</td>\n",
       "      <td>1.781359</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>12.579184</td>\n",
       "      <td>4</td>\n",
       "      <td>2.382692</td>\n",
       "      <td>3.930922</td>\n",
       "      <td>16.434108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.692213</td>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>15.634907</td>\n",
       "      <td>8.292682</td>\n",
       "      <td>1.95122</td>\n",
       "      <td>6.592012</td>\n",
       "      <td>5.919002</td>\n",
       "      <td>2.395187e-07</td>\n",
       "      <td>6.085711</td>\n",
       "      <td>1.059603</td>\n",
       "      <td>1.294964</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.98978</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>21</td>\n",
       "      <td>1.804126</td>\n",
       "      <td>3.113719</td>\n",
       "      <td>2.024285</td>\n",
       "      <td>0</td>\n",
       "      <td>2.86683</td>\n",
       "      <td>0.79503</td>\n",
       "      <td>2.857144</td>\n",
       "      <td>11.636387</td>\n",
       "      <td>1.355013</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>3.67035</td>\n",
       "      <td>0.190826</td>\n",
       "      <td>0.154388</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>18.869283</td>\n",
       "      <td>7.730923</td>\n",
       "      <td>21416</td>\n",
       "      <td>-1.716131e-08</td>\n",
       "      <td>2</td>\n",
       "      <td>0.139412</td>\n",
       "      <td>1.720818</td>\n",
       "      <td>3.393503</td>\n",
       "      <td>1.212316</td>\n",
       "      <td>8.880867</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.083033</td>\n",
       "      <td>1.111398</td>\n",
       "      <td>7.270147</td>\n",
       "      <td>8.375452</td>\n",
       "      <td>11.326592</td>\n",
       "      <td>0.454546</td>\n",
       "      <td>0</td>\n",
       "      <td>4.012088</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.711453</td>\n",
       "      <td>7.653429</td>\n",
       "      <td>12.707581</td>\n",
       "      <td>2.015505</td>\n",
       "      <td>10.498338</td>\n",
       "      <td>9.848672</td>\n",
       "      <td>0.113561</td>\n",
       "      <td>2</td>\n",
       "      <td>12.171733</td>\n",
       "      <td>8.086643</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>0.89942</td>\n",
       "      <td>7.277792</td>\n",
       "      <td>6</td>\n",
       "      <td>16.747968</td>\n",
       "      <td>0.04973</td>\n",
       "      <td>1.299638</td>\n",
       "      <td>90</td>\n",
       "      <td>3.971118</td>\n",
       "      <td>0.529802</td>\n",
       "      <td>10.890984</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>1.588448</td>\n",
       "      <td>15.858152</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153461</td>\n",
       "      <td>6.363189</td>\n",
       "      <td>18.303925</td>\n",
       "      <td>2</td>\n",
       "      <td>9.314079</td>\n",
       "      <td>15.231789</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>11.784549</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.614988</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.23094</td>\n",
       "      <td>7.292418</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024422</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.528326</td>\n",
       "      <td>8.861647</td>\n",
       "      <td>1.049409</td>\n",
       "      <td>1.299638</td>\n",
       "      <td>1.707317</td>\n",
       "      <td>0.866426</td>\n",
       "      <td>9.551836</td>\n",
       "      <td>3.3213</td>\n",
       "      <td>0.095678</td>\n",
       "      <td>9.999999</td>\n",
       "      <td>0.905342</td>\n",
       "      <td>0</td>\n",
       "      <td>0.442252</td>\n",
       "      <td>5.814018</td>\n",
       "      <td>3.51772</td>\n",
       "      <td>0.462019</td>\n",
       "      <td>7.436824</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>8.877414</td>\n",
       "      <td>1.191337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.335739</td>\n",
       "      <td>0.525165</td>\n",
       "      <td>19.470199</td>\n",
       "      <td>8.389237</td>\n",
       "      <td>2.757375</td>\n",
       "      <td>4.374296</td>\n",
       "      <td>1.781359</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>12.579184</td>\n",
       "      <td>1</td>\n",
       "      <td>2.382692</td>\n",
       "      <td>3.930922</td>\n",
       "      <td>16.434108</td>\n",
       "      <td>0</td>\n",
       "      <td>1.692213</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>15.634907</td>\n",
       "      <td>8.292682</td>\n",
       "      <td>1.95122</td>\n",
       "      <td>6.592012</td>\n",
       "      <td>5.919002</td>\n",
       "      <td>2.395187e-07</td>\n",
       "      <td>6.085711</td>\n",
       "      <td>1.059603</td>\n",
       "      <td>1.294964</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.98978</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>6</td>\n",
       "      <td>1.804126</td>\n",
       "      <td>3.113719</td>\n",
       "      <td>2.024285</td>\n",
       "      <td>0</td>\n",
       "      <td>2.86683</td>\n",
       "      <td>0.79503</td>\n",
       "      <td>2.857144</td>\n",
       "      <td>11.636387</td>\n",
       "      <td>1.355013</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>3.67035</td>\n",
       "      <td>0.190826</td>\n",
       "      <td>0.154388</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>18.869283</td>\n",
       "      <td>7.730923</td>\n",
       "      <td>9923</td>\n",
       "      <td>-1.716131e-08</td>\n",
       "      <td>2</td>\n",
       "      <td>0.139412</td>\n",
       "      <td>1.720818</td>\n",
       "      <td>3.393503</td>\n",
       "      <td>1.212316</td>\n",
       "      <td>8.880867</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.083033</td>\n",
       "      <td>1.111398</td>\n",
       "      <td>7.270147</td>\n",
       "      <td>8.375452</td>\n",
       "      <td>11.326592</td>\n",
       "      <td>0.454546</td>\n",
       "      <td>0</td>\n",
       "      <td>4.012088</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.711453</td>\n",
       "      <td>7.653429</td>\n",
       "      <td>12.707581</td>\n",
       "      <td>2.015505</td>\n",
       "      <td>10.498338</td>\n",
       "      <td>9.848672</td>\n",
       "      <td>0.113561</td>\n",
       "      <td>4</td>\n",
       "      <td>12.171733</td>\n",
       "      <td>8.086643</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>0.89942</td>\n",
       "      <td>7.277792</td>\n",
       "      <td>6</td>\n",
       "      <td>16.747968</td>\n",
       "      <td>0.04973</td>\n",
       "      <td>1.299638</td>\n",
       "      <td>106</td>\n",
       "      <td>3.971118</td>\n",
       "      <td>0.529802</td>\n",
       "      <td>10.890984</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>1.588448</td>\n",
       "      <td>15.858152</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153461</td>\n",
       "      <td>6.363189</td>\n",
       "      <td>18.303925</td>\n",
       "      <td>0</td>\n",
       "      <td>9.314079</td>\n",
       "      <td>15.231789</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>11.784549</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.614988</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.23094</td>\n",
       "      <td>7.292418</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>3</td>\n",
       "      <td>0.024422</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.528326</td>\n",
       "      <td>8.861647</td>\n",
       "      <td>1.049409</td>\n",
       "      <td>1.299638</td>\n",
       "      <td>1.707317</td>\n",
       "      <td>0.866426</td>\n",
       "      <td>9.551836</td>\n",
       "      <td>3.3213</td>\n",
       "      <td>0.095678</td>\n",
       "      <td>9.999999</td>\n",
       "      <td>0.905342</td>\n",
       "      <td>1</td>\n",
       "      <td>0.442252</td>\n",
       "      <td>5.814018</td>\n",
       "      <td>3.51772</td>\n",
       "      <td>0.462019</td>\n",
       "      <td>7.436824</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>8.877414</td>\n",
       "      <td>1.191337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.335739</td>\n",
       "      <td>0.525165</td>\n",
       "      <td>19.470199</td>\n",
       "      <td>8.389237</td>\n",
       "      <td>2.757375</td>\n",
       "      <td>4.374296</td>\n",
       "      <td>1.781359</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>12.579184</td>\n",
       "      <td>2</td>\n",
       "      <td>2.382692</td>\n",
       "      <td>3.930922</td>\n",
       "      <td>16.434108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.692213</td>\n",
       "      <td>18</td>\n",
       "      <td>35</td>\n",
       "      <td>15.634907</td>\n",
       "      <td>8.292682</td>\n",
       "      <td>1.95122</td>\n",
       "      <td>6.592012</td>\n",
       "      <td>5.919002</td>\n",
       "      <td>2.395187e-07</td>\n",
       "      <td>6.085711</td>\n",
       "      <td>1.059603</td>\n",
       "      <td>1.294964</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.98978</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>5</td>\n",
       "      <td>1.804126</td>\n",
       "      <td>3.113719</td>\n",
       "      <td>2.024285</td>\n",
       "      <td>0</td>\n",
       "      <td>2.86683</td>\n",
       "      <td>0.79503</td>\n",
       "      <td>2.857144</td>\n",
       "      <td>11.636387</td>\n",
       "      <td>1.355013</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>3.67035</td>\n",
       "      <td>0.190826</td>\n",
       "      <td>0.154388</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>18.869283</td>\n",
       "      <td>7.730923</td>\n",
       "      <td>9090</td>\n",
       "      <td>-1.716131e-08</td>\n",
       "      <td>4</td>\n",
       "      <td>0.139412</td>\n",
       "      <td>1.720818</td>\n",
       "      <td>3.393503</td>\n",
       "      <td>1.212316</td>\n",
       "      <td>8.880867</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.083033</td>\n",
       "      <td>1.111398</td>\n",
       "      <td>7.270147</td>\n",
       "      <td>8.375452</td>\n",
       "      <td>11.326592</td>\n",
       "      <td>0.454546</td>\n",
       "      <td>0</td>\n",
       "      <td>4.012088</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.711453</td>\n",
       "      <td>7.653429</td>\n",
       "      <td>12.707581</td>\n",
       "      <td>2.015505</td>\n",
       "      <td>10.498338</td>\n",
       "      <td>9.848672</td>\n",
       "      <td>0.113561</td>\n",
       "      <td>2</td>\n",
       "      <td>12.171733</td>\n",
       "      <td>8.086643</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>0.89942</td>\n",
       "      <td>7.277792</td>\n",
       "      <td>5</td>\n",
       "      <td>16.747968</td>\n",
       "      <td>0.04973</td>\n",
       "      <td>1.299638</td>\n",
       "      <td>19</td>\n",
       "      <td>3.971118</td>\n",
       "      <td>0.529802</td>\n",
       "      <td>10.890984</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>1.588448</td>\n",
       "      <td>15.858152</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153461</td>\n",
       "      <td>6.363189</td>\n",
       "      <td>18.303925</td>\n",
       "      <td>0</td>\n",
       "      <td>9.314079</td>\n",
       "      <td>15.231789</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>11.784549</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.614988</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.23094</td>\n",
       "      <td>7.292418</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024422</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.528326</td>\n",
       "      <td>8.861647</td>\n",
       "      <td>1.049409</td>\n",
       "      <td>1.299638</td>\n",
       "      <td>1.707317</td>\n",
       "      <td>0.866426</td>\n",
       "      <td>9.551836</td>\n",
       "      <td>3.3213</td>\n",
       "      <td>0.095678</td>\n",
       "      <td>9.999999</td>\n",
       "      <td>0.905342</td>\n",
       "      <td>6</td>\n",
       "      <td>0.442252</td>\n",
       "      <td>5.814018</td>\n",
       "      <td>3.51772</td>\n",
       "      <td>0.462019</td>\n",
       "      <td>7.436824</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>8.877414</td>\n",
       "      <td>1.191337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.335739</td>\n",
       "      <td>0.525165</td>\n",
       "      <td>19.470199</td>\n",
       "      <td>8.389237</td>\n",
       "      <td>2.757375</td>\n",
       "      <td>4.374296</td>\n",
       "      <td>1.781359</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>12.579184</td>\n",
       "      <td>1</td>\n",
       "      <td>2.382692</td>\n",
       "      <td>3.930922</td>\n",
       "      <td>16.434108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.692213</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>15.634907</td>\n",
       "      <td>8.292682</td>\n",
       "      <td>1.95122</td>\n",
       "      <td>6.592012</td>\n",
       "      <td>5.919002</td>\n",
       "      <td>2.395187e-07</td>\n",
       "      <td>6.085711</td>\n",
       "      <td>1.059603</td>\n",
       "      <td>1.294964</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.98978</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>64</td>\n",
       "      <td>1.804126</td>\n",
       "      <td>3.113719</td>\n",
       "      <td>2.024285</td>\n",
       "      <td>0</td>\n",
       "      <td>2.86683</td>\n",
       "      <td>0.79503</td>\n",
       "      <td>2.857144</td>\n",
       "      <td>11.636387</td>\n",
       "      <td>1.355013</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>3.67035</td>\n",
       "      <td>0.190826</td>\n",
       "      <td>0.154388</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>18.869283</td>\n",
       "      <td>7.730923</td>\n",
       "      <td>1953</td>\n",
       "      <td>-1.716131e-08</td>\n",
       "      <td>3</td>\n",
       "      <td>0.139412</td>\n",
       "      <td>1.720818</td>\n",
       "      <td>3.393503</td>\n",
       "      <td>1.212316</td>\n",
       "      <td>8.880867</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.083033</td>\n",
       "      <td>1.111398</td>\n",
       "      <td>7.270147</td>\n",
       "      <td>8.375452</td>\n",
       "      <td>11.326592</td>\n",
       "      <td>0.454546</td>\n",
       "      <td>0</td>\n",
       "      <td>4.012088</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.711453</td>\n",
       "      <td>7.653429</td>\n",
       "      <td>12.707581</td>\n",
       "      <td>2.015505</td>\n",
       "      <td>10.498338</td>\n",
       "      <td>9.848672</td>\n",
       "      <td>0.113561</td>\n",
       "      <td>2</td>\n",
       "      <td>12.171733</td>\n",
       "      <td>8.086643</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>0.89942</td>\n",
       "      <td>7.277792</td>\n",
       "      <td>7</td>\n",
       "      <td>16.747968</td>\n",
       "      <td>0.04973</td>\n",
       "      <td>1.299638</td>\n",
       "      <td>50</td>\n",
       "      <td>3.971118</td>\n",
       "      <td>0.529802</td>\n",
       "      <td>10.890984</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>1.588448</td>\n",
       "      <td>15.858152</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153461</td>\n",
       "      <td>6.363189</td>\n",
       "      <td>18.303925</td>\n",
       "      <td>0</td>\n",
       "      <td>9.314079</td>\n",
       "      <td>15.231789</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>11.784549</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.614988</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.23094</td>\n",
       "      <td>7.292418</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024422</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.528326</td>\n",
       "      <td>8.861647</td>\n",
       "      <td>1.049409</td>\n",
       "      <td>1.299638</td>\n",
       "      <td>1.707317</td>\n",
       "      <td>0.866426</td>\n",
       "      <td>9.551836</td>\n",
       "      <td>3.3213</td>\n",
       "      <td>0.095678</td>\n",
       "      <td>9.999999</td>\n",
       "      <td>0.905342</td>\n",
       "      <td>1</td>\n",
       "      <td>0.442252</td>\n",
       "      <td>5.814018</td>\n",
       "      <td>3.51772</td>\n",
       "      <td>0.462019</td>\n",
       "      <td>7.436824</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>8.877414</td>\n",
       "      <td>1.191337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.335739</td>\n",
       "      <td>0.525165</td>\n",
       "      <td>19.470199</td>\n",
       "      <td>8.389237</td>\n",
       "      <td>2.757375</td>\n",
       "      <td>4.374296</td>\n",
       "      <td>1.781359</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>12.579184</td>\n",
       "      <td>2</td>\n",
       "      <td>2.382692</td>\n",
       "      <td>3.930922</td>\n",
       "      <td>16.434108</td>\n",
       "      <td>0</td>\n",
       "      <td>1.692213</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>15.634907</td>\n",
       "      <td>8.292682</td>\n",
       "      <td>1.95122</td>\n",
       "      <td>6.592012</td>\n",
       "      <td>5.919002</td>\n",
       "      <td>2.395187e-07</td>\n",
       "      <td>6.085711</td>\n",
       "      <td>1.059603</td>\n",
       "      <td>1.294964</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.98978</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>90</td>\n",
       "      <td>1.804126</td>\n",
       "      <td>3.113719</td>\n",
       "      <td>2.024285</td>\n",
       "      <td>0</td>\n",
       "      <td>2.86683</td>\n",
       "      <td>0.79503</td>\n",
       "      <td>2.857144</td>\n",
       "      <td>11.636387</td>\n",
       "      <td>1.355013</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>3.67035</td>\n",
       "      <td>0.190826</td>\n",
       "      <td>0.154388</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>18.869283</td>\n",
       "      <td>7.730923</td>\n",
       "      <td>10300</td>\n",
       "      <td>-1.716131e-08</td>\n",
       "      <td>4</td>\n",
       "      <td>0.139412</td>\n",
       "      <td>1.720818</td>\n",
       "      <td>3.393503</td>\n",
       "      <td>1.212316</td>\n",
       "      <td>8.880867</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.083033</td>\n",
       "      <td>1.111398</td>\n",
       "      <td>7.270147</td>\n",
       "      <td>8.375452</td>\n",
       "      <td>11.326592</td>\n",
       "      <td>0.454546</td>\n",
       "      <td>0</td>\n",
       "      <td>4.012088</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.711453</td>\n",
       "      <td>7.653429</td>\n",
       "      <td>12.707581</td>\n",
       "      <td>2.015505</td>\n",
       "      <td>10.498338</td>\n",
       "      <td>9.848672</td>\n",
       "      <td>0.113561</td>\n",
       "      <td>8</td>\n",
       "      <td>12.171733</td>\n",
       "      <td>8.086643</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>0.89942</td>\n",
       "      <td>7.277792</td>\n",
       "      <td>7</td>\n",
       "      <td>16.747968</td>\n",
       "      <td>0.04973</td>\n",
       "      <td>1.299638</td>\n",
       "      <td>128</td>\n",
       "      <td>3.971118</td>\n",
       "      <td>0.529802</td>\n",
       "      <td>10.890984</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>1.588448</td>\n",
       "      <td>15.858152</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153461</td>\n",
       "      <td>6.363189</td>\n",
       "      <td>18.303925</td>\n",
       "      <td>2</td>\n",
       "      <td>9.314079</td>\n",
       "      <td>15.231789</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>11.784549</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.614988</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.23094</td>\n",
       "      <td>7.292418</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>2</td>\n",
       "      <td>0.024422</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.528326</td>\n",
       "      <td>8.861647</td>\n",
       "      <td>1.049409</td>\n",
       "      <td>1.299638</td>\n",
       "      <td>1.707317</td>\n",
       "      <td>0.866426</td>\n",
       "      <td>9.551836</td>\n",
       "      <td>3.3213</td>\n",
       "      <td>0.095678</td>\n",
       "      <td>9.999999</td>\n",
       "      <td>0.905342</td>\n",
       "      <td>6</td>\n",
       "      <td>0.442252</td>\n",
       "      <td>5.814018</td>\n",
       "      <td>3.51772</td>\n",
       "      <td>0.462019</td>\n",
       "      <td>7.436824</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>8.877414</td>\n",
       "      <td>1.191337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  target        v1       v10       v100      v101      v102      v103  \\\n",
       "0   3     1.0  1.335739  0.525165  19.470199  8.389237  2.757375  4.374296   \n",
       "1   4     1.0  1.335739  0.525165  19.470199  8.389237  2.757375  4.374296   \n",
       "2   5     1.0  1.335739  0.525165  19.470199  8.389237  2.757375  4.374296   \n",
       "3   6     1.0  1.335739  0.525165  19.470199  8.389237  2.757375  4.374296   \n",
       "4   8     1.0  1.335739  0.525165  19.470199  8.389237  2.757375  4.374296   \n",
       "\n",
       "       v104      v105       v106  v107      v108      v109        v11  v110  \\\n",
       "0  1.781359  0.014526  12.579184     4  2.382692  3.930922  16.434108     1   \n",
       "1  1.781359  0.014526  12.579184     1  2.382692  3.930922  16.434108     0   \n",
       "2  1.781359  0.014526  12.579184     2  2.382692  3.930922  16.434108     1   \n",
       "3  1.781359  0.014526  12.579184     1  2.382692  3.930922  16.434108     1   \n",
       "4  1.781359  0.014526  12.579184     2  2.382692  3.930922  16.434108     0   \n",
       "\n",
       "       v111  v112  v113       v114      v115     v116      v117      v118  \\\n",
       "0  1.692213    14    35  15.634907  8.292682  1.95122  6.592012  5.919002   \n",
       "1  1.692213    20    17  15.634907  8.292682  1.95122  6.592012  5.919002   \n",
       "2  1.692213    18    35  15.634907  8.292682  1.95122  6.592012  5.919002   \n",
       "3  1.692213     9    35  15.634907  8.292682  1.95122  6.592012  5.919002   \n",
       "4  1.692213    19    17  15.634907  8.292682  1.95122  6.592012  5.919002   \n",
       "\n",
       "           v119       v12      v120      v121  v122     v123      v124  v125  \\\n",
       "0  2.395187e-07  6.085711  1.059603  1.294964   8.0  1.98978  0.035754    21   \n",
       "1  2.395187e-07  6.085711  1.059603  1.294964   8.0  1.98978  0.035754     6   \n",
       "2  2.395187e-07  6.085711  1.059603  1.294964   8.0  1.98978  0.035754     5   \n",
       "3  2.395187e-07  6.085711  1.059603  1.294964   8.0  1.98978  0.035754    64   \n",
       "4  2.395187e-07  6.085711  1.059603  1.294964   8.0  1.98978  0.035754    90   \n",
       "\n",
       "       v126      v127      v128  v129      v13     v130      v131        v14  \\\n",
       "0  1.804126  3.113719  2.024285     0  2.86683  0.79503  2.857144  11.636387   \n",
       "1  1.804126  3.113719  2.024285     0  2.86683  0.79503  2.857144  11.636387   \n",
       "2  1.804126  3.113719  2.024285     0  2.86683  0.79503  2.857144  11.636387   \n",
       "3  1.804126  3.113719  2.024285     0  2.86683  0.79503  2.857144  11.636387   \n",
       "4  1.804126  3.113719  2.024285     0  2.86683  0.79503  2.857144  11.636387   \n",
       "\n",
       "        v15       v16      v17       v18       v19        v2        v20  \\\n",
       "0  1.355013  8.571429  3.67035  0.190826  0.154388  8.727474  18.869283   \n",
       "1  1.355013  8.571429  3.67035  0.190826  0.154388  8.727474  18.869283   \n",
       "2  1.355013  8.571429  3.67035  0.190826  0.154388  8.727474  18.869283   \n",
       "3  1.355013  8.571429  3.67035  0.190826  0.154388  8.727474  18.869283   \n",
       "4  1.355013  8.571429  3.67035  0.190826  0.154388  8.727474  18.869283   \n",
       "\n",
       "        v21    v22           v23  v24       v25       v26       v27       v28  \\\n",
       "0  7.730923  21416 -1.716131e-08    2  0.139412  1.720818  3.393503  1.212316   \n",
       "1  7.730923   9923 -1.716131e-08    2  0.139412  1.720818  3.393503  1.212316   \n",
       "2  7.730923   9090 -1.716131e-08    4  0.139412  1.720818  3.393503  1.212316   \n",
       "3  7.730923   1953 -1.716131e-08    3  0.139412  1.720818  3.393503  1.212316   \n",
       "4  7.730923  10300 -1.716131e-08    4  0.139412  1.720818  3.393503  1.212316   \n",
       "\n",
       "        v29  v3  v30  v31       v32       v33       v34       v35        v36  \\\n",
       "0  8.880867   2    2    0  1.083033  1.111398  7.270147  8.375452  11.326592   \n",
       "1  8.880867   2    2    0  1.083033  1.111398  7.270147  8.375452  11.326592   \n",
       "2  8.880867   2    7    0  1.083033  1.111398  7.270147  8.375452  11.326592   \n",
       "3  8.880867   2    2    1  1.083033  1.111398  7.270147  8.375452  11.326592   \n",
       "4  8.880867   2    7    0  1.083033  1.111398  7.270147  8.375452  11.326592   \n",
       "\n",
       "        v37  v38       v39        v4       v40       v41        v42       v43  \\\n",
       "0  0.454546    0  4.012088  3.921026  7.711453  7.653429  12.707581  2.015505   \n",
       "1  0.454546    0  4.012088  3.921026  7.711453  7.653429  12.707581  2.015505   \n",
       "2  0.454546    0  4.012088  3.921026  7.711453  7.653429  12.707581  2.015505   \n",
       "3  0.454546    0  4.012088  3.921026  7.711453  7.653429  12.707581  2.015505   \n",
       "4  0.454546    0  4.012088  3.921026  7.711453  7.653429  12.707581  2.015505   \n",
       "\n",
       "         v44       v45       v46  v47        v48       v49        v5      v50  \\\n",
       "0  10.498338  9.848672  0.113561    2  12.171733  8.086643  7.915266  0.89942   \n",
       "1  10.498338  9.848672  0.113561    4  12.171733  8.086643  7.915266  0.89942   \n",
       "2  10.498338  9.848672  0.113561    2  12.171733  8.086643  7.915266  0.89942   \n",
       "3  10.498338  9.848672  0.113561    2  12.171733  8.086643  7.915266  0.89942   \n",
       "4  10.498338  9.848672  0.113561    8  12.171733  8.086643  7.915266  0.89942   \n",
       "\n",
       "        v51  v52        v53      v54       v55  v56       v57       v58  \\\n",
       "0  7.277792    6  16.747968  0.04973  1.299638   90  3.971118  0.529802   \n",
       "1  7.277792    6  16.747968  0.04973  1.299638  106  3.971118  0.529802   \n",
       "2  7.277792    5  16.747968  0.04973  1.299638   19  3.971118  0.529802   \n",
       "3  7.277792    7  16.747968  0.04973  1.299638   50  3.971118  0.529802   \n",
       "4  7.277792    7  16.747968  0.04973  1.299638  128  3.971118  0.529802   \n",
       "\n",
       "         v59        v6       v60        v61  v62       v63       v64  \\\n",
       "0  10.890984  2.599278  1.588448  15.858152    1  0.153461  6.363189   \n",
       "1  10.890984  2.599278  1.588448  15.858152    1  0.153461  6.363189   \n",
       "2  10.890984  2.599278  1.588448  15.858152    1  0.153461  6.363189   \n",
       "3  10.890984  2.599278  1.588448  15.858152    1  0.153461  6.363189   \n",
       "4  10.890984  2.599278  1.588448  15.858152    1  0.153461  6.363189   \n",
       "\n",
       "         v65  v66       v67        v68        v69        v7        v70  v71  \\\n",
       "0  18.303925    2  9.314079  15.231789  17.142857  3.176895  11.784549    5   \n",
       "1  18.303925    0  9.314079  15.231789  17.142857  3.176895  11.784549    5   \n",
       "2  18.303925    0  9.314079  15.231789  17.142857  3.176895  11.784549    1   \n",
       "3  18.303925    0  9.314079  15.231789  17.142857  3.176895  11.784549    5   \n",
       "4  18.303925    2  9.314079  15.231789  17.142857  3.176895  11.784549    5   \n",
       "\n",
       "   v72       v73  v74  v75      v76       v77        v78  v79        v8  v80  \\\n",
       "0    1  1.614988    1    3  2.23094  7.292418  11.111111    4  0.024422  3.0   \n",
       "1    1  1.614988    1    3  2.23094  7.292418  11.111111    3  0.024422  3.0   \n",
       "2    1  1.614988    1    1  2.23094  7.292418  11.111111    4  0.024422  3.0   \n",
       "3    1  1.614988    1    3  2.23094  7.292418  11.111111    1  0.024422  3.0   \n",
       "4    1  1.614988    1    3  2.23094  7.292418  11.111111    2  0.024422  3.0   \n",
       "\n",
       "        v81       v82       v83       v84       v85       v86       v87  \\\n",
       "0  7.528326  8.861647  1.049409  1.299638  1.707317  0.866426  9.551836   \n",
       "1  7.528326  8.861647  1.049409  1.299638  1.707317  0.866426  9.551836   \n",
       "2  7.528326  8.861647  1.049409  1.299638  1.707317  0.866426  9.551836   \n",
       "3  7.528326  8.861647  1.049409  1.299638  1.707317  0.866426  9.551836   \n",
       "4  7.528326  8.861647  1.049409  1.299638  1.707317  0.866426  9.551836   \n",
       "\n",
       "      v88       v89        v9       v90  v91       v92       v93      v94  \\\n",
       "0  3.3213  0.095678  9.999999  0.905342    0  0.442252  5.814018  3.51772   \n",
       "1  3.3213  0.095678  9.999999  0.905342    1  0.442252  5.814018  3.51772   \n",
       "2  3.3213  0.095678  9.999999  0.905342    6  0.442252  5.814018  3.51772   \n",
       "3  3.3213  0.095678  9.999999  0.905342    1  0.442252  5.814018  3.51772   \n",
       "4  3.3213  0.095678  9.999999  0.905342    6  0.442252  5.814018  3.51772   \n",
       "\n",
       "        v95       v96       v97       v98       v99  \n",
       "0  0.462019  7.436824  5.454545  8.877414  1.191337  \n",
       "1  0.462019  7.436824  5.454545  8.877414  1.191337  \n",
       "2  0.462019  7.436824  5.454545  8.877414  1.191337  \n",
       "3  0.462019  7.436824  5.454545  8.877414  1.191337  \n",
       "4  0.462019  7.436824  5.454545  8.877414  1.191337  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo LigthGBM com Hyperparametros"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Buscando os melhores parametros\n",
    "# Utilizei o Hyperopt para otimizacao e o metodo Bayesian Optimization Primer\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "with open('trials.json') as file_data:\n",
    "    trial_json = json.load(file_data)\n",
    "\n",
    "data = json_normalize(trial_json)\n",
    "data.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações Gerais\n",
    "\n",
    "GENERATE_SUBMISSION_FILES = True\n",
    "SUBMISSION_SUFIX = \"_lgbm_v.1.0.3\"\n",
    "STRATIFIED_KFOLD = False\n",
    "RANDOM_SEED = np.random.seed(123)\n",
    "NUM_THREADS = 4\n",
    "NUM_FOLDS = 10\n",
    "EARLY_STOPPING = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando os melhores parametros\n",
    "LIGHTGBM_PARAMS = {'boosting_type': 'gbdt', \n",
    "                  'colsample_bytree': 0.881783, \n",
    "                  'is_unbalance': True, \n",
    "                  'learning_rate': 0.0129388, \n",
    "                  'min_child_samples': 315, \n",
    "                  'num_leaves': 139, \n",
    "                  'reg_alpha': 0.484807, \n",
    "                  'reg_lambda': 0.515065, \n",
    "                  'subsample_for_bin': 280000, \n",
    "                  'subsample': 0.635119, \n",
    "                  'n_estimators': 10000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- LIGHTGBM MODEL -------------------------\n",
    "# Funcao para processar todo o pipeline do treinamento e gerar a submissao\n",
    "def run_model(dataset, categorical_feature = None):\n",
    "    \n",
    "    # Separar o dataset de treino e teste\n",
    "    treino = dataset[dataset['target'].notnull()]\n",
    "    teste  = dataset[dataset['target'].isnull()]\n",
    "    \n",
    "    # Separando features preditoras e target\n",
    "    X_ = treino.drop(['ID','target'], axis=1)\n",
    "    y_ = treino['target']\n",
    "    \n",
    "    # Aplicando a funcao SMOTE\n",
    "    # SMOTE eh um metodo de oversampling. Ele cria exemplos sinteticos da classe minoritaria ao inves de criar copias\n",
    "    sm = SMOTE(random_state=0)\n",
    "    X, y = sm.fit_sample(X_, y_)\n",
    "\n",
    "    #X = X_.copy()\n",
    "    #y = y_.copy()\n",
    "    \n",
    "    # Padronizando os dados de treino\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "\n",
    "    #del_features = ['ID','target']\n",
    "    predictors = list(X_.columns)\n",
    "    \n",
    "    print(\"Train/valid shape: {}, test shape: {}\".format(X.shape, y.shape))\n",
    "\n",
    "    # Defini o tipo de Cross-Validation\n",
    "    if not STRATIFIED_KFOLD:\n",
    "        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n",
    "    else:\n",
    "        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n",
    "\n",
    "    # Hold oof predictions, test predictions, feature importance and training/valid auc\n",
    "    oof_preds = np.zeros(X.shape[0])\n",
    "    sub_preds = np.zeros(teste.shape[0])\n",
    "    importance_df = pd.DataFrame()\n",
    "    eval_results = dict()\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "        train_x, train_y = X[train_idx], y[train_idx]\n",
    "        valid_x, valid_y = X[valid_idx], y[valid_idx]\n",
    "\n",
    "        params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n",
    "        \n",
    "        clf = LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n",
    "        \n",
    "        if not categorical_feature:\n",
    "            clf.fit(train_x, train_y, \n",
    "                    eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                    eval_metric='logloss', \n",
    "                    verbose=100, \n",
    "                    early_stopping_rounds= EARLY_STOPPING)\n",
    "        else:\n",
    "            clf.fit(train_x, train_y, \n",
    "                    eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                    eval_metric='logloss', \n",
    "                    verbose=100, \n",
    "                    early_stopping_rounds=EARLY_STOPPING,\n",
    "                    feature_name= list(X_[predictors].columns), \n",
    "                    categorical_feature= categorical_feature)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(teste.drop(['ID','target'], axis=1), num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        # Feature importance by GAIN and SPLIT\n",
    "        fold_importance = pd.DataFrame()\n",
    "        fold_importance[\"feature\"] = predictors\n",
    "        fold_importance[\"gain\"] = clf.booster_.feature_importance(importance_type='gain')\n",
    "        fold_importance[\"split\"] = clf.booster_.feature_importance(importance_type='split')\n",
    "        importance_df = pd.concat([importance_df, fold_importance], axis=0)\n",
    "        eval_results['train_{}'.format(n_fold+1)]  = clf.evals_result_['training']['binary_logloss']\n",
    "        eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['binary_logloss']\n",
    "\n",
    "        print('Fold %2d Log Loss : %.6f' % (n_fold + 1, log_loss(valid_y, oof_preds[valid_idx])))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    print('Full Log Loss score %.6f' % log_loss(y, oof_preds))\n",
    "    teste['target'] = sub_preds.copy()\n",
    "\n",
    "    # Get the average feature importance between folds\n",
    "    mean_importance = importance_df.groupby('feature').mean().reset_index()\n",
    "    mean_importance.sort_values(by= 'gain', ascending=False, inplace=True)\n",
    "    \n",
    "    # Save feature importance, test predictions and oof predictions as csv\n",
    "    if GENERATE_SUBMISSION_FILES:\n",
    "\n",
    "        # Save submission (test data) and feature importance\n",
    "        submission = pd.read_csv('../dataset/sample_submission.csv')\n",
    "        submission['PredictedProb'] = sub_preds.copy()\n",
    "        submission.to_csv('../submission/submission{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
    "        \n",
    "        mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
    "        plt.hist(submission.PredictedProb)\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    return mean_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/valid shape: (114321, 131), test shape: (114321,)\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.556955\tvalid_1's binary_logloss: 0.561395\n",
      "[200]\ttraining's binary_logloss: 0.584978\tvalid_1's binary_logloss: 0.59416\n",
      "[300]\ttraining's binary_logloss: 0.58889\tvalid_1's binary_logloss: 0.603554\n",
      "[400]\ttraining's binary_logloss: 0.584124\tvalid_1's binary_logloss: 0.604451\n",
      "[500]\ttraining's binary_logloss: 0.577714\tvalid_1's binary_logloss: 0.603314\n",
      "[600]\ttraining's binary_logloss: 0.571009\tvalid_1's binary_logloss: 0.601735\n",
      "[700]\ttraining's binary_logloss: 0.564507\tvalid_1's binary_logloss: 0.600222\n",
      "[800]\ttraining's binary_logloss: 0.558324\tvalid_1's binary_logloss: 0.598828\n",
      "[900]\ttraining's binary_logloss: 0.552476\tvalid_1's binary_logloss: 0.597535\n",
      "[1000]\ttraining's binary_logloss: 0.546927\tvalid_1's binary_logloss: 0.596251\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's binary_logloss: 0.52988\tvalid_1's binary_logloss: 0.53191\n",
      "Fold  1 Log Loss : 0.531910\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.556863\tvalid_1's binary_logloss: 0.560369\n",
      "[200]\ttraining's binary_logloss: 0.584924\tvalid_1's binary_logloss: 0.593638\n",
      "[300]\ttraining's binary_logloss: 0.588417\tvalid_1's binary_logloss: 0.603242\n",
      "[400]\ttraining's binary_logloss: 0.583736\tvalid_1's binary_logloss: 0.604595\n",
      "[500]\ttraining's binary_logloss: 0.577373\tvalid_1's binary_logloss: 0.603375\n",
      "[600]\ttraining's binary_logloss: 0.570655\tvalid_1's binary_logloss: 0.601542\n",
      "[700]\ttraining's binary_logloss: 0.564289\tvalid_1's binary_logloss: 0.599842\n",
      "[800]\ttraining's binary_logloss: 0.558086\tvalid_1's binary_logloss: 0.598465\n",
      "[900]\ttraining's binary_logloss: 0.552244\tvalid_1's binary_logloss: 0.597118\n",
      "[1000]\ttraining's binary_logloss: 0.546635\tvalid_1's binary_logloss: 0.596078\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's binary_logloss: 0.530066\tvalid_1's binary_logloss: 0.530252\n",
      "Fold  2 Log Loss : 0.530252\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.556686\tvalid_1's binary_logloss: 0.562661\n",
      "[200]\ttraining's binary_logloss: 0.584125\tvalid_1's binary_logloss: 0.596432\n",
      "[300]\ttraining's binary_logloss: 0.587923\tvalid_1's binary_logloss: 0.606257\n",
      "[400]\ttraining's binary_logloss: 0.583179\tvalid_1's binary_logloss: 0.607289\n",
      "[500]\ttraining's binary_logloss: 0.576648\tvalid_1's binary_logloss: 0.605988\n",
      "[600]\ttraining's binary_logloss: 0.570078\tvalid_1's binary_logloss: 0.604424\n",
      "[700]\ttraining's binary_logloss: 0.563615\tvalid_1's binary_logloss: 0.602685\n",
      "[800]\ttraining's binary_logloss: 0.557486\tvalid_1's binary_logloss: 0.601206\n",
      "[900]\ttraining's binary_logloss: 0.551686\tvalid_1's binary_logloss: 0.599726\n",
      "[1000]\ttraining's binary_logloss: 0.546018\tvalid_1's binary_logloss: 0.59847\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's binary_logloss: 0.530563\tvalid_1's binary_logloss: 0.529355\n",
      "Fold  3 Log Loss : 0.529355\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.555911\tvalid_1's binary_logloss: 0.56674\n",
      "[200]\ttraining's binary_logloss: 0.583808\tvalid_1's binary_logloss: 0.599914\n",
      "[300]\ttraining's binary_logloss: 0.587469\tvalid_1's binary_logloss: 0.609437\n",
      "[400]\ttraining's binary_logloss: 0.58272\tvalid_1's binary_logloss: 0.610939\n",
      "[500]\ttraining's binary_logloss: 0.576183\tvalid_1's binary_logloss: 0.61015\n",
      "[600]\ttraining's binary_logloss: 0.569557\tvalid_1's binary_logloss: 0.609106\n",
      "[700]\ttraining's binary_logloss: 0.563156\tvalid_1's binary_logloss: 0.607652\n",
      "[800]\ttraining's binary_logloss: 0.557061\tvalid_1's binary_logloss: 0.606273\n",
      "[900]\ttraining's binary_logloss: 0.551181\tvalid_1's binary_logloss: 0.6052\n",
      "[1000]\ttraining's binary_logloss: 0.545626\tvalid_1's binary_logloss: 0.604136\n",
      "Early stopping, best iteration is:\n",
      "[28]\ttraining's binary_logloss: 0.529038\tvalid_1's binary_logloss: 0.537434\n",
      "Fold  4 Log Loss : 0.537434\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.556935\tvalid_1's binary_logloss: 0.55951\n",
      "[200]\ttraining's binary_logloss: 0.584847\tvalid_1's binary_logloss: 0.591964\n",
      "[300]\ttraining's binary_logloss: 0.588719\tvalid_1's binary_logloss: 0.601477\n",
      "[400]\ttraining's binary_logloss: 0.583986\tvalid_1's binary_logloss: 0.602909\n",
      "[500]\ttraining's binary_logloss: 0.577544\tvalid_1's binary_logloss: 0.601588\n",
      "[600]\ttraining's binary_logloss: 0.570984\tvalid_1's binary_logloss: 0.599962\n",
      "[700]\ttraining's binary_logloss: 0.564526\tvalid_1's binary_logloss: 0.598511\n",
      "[800]\ttraining's binary_logloss: 0.558256\tvalid_1's binary_logloss: 0.597038\n",
      "[900]\ttraining's binary_logloss: 0.552273\tvalid_1's binary_logloss: 0.59572\n",
      "[1000]\ttraining's binary_logloss: 0.546629\tvalid_1's binary_logloss: 0.594661\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's binary_logloss: 0.530041\tvalid_1's binary_logloss: 0.530601\n",
      "Fold  5 Log Loss : 0.530601\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.556687\tvalid_1's binary_logloss: 0.563725\n",
      "[200]\ttraining's binary_logloss: 0.584352\tvalid_1's binary_logloss: 0.598232\n",
      "[300]\ttraining's binary_logloss: 0.588048\tvalid_1's binary_logloss: 0.608065\n",
      "[400]\ttraining's binary_logloss: 0.583207\tvalid_1's binary_logloss: 0.60949\n",
      "[500]\ttraining's binary_logloss: 0.576665\tvalid_1's binary_logloss: 0.608575\n",
      "[600]\ttraining's binary_logloss: 0.569927\tvalid_1's binary_logloss: 0.606972\n",
      "[700]\ttraining's binary_logloss: 0.563427\tvalid_1's binary_logloss: 0.605417\n",
      "[800]\ttraining's binary_logloss: 0.557309\tvalid_1's binary_logloss: 0.604118\n",
      "[900]\ttraining's binary_logloss: 0.551329\tvalid_1's binary_logloss: 0.602689\n",
      "[1000]\ttraining's binary_logloss: 0.545538\tvalid_1's binary_logloss: 0.601453\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's binary_logloss: 0.529956\tvalid_1's binary_logloss: 0.531418\n",
      "Fold  6 Log Loss : 0.531418\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.556419\tvalid_1's binary_logloss: 0.562308\n",
      "[200]\ttraining's binary_logloss: 0.584296\tvalid_1's binary_logloss: 0.595701\n",
      "[300]\ttraining's binary_logloss: 0.587875\tvalid_1's binary_logloss: 0.605164\n",
      "[400]\ttraining's binary_logloss: 0.583247\tvalid_1's binary_logloss: 0.606243\n",
      "[500]\ttraining's binary_logloss: 0.576793\tvalid_1's binary_logloss: 0.604983\n",
      "[600]\ttraining's binary_logloss: 0.570187\tvalid_1's binary_logloss: 0.603479\n",
      "[700]\ttraining's binary_logloss: 0.563914\tvalid_1's binary_logloss: 0.601992\n",
      "[800]\ttraining's binary_logloss: 0.557842\tvalid_1's binary_logloss: 0.600679\n",
      "[900]\ttraining's binary_logloss: 0.552029\tvalid_1's binary_logloss: 0.599278\n",
      "[1000]\ttraining's binary_logloss: 0.546378\tvalid_1's binary_logloss: 0.598073\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttraining's binary_logloss: 0.529693\tvalid_1's binary_logloss: 0.53213\n",
      "Fold  7 Log Loss : 0.532130\n",
      "Training until validation scores don't improve for 1000 rounds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1de2f2b838a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-d7de546b4bde>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(dataset, categorical_feature)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logloss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     early_stopping_rounds= EARLY_STOPPING)\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             clf.fit(train_x, train_y, \n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    798\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    593\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1924\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1925\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1926\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1927\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_model(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03f293c5aaf34fd7a50a3b2bf14dab7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0af695c1d5e24a17b3a2aebc133ddd39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6782fd7c22664c55904d66122a64843b",
        "IPY_MODEL_9163f58ecd4f4a84922abc60cc69d789"
       ],
       "layout": "IPY_MODEL_d8264eeec6af442097559d8923081902"
      }
     },
     "1c88767c318e4d86a9ab43245f5daa77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "219092655bed428f83aeb751ccaa2a4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "44521ec2de994d5186cdf7d1e445d158": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6782fd7c22664c55904d66122a64843b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c5e6acfe3f654e7ca86e4cddf862e984",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_219092655bed428f83aeb751ccaa2a4d",
       "value": 1000
      }
     },
     "9163f58ecd4f4a84922abc60cc69d789": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9a5c32201e5c447b829c604d757794df",
       "placeholder": "​",
       "style": "IPY_MODEL_b90e11ceb23b4a84a2f4908689dff66f",
       "value": " 1000/1000 [01:50&lt;00:00,  9.06it/s]"
      }
     },
     "9a5c32201e5c447b829c604d757794df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a8bfc018378d49efa1570fcb43b496f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b90e11ceb23b4a84a2f4908689dff66f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c5e6acfe3f654e7ca86e4cddf862e984": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c95734415f404758ba787c86b58e7cc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d0cb9d8667c340c4b49fb7cf32acaf7c",
        "IPY_MODEL_e1aabb08150346cc9871c9f6b3bb5d36"
       ],
       "layout": "IPY_MODEL_1c88767c318e4d86a9ab43245f5daa77"
      }
     },
     "d0cb9d8667c340c4b49fb7cf32acaf7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_44521ec2de994d5186cdf7d1e445d158",
       "max": 17000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_eec34a1f17e2477fb10e7c11419a1382",
       "value": 17000
      }
     },
     "d8264eeec6af442097559d8923081902": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1aabb08150346cc9871c9f6b3bb5d36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a8bfc018378d49efa1570fcb43b496f3",
       "placeholder": "​",
       "style": "IPY_MODEL_03f293c5aaf34fd7a50a3b2bf14dab7d",
       "value": " 17000/17000 [11:01&lt;00:00, 25.68it/s]"
      }
     },
     "eec34a1f17e2477fb10e7c11419a1382": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
