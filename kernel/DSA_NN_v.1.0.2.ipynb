{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle\n",
    "## Competição DSA de Machine Learning - Dezembro 2019\n",
    "\n",
    "- Teste com redes neurais multicamadas (MLP)\n",
    "- Se gostou ou achou útil, up-vote!! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Versão 1.0.0: LB = 0.50744 / CV = ???**\n",
    "- modelo: NN com 3 camadas\n",
    "- features categoricas: removido\n",
    "- dados missing: atribuído o valor medio\n",
    "- feature selection: 25\n",
    "\n",
    "**Versão 1.0.1: LB = 0.52913 / CV = 0.471703**\n",
    "- modelo: NN com 3 camadas ocultas\n",
    "- dados missing: removido colunas com mais de 50% de NA e as demais usei a média\n",
    "- features categoricas: label encoder\n",
    "- feature engineering: usando pacote Boruta (dica do Allyson)\n",
    "\n",
    "**Versão 1.0.2: LB = ??? / CV = 0.472390**\n",
    "- modelo: NN com 3 camadas ocultas\n",
    "- features engineering: Kernel_Feature_Engineering_v.1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar os principais pacotes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "import random as rd\n",
    "import os\n",
    "import codecs\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from numba import jit\n",
    "from collections import Counter\n",
    "import copy\n",
    "from typing import Any\n",
    "\n",
    "seed = 12345\n",
    "np.random.seed(seed)\n",
    "rd.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Evitar que aparece os warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Seta algumas opções no Jupyter para exibição dos datasets\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# Variavel para controlar o treinamento no Kaggle\n",
    "TRAIN_OFFLINE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importa os pacotes de algoritmos\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb \n",
    "\n",
    "# Importa os pacotes de algoritmos de redes neurais (Keras)\n",
    "import keras\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda,BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import Callback,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Importa pacotes do sklearn\n",
    "from sklearn import preprocessing\n",
    "import sklearn.metrics as mtr\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, log_loss, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, StandardScaler\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler as SS\n",
    "from sklearn.model_selection import train_test_split as TTS\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau as RLRP\n",
    "from keras.callbacks import EarlyStopping as ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando os dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \n",
    "    if TRAIN_OFFLINE:\n",
    "        print('Carregando arquivo dataset_treino_new.csv....')\n",
    "        train = pd.read_csv('../dataset/dataset_treino_new.csv')\n",
    "        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n",
    "\n",
    "        print('Carregando arquivo dataset_teste_new.csv....')\n",
    "        test = pd.read_csv('../dataset/dataset_teste_new.csv')\n",
    "        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n",
    "\n",
    "        \n",
    "    else:\n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        train = pd.read_csv('/kaggle/input/competicao-dsa-machine-learning-dec-2019/dataset_treino.csv')\n",
    "        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n",
    "        \n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        test = pd.read_csv('/kaggle/input/competicao-dsa-machine-learning-dec-2019/dataset_teste.csv')\n",
    "        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando arquivo dataset_treino_new.csv....\n",
      "dataset_treino.csv tem 114321 linhas and 184 colunas\n",
      "Carregando arquivo dataset_teste_new.csv....\n",
      "dataset_teste.csv tem 114393 linhas and 183 colunas\n"
     ]
    }
   ],
   "source": [
    "# Leitura dos dados\n",
    "train, test = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando arquivo dataset_treino_new.csv....\n",
      "dataset_treino.csv tem 114321 linhas and 184 colunas\n",
      "Carregando arquivo dataset_teste_new.csv....\n",
      "dataset_teste.csv tem 114393 linhas and 183 colunas\n"
     ]
    }
   ],
   "source": [
    "# Leitura dos dados\n",
    "train, test = read_data()\n",
    "df = train.append(test)\n",
    "df = df.drop(columns = ['1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Numerical features:  164\n",
      "Number of Categorical features:  19\n"
     ]
    }
   ],
   "source": [
    "# Verificar a quantidade de features numericas e categoricas\n",
    "\n",
    "numerical_feats = df.dtypes[df.dtypes != \"object\"]\n",
    "print(\"Number of Numerical features: \", len(numerical_feats))\n",
    "\n",
    "categorical_feats = df.dtypes[df.dtypes == \"object\"].index\n",
    "print(\"Number of Categorical features: \", len(categorical_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando as features categorias com LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "for i, col in enumerate(df):\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = le.fit_transform(np.array(df[col].astype(str)).reshape((-1,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grp_5',\n",
       " 'grp_8',\n",
       " 'pca_1',\n",
       " 'srp_12',\n",
       " 'srp_2',\n",
       " 'srp_4',\n",
       " 'srp_5',\n",
       " 'srp_8',\n",
       " 'tsvd_1',\n",
       " 'tsvd_2',\n",
       " 'tsvd_6',\n",
       " 'tsvd_7',\n",
       " 'tsvd_8',\n",
       " 'tsvd_9',\n",
       " 'v114',\n",
       " 'v119 v123',\n",
       " 'v119 v48',\n",
       " 'v119 v85',\n",
       " 'v119^2',\n",
       " 'v119_x',\n",
       " 'v119_y',\n",
       " 'v123 v48',\n",
       " 'v123 v85',\n",
       " 'v123^2',\n",
       " 'v123_x',\n",
       " 'v123_y',\n",
       " 'v40',\n",
       " 'v48 v85',\n",
       " 'v48^2',\n",
       " 'v48_x',\n",
       " 'v48_y',\n",
       " 'v58',\n",
       " 'v62 v48',\n",
       " 'v62_x',\n",
       " 'v62_y',\n",
       " 'v69',\n",
       " 'v75',\n",
       " 'v85_x',\n",
       " 'v85_y']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = new_df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228714, 144)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop features \n",
    "new_df = new_df.drop(new_df[to_drop], axis=1)\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar o dataset de treino e teste\n",
    "treino = new_df[new_df['target'].notnull()]\n",
    "teste  = new_df[new_df['target'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature  importance\n",
      "106       v50    0.161647\n",
      "118       v66    0.055839\n",
      "104       v47    0.024312\n",
      "95        v31    0.022960\n",
      "126       v79    0.021386\n",
      "109       v56    0.017183\n",
      "15      ica_3    0.017015\n",
      "35     srp_11    0.016480\n",
      "47        v10    0.014563\n",
      "39      srp_9    0.013490\n",
      "86        v22    0.013485\n",
      "31      pca_8    0.012646\n",
      "59       v110    0.011759\n",
      "85        v21    0.011732\n",
      "34     srp_10    0.011441\n",
      "88        v24    0.010797\n",
      "71       v125    0.010354\n",
      "28      pca_5    0.009684\n",
      "4       grp_2    0.009178\n",
      "0       grp_1    0.008981\n",
      "62       v113    0.008955\n",
      "77        v14    0.008915\n",
      "6       grp_4    0.008878\n",
      "2      grp_11    0.008633\n",
      "13     ica_12    0.008621\n",
      "7       grp_6    0.008312\n",
      "19      ica_7    0.008035\n",
      "14      ica_2    0.008002\n",
      "41    tsvd_11    0.007690\n",
      "32      pca_9    0.007641\n",
      "74       v129    0.007550\n",
      "24     pca_12    0.007492\n",
      "40    tsvd_10    0.007491\n",
      "30      pca_7    0.007402\n",
      "1      grp_10    0.007354\n",
      "21      ica_9    0.007211\n",
      "20      ica_8    0.007175\n",
      "17      ica_5    0.007158\n",
      "10      ica_1    0.007126\n",
      "18      ica_6    0.007070\n",
      "9       grp_9    0.007045\n",
      "26      pca_3    0.007036\n",
      "29      pca_6    0.006986\n",
      "11     ica_10    0.006947\n",
      "12     ica_11    0.006934\n",
      "42    tsvd_12    0.006931\n",
      "61       v112    0.006886\n",
      "22     pca_10    0.006878\n",
      "16      ica_4    0.006849\n",
      "45     tsvd_5    0.006795\n",
      "23     pca_11    0.006757\n",
      "43     tsvd_3    0.006661\n",
      "8       grp_7    0.006632\n",
      "5       grp_3    0.006483\n",
      "108       v52    0.006454\n",
      "27      pca_4    0.006324\n",
      "3      grp_12    0.006233\n",
      "25      pca_2    0.006101\n",
      "44     tsvd_4    0.006082\n",
      "36      srp_3    0.005608\n",
      "94        v30    0.004702\n",
      "115  v62 v123    0.004258\n",
      "63       v115    0.003851\n",
      "141       v99    0.003819\n",
      "112        v6    0.003732\n",
      "79        v16    0.003626\n",
      "67       v120    0.003622\n",
      "37      srp_6    0.003591\n",
      "133       v88    0.003560\n",
      "38      srp_7    0.003536\n",
      "136       v91    0.003533\n",
      "55       v107    0.003533\n",
      "50       v102    0.003499\n",
      "101       v42    0.003468\n",
      "110       v57    0.003438\n",
      "132       v86    0.003415\n",
      "76       v131    0.003409\n",
      "125       v78    0.003354\n",
      "134        v9    0.003338\n",
      "98        v37    0.003268\n",
      "69       v122    0.003263\n",
      "135       v90    0.003259\n",
      "96        v35    0.003250\n",
      "99        v39    0.003249\n",
      "72       v126    0.003234\n",
      "91        v28    0.003229\n",
      "129       v82    0.003201\n",
      "66       v118    0.003199\n",
      "46         v1    0.003193\n",
      "58        v11    0.003183\n",
      "48       v100    0.003174\n",
      "140       v98    0.003152\n",
      "103       v45    0.003108\n",
      "70       v124    0.003102\n",
      "119       v68    0.003093\n",
      "81        v18    0.003069\n",
      "51       v103    0.003050\n",
      "73       v127    0.003048\n",
      "64       v116    0.003044\n",
      "120        v7    0.003031\n",
      "52       v104    0.003008\n",
      "121       v70    0.002998\n",
      "78        v15    0.002971\n",
      "116   v62 v85    0.002966\n",
      "139       v94    0.002947\n",
      "89        v26    0.002870\n",
      "53       v105    0.002869\n",
      "83         v2    0.002863\n",
      "97        v36    0.002860\n",
      "130       v84    0.002855\n",
      "82        v19    0.002847\n",
      "137       v92    0.002841\n",
      "111       v59    0.002789\n",
      "84        v20    0.002788\n",
      "90        v27    0.002779\n",
      "102       v44    0.002767\n",
      "60       v111    0.002738\n",
      "75       v130    0.002728\n",
      "68       v121    0.002696\n",
      "33      srp_1    0.002678\n",
      "127       v80    0.002654\n",
      "122       v71    0.002613\n",
      "65       v117    0.002586\n",
      "57       v109    0.002560\n",
      "56       v108    0.002546\n",
      "105        v5    0.002530\n",
      "49       v101    0.002500\n",
      "114  v62 v119    0.002464\n",
      "138       v93    0.002424\n",
      "100        v4    0.002419\n",
      "92        v29    0.002413\n",
      "128       v81    0.002392\n",
      "113       v61    0.002338\n",
      "107       v51    0.002314\n",
      "131     v85^2    0.002100\n",
      "80        v17    0.002034\n",
      "54       v106    0.001839\n",
      "123       v72    0.000897\n",
      "124       v74    0.000527\n",
      "117     v62^2    0.000504\n",
      "87        v23    0.000428\n",
      "93         v3    0.000114\n"
     ]
    }
   ],
   "source": [
    "# Importância do Atributo com o Random Forest Regressor\n",
    "X_ = treino.drop(['ID','target'], axis=1)\n",
    "y_ = treino['target']\n",
    "\n",
    "# Padronizando os dados (0 para a média, 1 para o desvio padrão)\n",
    "scaler = StandardScaler()\n",
    "X_ = scaler.fit_transform(X_)\n",
    "\n",
    "# Criação do Modelo - Feature Selection\n",
    "modeloRF = RandomForestRegressor(bootstrap=False, \n",
    "                                 max_features=0.3, \n",
    "                                 min_samples_leaf=15, \n",
    "                                 min_samples_split=8, \n",
    "                                 n_estimators=50, \n",
    "                                 n_jobs=-1, \n",
    "                                 random_state=42)\n",
    "modeloRF.fit(X_, y_)\n",
    "\n",
    "# Convertendo o resultado em um dataframe\n",
    "feature_importance_df = pd.DataFrame(treino.drop(['ID','target'], axis=1).columns,columns=['Feature'])\n",
    "feature_importance_df['importance'] = pd.DataFrame(modeloRF.feature_importances_.astype(float))\n",
    "\n",
    "# Realizando a ordenacao por Importancia (Maior para Menor)\n",
    "result = feature_importance_df.sort_values('importance',ascending=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:25].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(18,16))\n",
    "sns.barplot(x=\"importance\",\n",
    "           y=\"Feature\",\n",
    "           data=best_features.sort_values(by=\"importance\",\n",
    "                                          ascending=False))\n",
    "plt.title('Importance Features')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo Neural Network MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114321, 25), (114321, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar um dataset somente com as colunas mais importantes conforme Feature Selection\n",
    "new_X = treino.loc[:,best_features['Feature']]\n",
    "\n",
    "# Padronizando os dados de treino\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(new_X)\n",
    "\n",
    "train_y = np_utils.to_categorical(treino['target'])\n",
    "\n",
    "# Verificando o shape dos datasets depois dos ajustes\n",
    "# Neste momento está pronto para ser usado pelo treinamento\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6833"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limpeza da memória\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando função para treinar a rede neural\n",
    "def get_nn(x_tr,y_tr,x_val,y_val,shape):\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Cria a estrutura da rede neural com 3 camadas ocultas\n",
    "    inp = Input(shape = (x_tr.shape[1],))\n",
    "\n",
    "    x = Dense(1024, input_dim=x_tr.shape[1], activation='relu')(inp)\n",
    "    x = Dropout(0.5)(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    out = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inp,out)\n",
    "    \n",
    "    model.compile(optimizer = 'Adam',\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['categorical_accuracy'])\n",
    "    \n",
    "    # Realiza a parada mais cedo quando percebe overfitting\n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                       mode='min',\n",
    "                       restore_best_weights=True, \n",
    "                       verbose=1, \n",
    "                       patience=20)\n",
    "\n",
    "    # Realiza checkpoint durante o treinamento\n",
    "    mc = ModelCheckpoint('best_model.h5',\n",
    "                         monitor='val_loss',\n",
    "                         mode='min',\n",
    "                         save_best_only=True, \n",
    "                         verbose=1, \n",
    "                         save_weights_only=True)\n",
    "\n",
    "    # Realize o ajuste na Learning Rate durante o treinamento\n",
    "    rl = ReduceLROnPlateau(monitor='val_loss', \n",
    "                           factor=0.1, \n",
    "                           patience=10, \n",
    "                           verbose=1, \n",
    "                           epsilon=1e-4, \n",
    "                           mode='min')\n",
    "\n",
    "    # Realiza o fit do modelo\n",
    "    model.fit(x_tr, y_tr,\n",
    "              validation_data=[x_val, y_val],\n",
    "              callbacks=[es,mc,rl],\n",
    "              epochs=250, \n",
    "              batch_size=1024,\n",
    "              verbose=1,\n",
    "              shuffle=True)\n",
    "    \n",
    "    # Carrega os melhores pesos\n",
    "    model.load_weights(\"best_model.h5\")\n",
    "    \n",
    "    # Realiza as previsões\n",
    "    y_pred = model.predict(x_val)\n",
    "    y_valid = y_val\n",
    "             \n",
    "    # Calcula o log loss\n",
    "    logloss = log_loss(y_valid, y_pred, eps=1e-15)\n",
    "\n",
    "    return model, logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Loop 1/2 Fold 1/5\n",
      "-----------\n",
      "Train on 91456 samples, validate on 22865 samples\n",
      "Epoch 1/250\n",
      "91456/91456 [==============================] - 10s 105us/step - loss: 0.6286 - categorical_accuracy: 0.6834 - val_loss: 0.5061 - val_categorical_accuracy: 0.7616\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50606, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91456/91456 [==============================] - 7s 73us/step - loss: 0.5231 - categorical_accuracy: 0.7566 - val_loss: 0.4984 - val_categorical_accuracy: 0.7573\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.50606 to 0.49836, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91456/91456 [==============================] - 6s 64us/step - loss: 0.5093 - categorical_accuracy: 0.7627 - val_loss: 0.4884 - val_categorical_accuracy: 0.7708\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49836 to 0.48842, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91456/91456 [==============================] - 6s 64us/step - loss: 0.5019 - categorical_accuracy: 0.7647 - val_loss: 0.4842 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48842 to 0.48422, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91456/91456 [==============================] - 6s 65us/step - loss: 0.4958 - categorical_accuracy: 0.7681 - val_loss: 0.4792 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48422 to 0.47920, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "91456/91456 [==============================] - 7s 74us/step - loss: 0.4946 - categorical_accuracy: 0.7691 - val_loss: 0.4779 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.47920 to 0.47792, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4900 - categorical_accuracy: 0.7712 - val_loss: 0.4772 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.47792 to 0.47723, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4872 - categorical_accuracy: 0.7723 - val_loss: 0.4751 - val_categorical_accuracy: 0.7805\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.47723 to 0.47507, saving model to best_model.h5\n",
      "Epoch 9/250\n",
      "91456/91456 [==============================] - 6s 67us/step - loss: 0.4867 - categorical_accuracy: 0.7728 - val_loss: 0.4742 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.47507 to 0.47424, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91456/91456 [==============================] - 7s 75us/step - loss: 0.4845 - categorical_accuracy: 0.7738 - val_loss: 0.4742 - val_categorical_accuracy: 0.7808\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.47424 to 0.47422, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "91456/91456 [==============================] - 7s 73us/step - loss: 0.4836 - categorical_accuracy: 0.7736 - val_loss: 0.4731 - val_categorical_accuracy: 0.7816\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.47422 to 0.47310, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4820 - categorical_accuracy: 0.7767 - val_loss: 0.4720 - val_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.47310 to 0.47203, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "91456/91456 [==============================] - 7s 72us/step - loss: 0.4810 - categorical_accuracy: 0.7752 - val_loss: 0.4736 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.47203\n",
      "Epoch 14/250\n",
      "91456/91456 [==============================] - 6s 64us/step - loss: 0.4810 - categorical_accuracy: 0.7751 - val_loss: 0.4720 - val_categorical_accuracy: 0.7805\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.47203 to 0.47195, saving model to best_model.h5\n",
      "Epoch 15/250\n",
      "91456/91456 [==============================] - 7s 71us/step - loss: 0.4794 - categorical_accuracy: 0.7765 - val_loss: 0.4711 - val_categorical_accuracy: 0.7810\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.47195 to 0.47111, saving model to best_model.h5\n",
      "Epoch 16/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4791 - categorical_accuracy: 0.7763 - val_loss: 0.4718 - val_categorical_accuracy: 0.7815\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.47111\n",
      "Epoch 17/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4787 - categorical_accuracy: 0.7765 - val_loss: 0.4717 - val_categorical_accuracy: 0.7816\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.47111\n",
      "Epoch 18/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4782 - categorical_accuracy: 0.7759 - val_loss: 0.4698 - val_categorical_accuracy: 0.7829\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.47111 to 0.46978, saving model to best_model.h5\n",
      "Epoch 19/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4777 - categorical_accuracy: 0.7784 - val_loss: 0.4720 - val_categorical_accuracy: 0.7818\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.46978\n",
      "Epoch 20/250\n",
      "91456/91456 [==============================] - 6s 65us/step - loss: 0.4767 - categorical_accuracy: 0.7773 - val_loss: 0.4694 - val_categorical_accuracy: 0.7814\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.46978 to 0.46937, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "91456/91456 [==============================] - 6s 64us/step - loss: 0.4761 - categorical_accuracy: 0.7778 - val_loss: 0.4703 - val_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.46937\n",
      "Epoch 22/250\n",
      "91456/91456 [==============================] - 7s 71us/step - loss: 0.4759 - categorical_accuracy: 0.7773 - val_loss: 0.4696 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.46937\n",
      "Epoch 23/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4765 - categorical_accuracy: 0.7773 - val_loss: 0.4696 - val_categorical_accuracy: 0.7830\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.46937\n",
      "Epoch 24/250\n",
      "91456/91456 [==============================] - 7s 71us/step - loss: 0.4754 - categorical_accuracy: 0.7785 - val_loss: 0.4705 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.46937\n",
      "Epoch 25/250\n",
      "91456/91456 [==============================] - 7s 72us/step - loss: 0.4746 - categorical_accuracy: 0.7779 - val_loss: 0.4705 - val_categorical_accuracy: 0.7812\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.46937\n",
      "Epoch 26/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4755 - categorical_accuracy: 0.7784 - val_loss: 0.4689 - val_categorical_accuracy: 0.7839\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.46937 to 0.46890, saving model to best_model.h5\n",
      "Epoch 27/250\n",
      "91456/91456 [==============================] - 7s 73us/step - loss: 0.4734 - categorical_accuracy: 0.7780 - val_loss: 0.4688 - val_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.46890 to 0.46878, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "91456/91456 [==============================] - 6s 71us/step - loss: 0.4738 - categorical_accuracy: 0.7797 - val_loss: 0.4693 - val_categorical_accuracy: 0.7832\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.46878\n",
      "Epoch 29/250\n",
      "91456/91456 [==============================] - 7s 73us/step - loss: 0.4738 - categorical_accuracy: 0.7785 - val_loss: 0.4678 - val_categorical_accuracy: 0.7834\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.46878 to 0.46784, saving model to best_model.h5\n",
      "Epoch 30/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4727 - categorical_accuracy: 0.7782 - val_loss: 0.4683 - val_categorical_accuracy: 0.7828\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.46784\n",
      "Epoch 31/250\n",
      "91456/91456 [==============================] - 7s 72us/step - loss: 0.4730 - categorical_accuracy: 0.7789 - val_loss: 0.4683 - val_categorical_accuracy: 0.7825\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.46784\n",
      "Epoch 32/250\n",
      "91456/91456 [==============================] - 6s 71us/step - loss: 0.4722 - categorical_accuracy: 0.7793 - val_loss: 0.4676 - val_categorical_accuracy: 0.7826\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.46784 to 0.46760, saving model to best_model.h5\n",
      "Epoch 33/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4722 - categorical_accuracy: 0.7796 - val_loss: 0.4675 - val_categorical_accuracy: 0.7832\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.46760 to 0.46752, saving model to best_model.h5\n",
      "Epoch 34/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91456/91456 [==============================] - 7s 74us/step - loss: 0.4712 - categorical_accuracy: 0.7793 - val_loss: 0.4681 - val_categorical_accuracy: 0.7819\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.46752\n",
      "Epoch 35/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4709 - categorical_accuracy: 0.7806 - val_loss: 0.4690 - val_categorical_accuracy: 0.7841\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.46752\n",
      "Epoch 36/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4707 - categorical_accuracy: 0.7797 - val_loss: 0.4669 - val_categorical_accuracy: 0.7832\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.46752 to 0.46687, saving model to best_model.h5\n",
      "Epoch 37/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4708 - categorical_accuracy: 0.7787 - val_loss: 0.4668 - val_categorical_accuracy: 0.7839\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.46687 to 0.46677, saving model to best_model.h5\n",
      "Epoch 38/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4698 - categorical_accuracy: 0.7808 - val_loss: 0.4694 - val_categorical_accuracy: 0.7831\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.46677\n",
      "Epoch 39/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4703 - categorical_accuracy: 0.7797 - val_loss: 0.4677 - val_categorical_accuracy: 0.7831\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.46677\n",
      "Epoch 40/250\n",
      "91456/91456 [==============================] - 7s 72us/step - loss: 0.4694 - categorical_accuracy: 0.7796 - val_loss: 0.4675 - val_categorical_accuracy: 0.7846\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.46677\n",
      "Epoch 41/250\n",
      "91456/91456 [==============================] - 6s 65us/step - loss: 0.4695 - categorical_accuracy: 0.7804 - val_loss: 0.4679 - val_categorical_accuracy: 0.7835\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.46677\n",
      "Epoch 42/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4694 - categorical_accuracy: 0.7809 - val_loss: 0.4690 - val_categorical_accuracy: 0.7834\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.46677\n",
      "Epoch 43/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4686 - categorical_accuracy: 0.7804 - val_loss: 0.4687 - val_categorical_accuracy: 0.7823\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.46677\n",
      "Epoch 44/250\n",
      "91456/91456 [==============================] - 6s 71us/step - loss: 0.4683 - categorical_accuracy: 0.7801 - val_loss: 0.4684 - val_categorical_accuracy: 0.7816\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.46677\n",
      "Epoch 45/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4680 - categorical_accuracy: 0.7814 - val_loss: 0.4677 - val_categorical_accuracy: 0.7843\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.46677\n",
      "Epoch 46/250\n",
      "91456/91456 [==============================] - 6s 66us/step - loss: 0.4676 - categorical_accuracy: 0.7808 - val_loss: 0.4665 - val_categorical_accuracy: 0.7835\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.46677 to 0.46652, saving model to best_model.h5\n",
      "Epoch 47/250\n",
      "91456/91456 [==============================] - 7s 72us/step - loss: 0.4672 - categorical_accuracy: 0.7805 - val_loss: 0.4668 - val_categorical_accuracy: 0.7836\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.46652\n",
      "Epoch 48/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4667 - categorical_accuracy: 0.7808 - val_loss: 0.4685 - val_categorical_accuracy: 0.7832\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.46652\n",
      "Epoch 49/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4666 - categorical_accuracy: 0.7807 - val_loss: 0.4669 - val_categorical_accuracy: 0.7834\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.46652\n",
      "Epoch 50/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4661 - categorical_accuracy: 0.7815 - val_loss: 0.4675 - val_categorical_accuracy: 0.7829\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.46652\n",
      "Epoch 51/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4658 - categorical_accuracy: 0.7820 - val_loss: 0.4683 - val_categorical_accuracy: 0.7827\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.46652\n",
      "Epoch 52/250\n",
      "91456/91456 [==============================] - 6s 66us/step - loss: 0.4651 - categorical_accuracy: 0.7813 - val_loss: 0.4694 - val_categorical_accuracy: 0.7827\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.46652\n",
      "Epoch 53/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4643 - categorical_accuracy: 0.7816 - val_loss: 0.4683 - val_categorical_accuracy: 0.7830\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.46652\n",
      "Epoch 54/250\n",
      "91456/91456 [==============================] - 7s 73us/step - loss: 0.4652 - categorical_accuracy: 0.7811 - val_loss: 0.4678 - val_categorical_accuracy: 0.7832\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.46652\n",
      "Epoch 55/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4640 - categorical_accuracy: 0.7817 - val_loss: 0.4677 - val_categorical_accuracy: 0.7829\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.46652\n",
      "Epoch 56/250\n",
      "91456/91456 [==============================] - 7s 73us/step - loss: 0.4642 - categorical_accuracy: 0.7815 - val_loss: 0.4681 - val_categorical_accuracy: 0.7835\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.46652\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 57/250\n",
      "91456/91456 [==============================] - 6s 66us/step - loss: 0.4605 - categorical_accuracy: 0.7832 - val_loss: 0.4675 - val_categorical_accuracy: 0.7839\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.46652\n",
      "Epoch 58/250\n",
      "91456/91456 [==============================] - 6s 65us/step - loss: 0.4595 - categorical_accuracy: 0.7837 - val_loss: 0.4672 - val_categorical_accuracy: 0.7838\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.46652\n",
      "Epoch 59/250\n",
      "91456/91456 [==============================] - 6s 64us/step - loss: 0.4582 - categorical_accuracy: 0.7844 - val_loss: 0.4674 - val_categorical_accuracy: 0.7841\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.46652\n",
      "Epoch 60/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4577 - categorical_accuracy: 0.7849 - val_loss: 0.4674 - val_categorical_accuracy: 0.7841\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.46652\n",
      "Epoch 61/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4572 - categorical_accuracy: 0.7849 - val_loss: 0.4674 - val_categorical_accuracy: 0.7836\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.46652\n",
      "Epoch 62/250\n",
      "91456/91456 [==============================] - 7s 71us/step - loss: 0.4568 - categorical_accuracy: 0.7851 - val_loss: 0.4677 - val_categorical_accuracy: 0.7835\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.46652\n",
      "Epoch 63/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4563 - categorical_accuracy: 0.7852 - val_loss: 0.4677 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.46652\n",
      "Epoch 64/250\n",
      "91456/91456 [==============================] - 6s 65us/step - loss: 0.4568 - categorical_accuracy: 0.7853 - val_loss: 0.4679 - val_categorical_accuracy: 0.7832\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.46652\n",
      "Epoch 65/250\n",
      "91456/91456 [==============================] - 7s 73us/step - loss: 0.4562 - categorical_accuracy: 0.7853 - val_loss: 0.4678 - val_categorical_accuracy: 0.7840\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.46652\n",
      "Epoch 66/250\n",
      "91456/91456 [==============================] - 6s 67us/step - loss: 0.4556 - categorical_accuracy: 0.7849 - val_loss: 0.4679 - val_categorical_accuracy: 0.7838\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.46652\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00066: early stopping\n",
      "the 1 fold Log-Loss (NN) is 0.466522\n",
      "-----------\n",
      "Loop 1/2 Fold 2/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 9s 93us/step - loss: 0.6263 - categorical_accuracy: 0.6854 - val_loss: 0.5144 - val_categorical_accuracy: 0.7579\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51441, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 6s 64us/step - loss: 0.5189 - categorical_accuracy: 0.7584 - val_loss: 0.5044 - val_categorical_accuracy: 0.7588\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51441 to 0.50439, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 7s 73us/step - loss: 0.5044 - categorical_accuracy: 0.7654 - val_loss: 0.4958 - val_categorical_accuracy: 0.7625\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.50439 to 0.49584, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4985 - categorical_accuracy: 0.7670 - val_loss: 0.4930 - val_categorical_accuracy: 0.7699\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49584 to 0.49299, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 7s 73us/step - loss: 0.4941 - categorical_accuracy: 0.7692 - val_loss: 0.4902 - val_categorical_accuracy: 0.7728\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49299 to 0.49016, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 7s 74us/step - loss: 0.4898 - categorical_accuracy: 0.7708 - val_loss: 0.4891 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.49016 to 0.48913, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4864 - categorical_accuracy: 0.7723 - val_loss: 0.4876 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.48913 to 0.48758, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4839 - categorical_accuracy: 0.7742 - val_loss: 0.4874 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.48758 to 0.48737, saving model to best_model.h5\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4828 - categorical_accuracy: 0.7748 - val_loss: 0.4858 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.48737 to 0.48579, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4811 - categorical_accuracy: 0.7754 - val_loss: 0.4851 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.48579 to 0.48514, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 7s 73us/step - loss: 0.4798 - categorical_accuracy: 0.7763 - val_loss: 0.4869 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48514\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4790 - categorical_accuracy: 0.7758 - val_loss: 0.4830 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.48514 to 0.48300, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 7s 75us/step - loss: 0.4779 - categorical_accuracy: 0.7775 - val_loss: 0.4834 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.48300\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4779 - categorical_accuracy: 0.7779 - val_loss: 0.4864 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.48300\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 10s 110us/step - loss: 0.4766 - categorical_accuracy: 0.7778 - val_loss: 0.4832 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.48300\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4752 - categorical_accuracy: 0.7796 - val_loss: 0.4832 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.48300\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4758 - categorical_accuracy: 0.7789 - val_loss: 0.4819 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.48300 to 0.48191, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4748 - categorical_accuracy: 0.7790 - val_loss: 0.4829 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.48191\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4747 - categorical_accuracy: 0.7789 - val_loss: 0.4822 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.48191\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4741 - categorical_accuracy: 0.7792 - val_loss: 0.4811 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.48191 to 0.48112, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4725 - categorical_accuracy: 0.7797 - val_loss: 0.4838 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.48112\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4728 - categorical_accuracy: 0.7798 - val_loss: 0.4816 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.48112\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 14s 150us/step - loss: 0.4729 - categorical_accuracy: 0.7789 - val_loss: 0.4821 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.48112\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 13s 146us/step - loss: 0.4717 - categorical_accuracy: 0.7793 - val_loss: 0.4824 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48112\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4717 - categorical_accuracy: 0.7802 - val_loss: 0.4809 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.48112 to 0.48093, saving model to best_model.h5\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4704 - categorical_accuracy: 0.7798 - val_loss: 0.4804 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.48093 to 0.48040, saving model to best_model.h5\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4714 - categorical_accuracy: 0.7804 - val_loss: 0.4811 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.48040\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4701 - categorical_accuracy: 0.7805 - val_loss: 0.4835 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.48040\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4702 - categorical_accuracy: 0.7804 - val_loss: 0.4822 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.48040\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4702 - categorical_accuracy: 0.7812 - val_loss: 0.4811 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.48040\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4699 - categorical_accuracy: 0.7810 - val_loss: 0.4827 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.48040\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4693 - categorical_accuracy: 0.7809 - val_loss: 0.4826 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.48040\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 13s 147us/step - loss: 0.4684 - categorical_accuracy: 0.7821 - val_loss: 0.4818 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.48040\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 13s 146us/step - loss: 0.4680 - categorical_accuracy: 0.7816 - val_loss: 0.4816 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.48040\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4681 - categorical_accuracy: 0.7809 - val_loss: 0.4813 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.48040\n",
      "Epoch 36/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4680 - categorical_accuracy: 0.7810 - val_loss: 0.4823 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.48040\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4648 - categorical_accuracy: 0.7831 - val_loss: 0.4807 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.48040\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4636 - categorical_accuracy: 0.7834 - val_loss: 0.4803 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.48040 to 0.48031, saving model to best_model.h5\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4632 - categorical_accuracy: 0.7834 - val_loss: 0.4802 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.48031 to 0.48025, saving model to best_model.h5\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4627 - categorical_accuracy: 0.7839 - val_loss: 0.4803 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.48025\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4623 - categorical_accuracy: 0.7835 - val_loss: 0.4802 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.48025 to 0.48021, saving model to best_model.h5\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 13s 146us/step - loss: 0.4625 - categorical_accuracy: 0.7837 - val_loss: 0.4805 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.48021\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 13s 147us/step - loss: 0.4619 - categorical_accuracy: 0.7840 - val_loss: 0.4803 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.48021\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 13s 146us/step - loss: 0.4626 - categorical_accuracy: 0.7834 - val_loss: 0.4799 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.48021 to 0.47994, saving model to best_model.h5\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4617 - categorical_accuracy: 0.7843 - val_loss: 0.4802 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47994\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 13s 146us/step - loss: 0.4614 - categorical_accuracy: 0.7848 - val_loss: 0.4801 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47994\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 13s 146us/step - loss: 0.4609 - categorical_accuracy: 0.7848 - val_loss: 0.4803 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47994\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4612 - categorical_accuracy: 0.7851 - val_loss: 0.4803 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47994\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4599 - categorical_accuracy: 0.7852 - val_loss: 0.4803 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47994\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4597 - categorical_accuracy: 0.7845 - val_loss: 0.4810 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47994\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 12s 128us/step - loss: 0.4605 - categorical_accuracy: 0.7854 - val_loss: 0.4806 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47994\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 12s 131us/step - loss: 0.4605 - categorical_accuracy: 0.7835 - val_loss: 0.4804 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47994\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 13s 146us/step - loss: 0.4599 - categorical_accuracy: 0.7850 - val_loss: 0.4803 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47994\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 13s 146us/step - loss: 0.4598 - categorical_accuracy: 0.7853 - val_loss: 0.4801 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47994\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4589 - categorical_accuracy: 0.7849 - val_loss: 0.4802 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47994\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4592 - categorical_accuracy: 0.7859 - val_loss: 0.4802 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47994\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4594 - categorical_accuracy: 0.7858 - val_loss: 0.4803 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47994\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4592 - categorical_accuracy: 0.7856 - val_loss: 0.4803 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47994\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4589 - categorical_accuracy: 0.7852 - val_loss: 0.4804 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47994\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4591 - categorical_accuracy: 0.7856 - val_loss: 0.4804 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47994\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4587 - categorical_accuracy: 0.7854 - val_loss: 0.4804 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47994\n",
      "Epoch 62/250\n",
      "91457/91457 [==============================] - 14s 149us/step - loss: 0.4590 - categorical_accuracy: 0.7855 - val_loss: 0.4804 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47994\n",
      "Epoch 63/250\n",
      "91457/91457 [==============================] - 15s 159us/step - loss: 0.4594 - categorical_accuracy: 0.7852 - val_loss: 0.4804 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47994\n",
      "Epoch 64/250\n",
      "91457/91457 [==============================] - 15s 168us/step - loss: 0.4588 - categorical_accuracy: 0.7849 - val_loss: 0.4804 - val_categorical_accuracy: 0.7744\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47994\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00064: early stopping\n",
      "the 2 fold Log-Loss (NN) is 0.479937\n",
      "-----------\n",
      "Loop 1/2 Fold 3/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 22s 240us/step - loss: 0.6231 - categorical_accuracy: 0.6882 - val_loss: 0.5114 - val_categorical_accuracy: 0.7626\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51139, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 12s 134us/step - loss: 0.5202 - categorical_accuracy: 0.7571 - val_loss: 0.4986 - val_categorical_accuracy: 0.7649\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51139 to 0.49861, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 13s 138us/step - loss: 0.5075 - categorical_accuracy: 0.7638 - val_loss: 0.4884 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49861 to 0.48838, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 15s 162us/step - loss: 0.5011 - categorical_accuracy: 0.7649 - val_loss: 0.4864 - val_categorical_accuracy: 0.7720\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48838 to 0.48636, saving model to best_model.h5\n",
      "Epoch 5/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 14s 157us/step - loss: 0.4949 - categorical_accuracy: 0.7693 - val_loss: 0.4849 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48636 to 0.48486, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4925 - categorical_accuracy: 0.7698 - val_loss: 0.4822 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.48486 to 0.48218, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 15s 162us/step - loss: 0.4895 - categorical_accuracy: 0.7710 - val_loss: 0.4806 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.48218 to 0.48064, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4873 - categorical_accuracy: 0.7719 - val_loss: 0.4786 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.48064 to 0.47862, saving model to best_model.h5\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 15s 162us/step - loss: 0.4857 - categorical_accuracy: 0.7734 - val_loss: 0.4771 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.47862 to 0.47712, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 15s 161us/step - loss: 0.4833 - categorical_accuracy: 0.7744 - val_loss: 0.4778 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.47712\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4820 - categorical_accuracy: 0.7757 - val_loss: 0.4765 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.47712 to 0.47649, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 14s 158us/step - loss: 0.4812 - categorical_accuracy: 0.7759 - val_loss: 0.4754 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.47649 to 0.47540, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4807 - categorical_accuracy: 0.7747 - val_loss: 0.4773 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.47540\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 15s 161us/step - loss: 0.4789 - categorical_accuracy: 0.7770 - val_loss: 0.4755 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.47540\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 14s 151us/step - loss: 0.4792 - categorical_accuracy: 0.7769 - val_loss: 0.4754 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.47540\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 14s 155us/step - loss: 0.4784 - categorical_accuracy: 0.7771 - val_loss: 0.4747 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.47540 to 0.47469, saving model to best_model.h5\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 14s 156us/step - loss: 0.4778 - categorical_accuracy: 0.7771 - val_loss: 0.4757 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.47469\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4766 - categorical_accuracy: 0.7776 - val_loss: 0.4753 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.47469\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 15s 164us/step - loss: 0.4769 - categorical_accuracy: 0.7777 - val_loss: 0.4737 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.47469 to 0.47367, saving model to best_model.h5\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4756 - categorical_accuracy: 0.7787 - val_loss: 0.4734 - val_categorical_accuracy: 0.7811\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.47367 to 0.47337, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 15s 164us/step - loss: 0.4750 - categorical_accuracy: 0.7789 - val_loss: 0.4748 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.47337\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 15s 159us/step - loss: 0.4748 - categorical_accuracy: 0.7784 - val_loss: 0.4733 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.47337 to 0.47327, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 15s 165us/step - loss: 0.4744 - categorical_accuracy: 0.7785 - val_loss: 0.4735 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.47327\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 14s 156us/step - loss: 0.4737 - categorical_accuracy: 0.7792 - val_loss: 0.4734 - val_categorical_accuracy: 0.7806\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.47327\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 15s 162us/step - loss: 0.4737 - categorical_accuracy: 0.7792 - val_loss: 0.4750 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.47327\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4732 - categorical_accuracy: 0.7790 - val_loss: 0.4747 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.47327\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 15s 162us/step - loss: 0.4726 - categorical_accuracy: 0.7793 - val_loss: 0.4721 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.47327 to 0.47211, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4723 - categorical_accuracy: 0.7789 - val_loss: 0.4723 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.47211\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4714 - categorical_accuracy: 0.7799 - val_loss: 0.4731 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.47211\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 15s 162us/step - loss: 0.4710 - categorical_accuracy: 0.7792 - val_loss: 0.4744 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.47211\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 15s 164us/step - loss: 0.4715 - categorical_accuracy: 0.7804 - val_loss: 0.4742 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.47211\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4706 - categorical_accuracy: 0.7799 - val_loss: 0.4728 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.47211\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4706 - categorical_accuracy: 0.7798 - val_loss: 0.4726 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47211\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 14s 157us/step - loss: 0.4701 - categorical_accuracy: 0.7802 - val_loss: 0.4726 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.47211\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 15s 164us/step - loss: 0.4691 - categorical_accuracy: 0.7812 - val_loss: 0.4722 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47211\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 15s 160us/step - loss: 0.4692 - categorical_accuracy: 0.7812 - val_loss: 0.4738 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47211\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 15s 164us/step - loss: 0.4686 - categorical_accuracy: 0.7808 - val_loss: 0.4734 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47211\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4659 - categorical_accuracy: 0.7822 - val_loss: 0.4720 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.47211 to 0.47198, saving model to best_model.h5\n",
      "Epoch 39/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4649 - categorical_accuracy: 0.7829 - val_loss: 0.4716 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.47198 to 0.47157, saving model to best_model.h5\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 15s 164us/step - loss: 0.4647 - categorical_accuracy: 0.7813 - val_loss: 0.4715 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.47157 to 0.47153, saving model to best_model.h5\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4646 - categorical_accuracy: 0.7814 - val_loss: 0.4714 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.47153 to 0.47142, saving model to best_model.h5\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4637 - categorical_accuracy: 0.7831 - val_loss: 0.4715 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47142\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4635 - categorical_accuracy: 0.7835 - val_loss: 0.4717 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47142\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 14s 156us/step - loss: 0.4630 - categorical_accuracy: 0.7834 - val_loss: 0.4716 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47142\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 14s 157us/step - loss: 0.4633 - categorical_accuracy: 0.7826 - val_loss: 0.4716 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47142\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 15s 164us/step - loss: 0.4625 - categorical_accuracy: 0.7833 - val_loss: 0.4716 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47142\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 15s 162us/step - loss: 0.4620 - categorical_accuracy: 0.7831 - val_loss: 0.4719 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47142\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4619 - categorical_accuracy: 0.7838 - val_loss: 0.4718 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47142\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 15s 162us/step - loss: 0.4613 - categorical_accuracy: 0.7845 - val_loss: 0.4717 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47142\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 15s 164us/step - loss: 0.4613 - categorical_accuracy: 0.7841 - val_loss: 0.4721 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47142\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4616 - categorical_accuracy: 0.7839 - val_loss: 0.4721 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47142\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 14s 159us/step - loss: 0.4606 - categorical_accuracy: 0.7843 - val_loss: 0.4720 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47142\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 14s 156us/step - loss: 0.4609 - categorical_accuracy: 0.7839 - val_loss: 0.4720 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47142\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 15s 162us/step - loss: 0.4607 - categorical_accuracy: 0.7838 - val_loss: 0.4720 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47142\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 14s 157us/step - loss: 0.4604 - categorical_accuracy: 0.7844 - val_loss: 0.4721 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47142\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 14s 157us/step - loss: 0.4600 - categorical_accuracy: 0.7845 - val_loss: 0.4721 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47142\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 15s 161us/step - loss: 0.4610 - categorical_accuracy: 0.7839 - val_loss: 0.4721 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47142\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 15s 167us/step - loss: 0.4607 - categorical_accuracy: 0.7843 - val_loss: 0.4721 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47142\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 14s 158us/step - loss: 0.4607 - categorical_accuracy: 0.7842 - val_loss: 0.4721 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47142\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 14s 157us/step - loss: 0.4604 - categorical_accuracy: 0.7842 - val_loss: 0.4721 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47142\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 14s 157us/step - loss: 0.4605 - categorical_accuracy: 0.7844 - val_loss: 0.4721 - val_categorical_accuracy: 0.7789\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47142\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00061: early stopping\n",
      "the 3 fold Log-Loss (NN) is 0.471421\n",
      "-----------\n",
      "Loop 1/2 Fold 4/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 23s 253us/step - loss: 0.6236 - categorical_accuracy: 0.6838 - val_loss: 0.5120 - val_categorical_accuracy: 0.7605\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51203, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 15s 161us/step - loss: 0.5178 - categorical_accuracy: 0.7588 - val_loss: 0.5021 - val_categorical_accuracy: 0.7615\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51203 to 0.50209, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 15s 164us/step - loss: 0.5060 - categorical_accuracy: 0.7630 - val_loss: 0.4932 - val_categorical_accuracy: 0.7684\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.50209 to 0.49319, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 15s 159us/step - loss: 0.4984 - categorical_accuracy: 0.7666 - val_loss: 0.4853 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49319 to 0.48534, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 15s 165us/step - loss: 0.4942 - categorical_accuracy: 0.7685 - val_loss: 0.4837 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48534 to 0.48366, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 15s 165us/step - loss: 0.4892 - categorical_accuracy: 0.7712 - val_loss: 0.4819 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.48366 to 0.48187, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 15s 166us/step - loss: 0.4870 - categorical_accuracy: 0.7735 - val_loss: 0.4811 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.48187 to 0.48108, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 15s 161us/step - loss: 0.4856 - categorical_accuracy: 0.7735 - val_loss: 0.4789 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.48108 to 0.47895, saving model to best_model.h5\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 15s 166us/step - loss: 0.4839 - categorical_accuracy: 0.7749 - val_loss: 0.4787 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.47895 to 0.47874, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 15s 167us/step - loss: 0.4826 - categorical_accuracy: 0.7757 - val_loss: 0.4785 - val_categorical_accuracy: 0.7780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_loss improved from 0.47874 to 0.47854, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 15s 167us/step - loss: 0.4815 - categorical_accuracy: 0.7756 - val_loss: 0.4781 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.47854 to 0.47808, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 15s 166us/step - loss: 0.4804 - categorical_accuracy: 0.7766 - val_loss: 0.4762 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.47808 to 0.47619, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 14s 158us/step - loss: 0.4796 - categorical_accuracy: 0.7776 - val_loss: 0.4765 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.47619\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4789 - categorical_accuracy: 0.7771 - val_loss: 0.4770 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.47619\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 15s 166us/step - loss: 0.4787 - categorical_accuracy: 0.7762 - val_loss: 0.4765 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.47619\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 15s 162us/step - loss: 0.4786 - categorical_accuracy: 0.7769 - val_loss: 0.4764 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.47619\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 15s 168us/step - loss: 0.4762 - categorical_accuracy: 0.7771 - val_loss: 0.4755 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.47619 to 0.47550, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 15s 165us/step - loss: 0.4768 - categorical_accuracy: 0.7769 - val_loss: 0.4746 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.47550 to 0.47462, saving model to best_model.h5\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 15s 165us/step - loss: 0.4756 - categorical_accuracy: 0.7784 - val_loss: 0.4747 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.47462\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 15s 164us/step - loss: 0.4755 - categorical_accuracy: 0.7781 - val_loss: 0.4754 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.47462\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 15s 159us/step - loss: 0.4756 - categorical_accuracy: 0.7785 - val_loss: 0.4760 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.47462\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 15s 159us/step - loss: 0.4747 - categorical_accuracy: 0.7782 - val_loss: 0.4745 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.47462 to 0.47454, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 15s 166us/step - loss: 0.4741 - categorical_accuracy: 0.7793 - val_loss: 0.4744 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.47454 to 0.47443, saving model to best_model.h5\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 15s 166us/step - loss: 0.4734 - categorical_accuracy: 0.7791 - val_loss: 0.4741 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.47443 to 0.47406, saving model to best_model.h5\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 15s 166us/step - loss: 0.4733 - categorical_accuracy: 0.7788 - val_loss: 0.4756 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.47406\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 15s 160us/step - loss: 0.4726 - categorical_accuracy: 0.7794 - val_loss: 0.4733 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.47406 to 0.47335, saving model to best_model.h5\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 16s 172us/step - loss: 0.4730 - categorical_accuracy: 0.7797 - val_loss: 0.4750 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.47335\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 15s 166us/step - loss: 0.4721 - categorical_accuracy: 0.7803 - val_loss: 0.4758 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.47335\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 15s 159us/step - loss: 0.4722 - categorical_accuracy: 0.7794 - val_loss: 0.4742 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.47335\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 15s 165us/step - loss: 0.4717 - categorical_accuracy: 0.7800 - val_loss: 0.4739 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.47335\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 15s 166us/step - loss: 0.4709 - categorical_accuracy: 0.7807 - val_loss: 0.4739 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.47335\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 15s 165us/step - loss: 0.4706 - categorical_accuracy: 0.7806 - val_loss: 0.4742 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.47335\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 15s 165us/step - loss: 0.4704 - categorical_accuracy: 0.7802 - val_loss: 0.4727 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.47335 to 0.47273, saving model to best_model.h5\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 15s 166us/step - loss: 0.4698 - categorical_accuracy: 0.7811 - val_loss: 0.4732 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.47273\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 15s 168us/step - loss: 0.4709 - categorical_accuracy: 0.7801 - val_loss: 0.4728 - val_categorical_accuracy: 0.7807\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47273\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 15s 164us/step - loss: 0.4693 - categorical_accuracy: 0.7804 - val_loss: 0.4731 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47273\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 15s 166us/step - loss: 0.4691 - categorical_accuracy: 0.7810 - val_loss: 0.4728 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47273\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 15s 168us/step - loss: 0.4688 - categorical_accuracy: 0.7800 - val_loss: 0.4725 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.47273 to 0.47247, saving model to best_model.h5\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 15s 163us/step - loss: 0.4689 - categorical_accuracy: 0.7811 - val_loss: 0.4758 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.47247\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 12s 129us/step - loss: 0.4685 - categorical_accuracy: 0.7815 - val_loss: 0.4721 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.47247 to 0.47213, saving model to best_model.h5\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 12s 129us/step - loss: 0.4673 - categorical_accuracy: 0.7812 - val_loss: 0.4724 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47213\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 13s 140us/step - loss: 0.4675 - categorical_accuracy: 0.7816 - val_loss: 0.4734 - val_categorical_accuracy: 0.7807\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47213\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4667 - categorical_accuracy: 0.7816 - val_loss: 0.4753 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47213\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4667 - categorical_accuracy: 0.7814 - val_loss: 0.4726 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47213\n",
      "Epoch 45/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 14s 150us/step - loss: 0.4662 - categorical_accuracy: 0.7821 - val_loss: 0.4737 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47213\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 14s 149us/step - loss: 0.4661 - categorical_accuracy: 0.7828 - val_loss: 0.4729 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47213\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4652 - categorical_accuracy: 0.7823 - val_loss: 0.4731 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47213\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4650 - categorical_accuracy: 0.7821 - val_loss: 0.4746 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47213\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4647 - categorical_accuracy: 0.7821 - val_loss: 0.4734 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47213\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 13s 147us/step - loss: 0.4641 - categorical_accuracy: 0.7830 - val_loss: 0.4731 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47213\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 14s 148us/step - loss: 0.4611 - categorical_accuracy: 0.7841 - val_loss: 0.4723 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47213\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4605 - categorical_accuracy: 0.7843 - val_loss: 0.4721 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.47213 to 0.47209, saving model to best_model.h5\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4591 - categorical_accuracy: 0.7846 - val_loss: 0.4721 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.47209 to 0.47205, saving model to best_model.h5\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4600 - categorical_accuracy: 0.7837 - val_loss: 0.4720 - val_categorical_accuracy: 0.7806\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.47205 to 0.47197, saving model to best_model.h5\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 14s 148us/step - loss: 0.4580 - categorical_accuracy: 0.7853 - val_loss: 0.4724 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47197\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 12s 136us/step - loss: 0.4588 - categorical_accuracy: 0.7854 - val_loss: 0.4722 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47197\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 12s 130us/step - loss: 0.4574 - categorical_accuracy: 0.7855 - val_loss: 0.4727 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47197\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 12s 128us/step - loss: 0.4581 - categorical_accuracy: 0.7854 - val_loss: 0.4724 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47197\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 12s 127us/step - loss: 0.4565 - categorical_accuracy: 0.7854 - val_loss: 0.4725 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47197\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 12s 129us/step - loss: 0.4567 - categorical_accuracy: 0.7871 - val_loss: 0.4726 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47197\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 11s 125us/step - loss: 0.4566 - categorical_accuracy: 0.7858 - val_loss: 0.4728 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47197\n",
      "Epoch 62/250\n",
      "91457/91457 [==============================] - 11s 125us/step - loss: 0.4571 - categorical_accuracy: 0.7850 - val_loss: 0.4726 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47197\n",
      "Epoch 63/250\n",
      "91457/91457 [==============================] - 12s 126us/step - loss: 0.4561 - categorical_accuracy: 0.7867 - val_loss: 0.4730 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47197\n",
      "Epoch 64/250\n",
      "91457/91457 [==============================] - 12s 127us/step - loss: 0.4562 - categorical_accuracy: 0.7863 - val_loss: 0.4731 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47197\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 65/250\n",
      "91457/91457 [==============================] - 12s 128us/step - loss: 0.4552 - categorical_accuracy: 0.7862 - val_loss: 0.4731 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47197\n",
      "Epoch 66/250\n",
      "91457/91457 [==============================] - 12s 126us/step - loss: 0.4551 - categorical_accuracy: 0.7854 - val_loss: 0.4730 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47197\n",
      "Epoch 67/250\n",
      "91457/91457 [==============================] - 12s 130us/step - loss: 0.4561 - categorical_accuracy: 0.7858 - val_loss: 0.4730 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.47197\n",
      "Epoch 68/250\n",
      "91457/91457 [==============================] - 12s 127us/step - loss: 0.4553 - categorical_accuracy: 0.7863 - val_loss: 0.4730 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.47197\n",
      "Epoch 69/250\n",
      "91457/91457 [==============================] - 12s 129us/step - loss: 0.4560 - categorical_accuracy: 0.7859 - val_loss: 0.4730 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47197\n",
      "Epoch 70/250\n",
      "91457/91457 [==============================] - 12s 127us/step - loss: 0.4555 - categorical_accuracy: 0.7858 - val_loss: 0.4730 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47197\n",
      "Epoch 71/250\n",
      "91457/91457 [==============================] - 12s 129us/step - loss: 0.4543 - categorical_accuracy: 0.7864 - val_loss: 0.4730 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47197\n",
      "Epoch 72/250\n",
      "91457/91457 [==============================] - 12s 134us/step - loss: 0.4554 - categorical_accuracy: 0.7855 - val_loss: 0.4730 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.47197\n",
      "Epoch 73/250\n",
      "91457/91457 [==============================] - 13s 142us/step - loss: 0.4550 - categorical_accuracy: 0.7867 - val_loss: 0.4730 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47197\n",
      "Epoch 74/250\n",
      "91457/91457 [==============================] - 13s 146us/step - loss: 0.4548 - categorical_accuracy: 0.7857 - val_loss: 0.4730 - val_categorical_accuracy: 0.7797\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.47197\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00074: early stopping\n",
      "the 4 fold Log-Loss (NN) is 0.471966\n",
      "-----------\n",
      "Loop 1/2 Fold 5/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 19s 211us/step - loss: 0.6216 - categorical_accuracy: 0.6909 - val_loss: 0.5151 - val_categorical_accuracy: 0.7630\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51510, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 12s 129us/step - loss: 0.5210 - categorical_accuracy: 0.7558 - val_loss: 0.4975 - val_categorical_accuracy: 0.7633\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51510 to 0.49750, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 12s 127us/step - loss: 0.5061 - categorical_accuracy: 0.7639 - val_loss: 0.4891 - val_categorical_accuracy: 0.7699\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49750 to 0.48907, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 12s 130us/step - loss: 0.5008 - categorical_accuracy: 0.7656 - val_loss: 0.4850 - val_categorical_accuracy: 0.7737\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48907 to 0.48496, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 13s 137us/step - loss: 0.4961 - categorical_accuracy: 0.7691 - val_loss: 0.4868 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48496\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 13s 142us/step - loss: 0.4912 - categorical_accuracy: 0.7705 - val_loss: 0.4799 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.48496 to 0.47991, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4884 - categorical_accuracy: 0.7719 - val_loss: 0.4793 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.47991 to 0.47932, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 13s 142us/step - loss: 0.4856 - categorical_accuracy: 0.7732 - val_loss: 0.4794 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.47932\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4846 - categorical_accuracy: 0.7740 - val_loss: 0.4779 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.47932 to 0.47794, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 13s 142us/step - loss: 0.4835 - categorical_accuracy: 0.7756 - val_loss: 0.4774 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.47794 to 0.47736, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 13s 142us/step - loss: 0.4817 - categorical_accuracy: 0.7757 - val_loss: 0.4761 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.47736 to 0.47613, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4803 - categorical_accuracy: 0.7771 - val_loss: 0.4758 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.47613 to 0.47578, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4795 - categorical_accuracy: 0.7767 - val_loss: 0.4755 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.47578 to 0.47547, saving model to best_model.h5\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 13s 142us/step - loss: 0.4788 - categorical_accuracy: 0.7767 - val_loss: 0.4758 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.47547\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4780 - categorical_accuracy: 0.7774 - val_loss: 0.4751 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.47547 to 0.47505, saving model to best_model.h5\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4778 - categorical_accuracy: 0.7774 - val_loss: 0.4762 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.47505\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4770 - categorical_accuracy: 0.7782 - val_loss: 0.4753 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.47505\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4771 - categorical_accuracy: 0.7772 - val_loss: 0.4745 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.47505 to 0.47448, saving model to best_model.h5\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4763 - categorical_accuracy: 0.7773 - val_loss: 0.4749 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.47448\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4751 - categorical_accuracy: 0.7798 - val_loss: 0.4740 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.47448 to 0.47396, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 13s 145us/step - loss: 0.4748 - categorical_accuracy: 0.7785 - val_loss: 0.4737 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.47396 to 0.47368, saving model to best_model.h5\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 13s 142us/step - loss: 0.4742 - categorical_accuracy: 0.7786 - val_loss: 0.4735 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.47368 to 0.47349, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 13s 142us/step - loss: 0.4739 - categorical_accuracy: 0.7796 - val_loss: 0.4750 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.47349\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 14s 149us/step - loss: 0.4738 - categorical_accuracy: 0.7796 - val_loss: 0.4731 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.47349 to 0.47310, saving model to best_model.h5\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 13s 142us/step - loss: 0.4729 - categorical_accuracy: 0.7785 - val_loss: 0.4739 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.47310\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 13s 142us/step - loss: 0.4734 - categorical_accuracy: 0.7797 - val_loss: 0.4729 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.47310 to 0.47293, saving model to best_model.h5\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4726 - categorical_accuracy: 0.7798 - val_loss: 0.4726 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.47293 to 0.47259, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 13s 142us/step - loss: 0.4722 - categorical_accuracy: 0.7804 - val_loss: 0.4719 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.47259 to 0.47188, saving model to best_model.h5\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4709 - categorical_accuracy: 0.7802 - val_loss: 0.4728 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.47188\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4715 - categorical_accuracy: 0.7802 - val_loss: 0.4723 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.47188\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4716 - categorical_accuracy: 0.7801 - val_loss: 0.4720 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.47188\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 13s 142us/step - loss: 0.4703 - categorical_accuracy: 0.7809 - val_loss: 0.4727 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.47188\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 13s 142us/step - loss: 0.4702 - categorical_accuracy: 0.7809 - val_loss: 0.4733 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47188\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 13s 144us/step - loss: 0.4704 - categorical_accuracy: 0.7803 - val_loss: 0.4740 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.47188\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 13s 143us/step - loss: 0.4701 - categorical_accuracy: 0.7800 - val_loss: 0.4722 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47188\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 11s 124us/step - loss: 0.4689 - categorical_accuracy: 0.7814 - val_loss: 0.4724 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47188\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 9s 93us/step - loss: 0.4689 - categorical_accuracy: 0.7811 - val_loss: 0.4718 - val_categorical_accuracy: 0.7792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00037: val_loss improved from 0.47188 to 0.47181, saving model to best_model.h5\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 7s 80us/step - loss: 0.4687 - categorical_accuracy: 0.7810 - val_loss: 0.4743 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47181\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4652 - categorical_accuracy: 0.7826 - val_loss: 0.4719 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.47181\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4651 - categorical_accuracy: 0.7826 - val_loss: 0.4717 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.47181 to 0.47171, saving model to best_model.h5\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4636 - categorical_accuracy: 0.7836 - val_loss: 0.4719 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47171\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4640 - categorical_accuracy: 0.7832 - val_loss: 0.4716 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.47171 to 0.47158, saving model to best_model.h5\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4636 - categorical_accuracy: 0.7835 - val_loss: 0.4718 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47158\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4640 - categorical_accuracy: 0.7832 - val_loss: 0.4718 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47158\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 7s 75us/step - loss: 0.4624 - categorical_accuracy: 0.7835 - val_loss: 0.4717 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47158\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4629 - categorical_accuracy: 0.7836 - val_loss: 0.4717 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47158\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4620 - categorical_accuracy: 0.7837 - val_loss: 0.4718 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47158\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4619 - categorical_accuracy: 0.7841 - val_loss: 0.4720 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47158\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4614 - categorical_accuracy: 0.7841 - val_loss: 0.4724 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47158\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 7s 71us/step - loss: 0.4618 - categorical_accuracy: 0.7844 - val_loss: 0.4722 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47158\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4603 - categorical_accuracy: 0.7851 - val_loss: 0.4724 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47158\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 6s 65us/step - loss: 0.4610 - categorical_accuracy: 0.7841 - val_loss: 0.4721 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47158\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 6s 71us/step - loss: 0.4605 - categorical_accuracy: 0.7841 - val_loss: 0.4722 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47158\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4609 - categorical_accuracy: 0.7837 - val_loss: 0.4722 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47158\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 7s 73us/step - loss: 0.4606 - categorical_accuracy: 0.7845 - val_loss: 0.4722 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47158\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4604 - categorical_accuracy: 0.7851 - val_loss: 0.4723 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47158\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 6s 65us/step - loss: 0.4605 - categorical_accuracy: 0.7852 - val_loss: 0.4722 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47158\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4603 - categorical_accuracy: 0.7848 - val_loss: 0.4722 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47158\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4606 - categorical_accuracy: 0.7843 - val_loss: 0.4723 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47158\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4601 - categorical_accuracy: 0.7851 - val_loss: 0.4723 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47158\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4603 - categorical_accuracy: 0.7846 - val_loss: 0.4723 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47158\n",
      "Epoch 62/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4606 - categorical_accuracy: 0.7845 - val_loss: 0.4723 - val_categorical_accuracy: 0.7787\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47158\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00062: early stopping\n",
      "the 5 fold Log-Loss (NN) is 0.471583\n",
      "PARTIAL: mean Log-Loss (NN) is 0.472286\n",
      "-----------\n",
      "Loop 2/2 Fold 1/5\n",
      "-----------\n",
      "Train on 91456 samples, validate on 22865 samples\n",
      "Epoch 1/250\n",
      "91456/91456 [==============================] - 8s 90us/step - loss: 0.6231 - categorical_accuracy: 0.6883 - val_loss: 0.5197 - val_categorical_accuracy: 0.7614\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51967, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91456/91456 [==============================] - 7s 72us/step - loss: 0.5180 - categorical_accuracy: 0.7594 - val_loss: 0.5010 - val_categorical_accuracy: 0.7623\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51967 to 0.50099, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.5082 - categorical_accuracy: 0.7634 - val_loss: 0.4966 - val_categorical_accuracy: 0.7668\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.50099 to 0.49664, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91456/91456 [==============================] - 6s 63us/step - loss: 0.4988 - categorical_accuracy: 0.7661 - val_loss: 0.4897 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49664 to 0.48969, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91456/91456 [==============================] - 7s 73us/step - loss: 0.4951 - categorical_accuracy: 0.7684 - val_loss: 0.4915 - val_categorical_accuracy: 0.7733\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48969\n",
      "Epoch 6/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4915 - categorical_accuracy: 0.7689 - val_loss: 0.4871 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.48969 to 0.48707, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "91456/91456 [==============================] - 6s 66us/step - loss: 0.4878 - categorical_accuracy: 0.7724 - val_loss: 0.4852 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.48707 to 0.48516, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4850 - categorical_accuracy: 0.7738 - val_loss: 0.4838 - val_categorical_accuracy: 0.7761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_loss improved from 0.48516 to 0.48376, saving model to best_model.h5\n",
      "Epoch 9/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4823 - categorical_accuracy: 0.7743 - val_loss: 0.4831 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.48376 to 0.48311, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4824 - categorical_accuracy: 0.7739 - val_loss: 0.4837 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48311\n",
      "Epoch 11/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4809 - categorical_accuracy: 0.7759 - val_loss: 0.4813 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.48311 to 0.48130, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4789 - categorical_accuracy: 0.7767 - val_loss: 0.4814 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.48130\n",
      "Epoch 13/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4788 - categorical_accuracy: 0.7760 - val_loss: 0.4810 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.48130 to 0.48103, saving model to best_model.h5\n",
      "Epoch 14/250\n",
      "91456/91456 [==============================] - 7s 71us/step - loss: 0.4783 - categorical_accuracy: 0.7764 - val_loss: 0.4817 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.48103\n",
      "Epoch 15/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4769 - categorical_accuracy: 0.7768 - val_loss: 0.4808 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.48103 to 0.48078, saving model to best_model.h5\n",
      "Epoch 16/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4772 - categorical_accuracy: 0.7767 - val_loss: 0.4801 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.48078 to 0.48012, saving model to best_model.h5\n",
      "Epoch 17/250\n",
      "91456/91456 [==============================] - 7s 71us/step - loss: 0.4757 - categorical_accuracy: 0.7768 - val_loss: 0.4797 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.48012 to 0.47967, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "91456/91456 [==============================] - 7s 72us/step - loss: 0.4751 - categorical_accuracy: 0.7783 - val_loss: 0.4803 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.47967\n",
      "Epoch 19/250\n",
      "91456/91456 [==============================] - 6s 67us/step - loss: 0.4753 - categorical_accuracy: 0.7784 - val_loss: 0.4790 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.47967 to 0.47896, saving model to best_model.h5\n",
      "Epoch 20/250\n",
      "91456/91456 [==============================] - 7s 72us/step - loss: 0.4745 - categorical_accuracy: 0.7790 - val_loss: 0.4813 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.47896\n",
      "Epoch 21/250\n",
      "91456/91456 [==============================] - 6s 66us/step - loss: 0.4746 - categorical_accuracy: 0.7786 - val_loss: 0.4793 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.47896\n",
      "Epoch 22/250\n",
      "91456/91456 [==============================] - 6s 66us/step - loss: 0.4729 - categorical_accuracy: 0.7792 - val_loss: 0.4794 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.47896\n",
      "Epoch 23/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4725 - categorical_accuracy: 0.7793 - val_loss: 0.4792 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.47896\n",
      "Epoch 24/250\n",
      "91456/91456 [==============================] - 6s 66us/step - loss: 0.4722 - categorical_accuracy: 0.7802 - val_loss: 0.4791 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.47896\n",
      "Epoch 25/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4714 - categorical_accuracy: 0.7796 - val_loss: 0.4802 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.47896\n",
      "Epoch 26/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4718 - categorical_accuracy: 0.7794 - val_loss: 0.4794 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.47896\n",
      "Epoch 27/250\n",
      "91456/91456 [==============================] - 6s 64us/step - loss: 0.4710 - categorical_accuracy: 0.7806 - val_loss: 0.4785 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.47896 to 0.47850, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "91456/91456 [==============================] - 6s 71us/step - loss: 0.4705 - categorical_accuracy: 0.7797 - val_loss: 0.4792 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.47850\n",
      "Epoch 29/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4698 - categorical_accuracy: 0.7810 - val_loss: 0.4797 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.47850\n",
      "Epoch 30/250\n",
      "91456/91456 [==============================] - 6s 67us/step - loss: 0.4701 - categorical_accuracy: 0.7803 - val_loss: 0.4786 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.47850\n",
      "Epoch 31/250\n",
      "91456/91456 [==============================] - 6s 65us/step - loss: 0.4691 - categorical_accuracy: 0.7804 - val_loss: 0.4794 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.47850\n",
      "Epoch 32/250\n",
      "91456/91456 [==============================] - 6s 67us/step - loss: 0.4692 - categorical_accuracy: 0.7802 - val_loss: 0.4791 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.47850\n",
      "Epoch 33/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4685 - categorical_accuracy: 0.7811 - val_loss: 0.4823 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47850\n",
      "Epoch 34/250\n",
      "91456/91456 [==============================] - 6s 65us/step - loss: 0.4688 - categorical_accuracy: 0.7806 - val_loss: 0.4804 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.47850\n",
      "Epoch 35/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4679 - categorical_accuracy: 0.7809 - val_loss: 0.4802 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47850\n",
      "Epoch 36/250\n",
      "91456/91456 [==============================] - 6s 71us/step - loss: 0.4674 - categorical_accuracy: 0.7798 - val_loss: 0.4784 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.47850 to 0.47841, saving model to best_model.h5\n",
      "Epoch 37/250\n",
      "91456/91456 [==============================] - 6s 63us/step - loss: 0.4663 - categorical_accuracy: 0.7819 - val_loss: 0.4800 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47841\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 38/250\n",
      "91456/91456 [==============================] - 7s 73us/step - loss: 0.4643 - categorical_accuracy: 0.7826 - val_loss: 0.4788 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47841\n",
      "Epoch 39/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4629 - categorical_accuracy: 0.7835 - val_loss: 0.4784 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.47841 to 0.47838, saving model to best_model.h5\n",
      "Epoch 40/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4627 - categorical_accuracy: 0.7839 - val_loss: 0.4779 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.47838 to 0.47792, saving model to best_model.h5\n",
      "Epoch 41/250\n",
      "91456/91456 [==============================] - 6s 71us/step - loss: 0.4622 - categorical_accuracy: 0.7838 - val_loss: 0.4782 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47792\n",
      "Epoch 42/250\n",
      "91456/91456 [==============================] - 6s 67us/step - loss: 0.4621 - categorical_accuracy: 0.7838 - val_loss: 0.4783 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47792\n",
      "Epoch 43/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91456/91456 [==============================] - 7s 73us/step - loss: 0.4619 - categorical_accuracy: 0.7835 - val_loss: 0.4785 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47792\n",
      "Epoch 44/250\n",
      "91456/91456 [==============================] - 6s 64us/step - loss: 0.4613 - categorical_accuracy: 0.7832 - val_loss: 0.4789 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47792\n",
      "Epoch 45/250\n",
      "91456/91456 [==============================] - 7s 72us/step - loss: 0.4620 - categorical_accuracy: 0.7835 - val_loss: 0.4790 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47792\n",
      "Epoch 46/250\n",
      "91456/91456 [==============================] - 6s 64us/step - loss: 0.4615 - categorical_accuracy: 0.7839 - val_loss: 0.4787 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47792\n",
      "Epoch 47/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4611 - categorical_accuracy: 0.7839 - val_loss: 0.4788 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47792\n",
      "Epoch 48/250\n",
      "91456/91456 [==============================] - 6s 69us/step - loss: 0.4608 - categorical_accuracy: 0.7836 - val_loss: 0.4788 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47792\n",
      "Epoch 49/250\n",
      "91456/91456 [==============================] - 6s 67us/step - loss: 0.4598 - categorical_accuracy: 0.7842 - val_loss: 0.4791 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47792\n",
      "Epoch 50/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4599 - categorical_accuracy: 0.7848 - val_loss: 0.4787 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47792\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 51/250\n",
      "91456/91456 [==============================] - 7s 71us/step - loss: 0.4591 - categorical_accuracy: 0.7850 - val_loss: 0.4789 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47792\n",
      "Epoch 52/250\n",
      "91456/91456 [==============================] - 7s 72us/step - loss: 0.4595 - categorical_accuracy: 0.7849 - val_loss: 0.4790 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47792\n",
      "Epoch 53/250\n",
      "91456/91456 [==============================] - 7s 72us/step - loss: 0.4590 - categorical_accuracy: 0.7844 - val_loss: 0.4792 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47792\n",
      "Epoch 54/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4598 - categorical_accuracy: 0.7853 - val_loss: 0.4792 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47792\n",
      "Epoch 55/250\n",
      "91456/91456 [==============================] - 6s 68us/step - loss: 0.4592 - categorical_accuracy: 0.7836 - val_loss: 0.4791 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47792\n",
      "Epoch 56/250\n",
      "91456/91456 [==============================] - 6s 71us/step - loss: 0.4593 - categorical_accuracy: 0.7850 - val_loss: 0.4792 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47792\n",
      "Epoch 57/250\n",
      "91456/91456 [==============================] - 6s 70us/step - loss: 0.4597 - categorical_accuracy: 0.7845 - val_loss: 0.4792 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47792\n",
      "Epoch 58/250\n",
      "91456/91456 [==============================] - 7s 71us/step - loss: 0.4592 - categorical_accuracy: 0.7844 - val_loss: 0.4793 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47792\n",
      "Epoch 59/250\n",
      "91456/91456 [==============================] - 6s 66us/step - loss: 0.4592 - categorical_accuracy: 0.7846 - val_loss: 0.4793 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47792\n",
      "Epoch 60/250\n",
      "91456/91456 [==============================] - 7s 72us/step - loss: 0.4597 - categorical_accuracy: 0.7844 - val_loss: 0.4793 - val_categorical_accuracy: 0.7787\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47792\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 00060: early stopping\n",
      "the 1 fold Log-Loss (NN) is 0.477924\n",
      "-----------\n",
      "Loop 2/2 Fold 2/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 9s 97us/step - loss: 0.6250 - categorical_accuracy: 0.6836 - val_loss: 0.5065 - val_categorical_accuracy: 0.7654\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50647, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.5226 - categorical_accuracy: 0.7549 - val_loss: 0.4915 - val_categorical_accuracy: 0.7666\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.50647 to 0.49150, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.5096 - categorical_accuracy: 0.7626 - val_loss: 0.4904 - val_categorical_accuracy: 0.7709\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49150 to 0.49040, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.5016 - categorical_accuracy: 0.7647 - val_loss: 0.4827 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49040 to 0.48269, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4983 - categorical_accuracy: 0.7662 - val_loss: 0.4798 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48269 to 0.47981, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 6s 65us/step - loss: 0.4935 - categorical_accuracy: 0.7692 - val_loss: 0.4760 - val_categorical_accuracy: 0.7807\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.47981 to 0.47605, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4914 - categorical_accuracy: 0.7701 - val_loss: 0.4764 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.47605\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4887 - categorical_accuracy: 0.7715 - val_loss: 0.4747 - val_categorical_accuracy: 0.7816\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.47605 to 0.47466, saving model to best_model.h5\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 7s 74us/step - loss: 0.4856 - categorical_accuracy: 0.7739 - val_loss: 0.4734 - val_categorical_accuracy: 0.7818\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.47466 to 0.47338, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 7s 73us/step - loss: 0.4839 - categorical_accuracy: 0.7740 - val_loss: 0.4736 - val_categorical_accuracy: 0.7819\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.47338\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4839 - categorical_accuracy: 0.7741 - val_loss: 0.4726 - val_categorical_accuracy: 0.7816\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.47338 to 0.47258, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4821 - categorical_accuracy: 0.7753 - val_loss: 0.4725 - val_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.47258 to 0.47249, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4811 - categorical_accuracy: 0.7758 - val_loss: 0.4715 - val_categorical_accuracy: 0.7818\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.47249 to 0.47152, saving model to best_model.h5\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 7s 71us/step - loss: 0.4810 - categorical_accuracy: 0.7753 - val_loss: 0.4713 - val_categorical_accuracy: 0.7827\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.47152 to 0.47127, saving model to best_model.h5\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4788 - categorical_accuracy: 0.7758 - val_loss: 0.4723 - val_categorical_accuracy: 0.7812\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.47127\n",
      "Epoch 16/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4788 - categorical_accuracy: 0.7766 - val_loss: 0.4714 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.47127\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 6s 63us/step - loss: 0.4779 - categorical_accuracy: 0.7780 - val_loss: 0.4705 - val_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.47127 to 0.47052, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 7s 73us/step - loss: 0.4774 - categorical_accuracy: 0.7777 - val_loss: 0.4698 - val_categorical_accuracy: 0.7824\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.47052 to 0.46985, saving model to best_model.h5\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 6s 71us/step - loss: 0.4773 - categorical_accuracy: 0.7768 - val_loss: 0.4690 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.46985 to 0.46903, saving model to best_model.h5\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4764 - categorical_accuracy: 0.7776 - val_loss: 0.4694 - val_categorical_accuracy: 0.7825\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.46903\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4759 - categorical_accuracy: 0.7771 - val_loss: 0.4695 - val_categorical_accuracy: 0.7814\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.46903\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4755 - categorical_accuracy: 0.7788 - val_loss: 0.4689 - val_categorical_accuracy: 0.7832\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.46903 to 0.46889, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4753 - categorical_accuracy: 0.7776 - val_loss: 0.4703 - val_categorical_accuracy: 0.7824\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.46889\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4745 - categorical_accuracy: 0.7788 - val_loss: 0.4693 - val_categorical_accuracy: 0.7825\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.46889\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4747 - categorical_accuracy: 0.7780 - val_loss: 0.4693 - val_categorical_accuracy: 0.7814\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.46889\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4740 - categorical_accuracy: 0.7780 - val_loss: 0.4693 - val_categorical_accuracy: 0.7821\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.46889\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 7s 71us/step - loss: 0.4738 - categorical_accuracy: 0.7786 - val_loss: 0.4690 - val_categorical_accuracy: 0.7823\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.46889\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4733 - categorical_accuracy: 0.7786 - val_loss: 0.4687 - val_categorical_accuracy: 0.7812\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.46889 to 0.46872, saving model to best_model.h5\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4731 - categorical_accuracy: 0.7794 - val_loss: 0.4686 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.46872 to 0.46861, saving model to best_model.h5\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4727 - categorical_accuracy: 0.7794 - val_loss: 0.4685 - val_categorical_accuracy: 0.7826\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.46861 to 0.46847, saving model to best_model.h5\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4728 - categorical_accuracy: 0.7798 - val_loss: 0.4686 - val_categorical_accuracy: 0.7825\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.46847\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 6s 71us/step - loss: 0.4720 - categorical_accuracy: 0.7806 - val_loss: 0.4690 - val_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.46847\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4717 - categorical_accuracy: 0.7797 - val_loss: 0.4685 - val_categorical_accuracy: 0.7829\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.46847\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4715 - categorical_accuracy: 0.7790 - val_loss: 0.4702 - val_categorical_accuracy: 0.7810\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.46847\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4710 - categorical_accuracy: 0.7802 - val_loss: 0.4693 - val_categorical_accuracy: 0.7821\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.46847\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4706 - categorical_accuracy: 0.7796 - val_loss: 0.4691 - val_categorical_accuracy: 0.7815\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.46847\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4699 - categorical_accuracy: 0.7803 - val_loss: 0.4673 - val_categorical_accuracy: 0.7828\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.46847 to 0.46727, saving model to best_model.h5\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 6s 64us/step - loss: 0.4703 - categorical_accuracy: 0.7803 - val_loss: 0.4680 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.46727\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4697 - categorical_accuracy: 0.7811 - val_loss: 0.4679 - val_categorical_accuracy: 0.7816\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.46727\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 6s 71us/step - loss: 0.4688 - categorical_accuracy: 0.7803 - val_loss: 0.4686 - val_categorical_accuracy: 0.7825\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.46727\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4691 - categorical_accuracy: 0.7810 - val_loss: 0.4679 - val_categorical_accuracy: 0.7829\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.46727\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4687 - categorical_accuracy: 0.7801 - val_loss: 0.4691 - val_categorical_accuracy: 0.7817\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.46727\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 6s 65us/step - loss: 0.4685 - categorical_accuracy: 0.7805 - val_loss: 0.4690 - val_categorical_accuracy: 0.7824\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.46727\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 7s 74us/step - loss: 0.4672 - categorical_accuracy: 0.7806 - val_loss: 0.4684 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.46727\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4672 - categorical_accuracy: 0.7813 - val_loss: 0.4690 - val_categorical_accuracy: 0.7836\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.46727\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 7s 73us/step - loss: 0.4665 - categorical_accuracy: 0.7810 - val_loss: 0.4674 - val_categorical_accuracy: 0.7830\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.46727\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 6s 71us/step - loss: 0.4662 - categorical_accuracy: 0.7816 - val_loss: 0.4677 - val_categorical_accuracy: 0.7825\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.46727\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 6s 64us/step - loss: 0.4634 - categorical_accuracy: 0.7821 - val_loss: 0.4669 - val_categorical_accuracy: 0.7824\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.46727 to 0.46693, saving model to best_model.h5\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4627 - categorical_accuracy: 0.7831 - val_loss: 0.4669 - val_categorical_accuracy: 0.7826\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.46693 to 0.46692, saving model to best_model.h5\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 6s 65us/step - loss: 0.4614 - categorical_accuracy: 0.7834 - val_loss: 0.4669 - val_categorical_accuracy: 0.7825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00050: val_loss improved from 0.46692 to 0.46691, saving model to best_model.h5\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 6s 71us/step - loss: 0.4615 - categorical_accuracy: 0.7830 - val_loss: 0.4671 - val_categorical_accuracy: 0.7827\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.46691\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4614 - categorical_accuracy: 0.7843 - val_loss: 0.4671 - val_categorical_accuracy: 0.7825\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.46691\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4598 - categorical_accuracy: 0.7837 - val_loss: 0.4671 - val_categorical_accuracy: 0.7821\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.46691\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4604 - categorical_accuracy: 0.7842 - val_loss: 0.4671 - val_categorical_accuracy: 0.7825\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.46691\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4599 - categorical_accuracy: 0.7844 - val_loss: 0.4672 - val_categorical_accuracy: 0.7821\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.46691\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4594 - categorical_accuracy: 0.7846 - val_loss: 0.4673 - val_categorical_accuracy: 0.7823\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.46691\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4596 - categorical_accuracy: 0.7848 - val_loss: 0.4676 - val_categorical_accuracy: 0.7826\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.46691\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4590 - categorical_accuracy: 0.7846 - val_loss: 0.4673 - val_categorical_accuracy: 0.7820\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.46691\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 6s 65us/step - loss: 0.4584 - categorical_accuracy: 0.7847 - val_loss: 0.4674 - val_categorical_accuracy: 0.7819\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.46691\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4583 - categorical_accuracy: 0.7852 - val_loss: 0.4675 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.46691\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4580 - categorical_accuracy: 0.7860 - val_loss: 0.4675 - val_categorical_accuracy: 0.7823\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.46691\n",
      "Epoch 62/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4589 - categorical_accuracy: 0.7839 - val_loss: 0.4675 - val_categorical_accuracy: 0.7824\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.46691\n",
      "Epoch 63/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4585 - categorical_accuracy: 0.7855 - val_loss: 0.4675 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.46691\n",
      "Epoch 64/250\n",
      "91457/91457 [==============================] - 6s 64us/step - loss: 0.4582 - categorical_accuracy: 0.7853 - val_loss: 0.4676 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.46691\n",
      "Epoch 65/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4575 - categorical_accuracy: 0.7846 - val_loss: 0.4677 - val_categorical_accuracy: 0.7823\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.46691\n",
      "Epoch 66/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4588 - categorical_accuracy: 0.7845 - val_loss: 0.4676 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.46691\n",
      "Epoch 67/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4579 - categorical_accuracy: 0.7844 - val_loss: 0.4676 - val_categorical_accuracy: 0.7823\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.46691\n",
      "Epoch 68/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4583 - categorical_accuracy: 0.7848 - val_loss: 0.4676 - val_categorical_accuracy: 0.7821\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.46691\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 69/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4584 - categorical_accuracy: 0.7855 - val_loss: 0.4676 - val_categorical_accuracy: 0.7821\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.46691\n",
      "Epoch 70/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4587 - categorical_accuracy: 0.7839 - val_loss: 0.4676 - val_categorical_accuracy: 0.7821\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.46691\n",
      "Epoch 00070: early stopping\n",
      "the 2 fold Log-Loss (NN) is 0.466911\n",
      "-----------\n",
      "Loop 2/2 Fold 3/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 9s 94us/step - loss: 0.6233 - categorical_accuracy: 0.6885 - val_loss: 0.5110 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51099, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 7s 73us/step - loss: 0.5209 - categorical_accuracy: 0.7575 - val_loss: 0.5007 - val_categorical_accuracy: 0.7596\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51099 to 0.50072, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.5067 - categorical_accuracy: 0.7645 - val_loss: 0.4916 - val_categorical_accuracy: 0.7654\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.50072 to 0.49156, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 6s 63us/step - loss: 0.5019 - categorical_accuracy: 0.7662 - val_loss: 0.4887 - val_categorical_accuracy: 0.7652\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49156 to 0.48866, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4972 - categorical_accuracy: 0.7690 - val_loss: 0.4822 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48866 to 0.48222, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4929 - categorical_accuracy: 0.7702 - val_loss: 0.4826 - val_categorical_accuracy: 0.7737\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48222\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 6s 62us/step - loss: 0.4902 - categorical_accuracy: 0.7725 - val_loss: 0.4801 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.48222 to 0.48007, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4872 - categorical_accuracy: 0.7732 - val_loss: 0.4788 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.48007 to 0.47878, saving model to best_model.h5\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 7s 71us/step - loss: 0.4851 - categorical_accuracy: 0.7746 - val_loss: 0.4787 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.47878 to 0.47866, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 7s 74us/step - loss: 0.4843 - categorical_accuracy: 0.7744 - val_loss: 0.4782 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.47866 to 0.47815, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4810 - categorical_accuracy: 0.7752 - val_loss: 0.4785 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.47815\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4802 - categorical_accuracy: 0.7763 - val_loss: 0.4769 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.47815 to 0.47694, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4803 - categorical_accuracy: 0.7770 - val_loss: 0.4789 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.47694\n",
      "Epoch 14/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4790 - categorical_accuracy: 0.7778 - val_loss: 0.4768 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.47694 to 0.47684, saving model to best_model.h5\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4785 - categorical_accuracy: 0.7768 - val_loss: 0.4766 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.47684 to 0.47658, saving model to best_model.h5\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4782 - categorical_accuracy: 0.7772 - val_loss: 0.4751 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.47658 to 0.47506, saving model to best_model.h5\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 6s 61us/step - loss: 0.4771 - categorical_accuracy: 0.7780 - val_loss: 0.4757 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.47506\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4770 - categorical_accuracy: 0.7785 - val_loss: 0.4744 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.47506 to 0.47437, saving model to best_model.h5\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4752 - categorical_accuracy: 0.7787 - val_loss: 0.4743 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.47437 to 0.47433, saving model to best_model.h5\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 6s 71us/step - loss: 0.4764 - categorical_accuracy: 0.7776 - val_loss: 0.4750 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.47433\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4751 - categorical_accuracy: 0.7788 - val_loss: 0.4746 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.47433\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4745 - categorical_accuracy: 0.7791 - val_loss: 0.4741 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.47433 to 0.47409, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4742 - categorical_accuracy: 0.7786 - val_loss: 0.4752 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.47409\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4741 - categorical_accuracy: 0.7788 - val_loss: 0.4748 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.47409\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4739 - categorical_accuracy: 0.7791 - val_loss: 0.4740 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.47409 to 0.47402, saving model to best_model.h5\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 7s 71us/step - loss: 0.4728 - categorical_accuracy: 0.7802 - val_loss: 0.4733 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.47402 to 0.47332, saving model to best_model.h5\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4724 - categorical_accuracy: 0.7800 - val_loss: 0.4743 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.47332\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4724 - categorical_accuracy: 0.7794 - val_loss: 0.4730 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.47332 to 0.47296, saving model to best_model.h5\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4718 - categorical_accuracy: 0.7804 - val_loss: 0.4747 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.47296\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 6s 64us/step - loss: 0.4717 - categorical_accuracy: 0.7797 - val_loss: 0.4734 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.47296\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 6s 71us/step - loss: 0.4711 - categorical_accuracy: 0.7801 - val_loss: 0.4745 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.47296\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4713 - categorical_accuracy: 0.7807 - val_loss: 0.4729 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.47296 to 0.47289, saving model to best_model.h5\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4705 - categorical_accuracy: 0.7817 - val_loss: 0.4737 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47289\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4694 - categorical_accuracy: 0.7818 - val_loss: 0.4753 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.47289\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4697 - categorical_accuracy: 0.7803 - val_loss: 0.4741 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47289\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4692 - categorical_accuracy: 0.7803 - val_loss: 0.4737 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47289\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4691 - categorical_accuracy: 0.7821 - val_loss: 0.4734 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47289\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 7s 73us/step - loss: 0.4685 - categorical_accuracy: 0.7817 - val_loss: 0.4737 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47289\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4653 - categorical_accuracy: 0.7826 - val_loss: 0.4725 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.47289 to 0.47248, saving model to best_model.h5\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4643 - categorical_accuracy: 0.7827 - val_loss: 0.4721 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.47248 to 0.47207, saving model to best_model.h5\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4640 - categorical_accuracy: 0.7835 - val_loss: 0.4722 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47207\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 6s 65us/step - loss: 0.4636 - categorical_accuracy: 0.7833 - val_loss: 0.4722 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47207\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4637 - categorical_accuracy: 0.7836 - val_loss: 0.4724 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47207\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 6s 62us/step - loss: 0.4635 - categorical_accuracy: 0.7833 - val_loss: 0.4723 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47207\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4637 - categorical_accuracy: 0.7830 - val_loss: 0.4723 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47207\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4626 - categorical_accuracy: 0.7841 - val_loss: 0.4721 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.47207 to 0.47207, saving model to best_model.h5\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 6s 64us/step - loss: 0.4624 - categorical_accuracy: 0.7842 - val_loss: 0.4723 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47207\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 6s 64us/step - loss: 0.4624 - categorical_accuracy: 0.7842 - val_loss: 0.4722 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47207\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4615 - categorical_accuracy: 0.7838 - val_loss: 0.4723 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47207\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4619 - categorical_accuracy: 0.7845 - val_loss: 0.4724 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47207\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 6s 64us/step - loss: 0.4616 - categorical_accuracy: 0.7847 - val_loss: 0.4723 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47207\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 6s 63us/step - loss: 0.4610 - categorical_accuracy: 0.7845 - val_loss: 0.4722 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47207\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4620 - categorical_accuracy: 0.7847 - val_loss: 0.4722 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47207\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 7s 73us/step - loss: 0.4614 - categorical_accuracy: 0.7842 - val_loss: 0.4723 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47207\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4608 - categorical_accuracy: 0.7841 - val_loss: 0.4723 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47207\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 6s 64us/step - loss: 0.4613 - categorical_accuracy: 0.7836 - val_loss: 0.4723 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47207\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4608 - categorical_accuracy: 0.7845 - val_loss: 0.4722 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47207\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 6s 65us/step - loss: 0.4605 - categorical_accuracy: 0.7842 - val_loss: 0.4723 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47207\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 6s 64us/step - loss: 0.4609 - categorical_accuracy: 0.7845 - val_loss: 0.4723 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47207\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4605 - categorical_accuracy: 0.7859 - val_loss: 0.4723 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47207\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 6s 61us/step - loss: 0.4606 - categorical_accuracy: 0.7843 - val_loss: 0.4723 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47207\n",
      "Epoch 62/250\n",
      "91457/91457 [==============================] - 6s 65us/step - loss: 0.4608 - categorical_accuracy: 0.7852 - val_loss: 0.4723 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47207\n",
      "Epoch 63/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.4604 - categorical_accuracy: 0.7847 - val_loss: 0.4723 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47207\n",
      "Epoch 64/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4606 - categorical_accuracy: 0.7851 - val_loss: 0.4723 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47207\n",
      "Epoch 65/250\n",
      "91457/91457 [==============================] - 6s 67us/step - loss: 0.4612 - categorical_accuracy: 0.7845 - val_loss: 0.4723 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47207\n",
      "Epoch 66/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4603 - categorical_accuracy: 0.7856 - val_loss: 0.4723 - val_categorical_accuracy: 0.7790\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47207\n",
      "Epoch 00066: early stopping\n",
      "the 3 fold Log-Loss (NN) is 0.472071\n",
      "-----------\n",
      "Loop 2/2 Fold 4/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 8s 84us/step - loss: 0.6369 - categorical_accuracy: 0.6828 - val_loss: 0.5130 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51298, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 6s 68us/step - loss: 0.5245 - categorical_accuracy: 0.7537 - val_loss: 0.4975 - val_categorical_accuracy: 0.7625\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51298 to 0.49753, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.5101 - categorical_accuracy: 0.7621 - val_loss: 0.4928 - val_categorical_accuracy: 0.7634\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49753 to 0.49283, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.5011 - categorical_accuracy: 0.7657 - val_loss: 0.4850 - val_categorical_accuracy: 0.7675\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49283 to 0.48503, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 7s 76us/step - loss: 0.4971 - categorical_accuracy: 0.7674 - val_loss: 0.4825 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48503 to 0.48246, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4926 - categorical_accuracy: 0.7696 - val_loss: 0.4811 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.48246 to 0.48111, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 7s 73us/step - loss: 0.4903 - categorical_accuracy: 0.7708 - val_loss: 0.4790 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.48111 to 0.47899, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4881 - categorical_accuracy: 0.7721 - val_loss: 0.4784 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.47899 to 0.47838, saving model to best_model.h5\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4857 - categorical_accuracy: 0.7740 - val_loss: 0.4765 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.47838 to 0.47654, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 6s 71us/step - loss: 0.4849 - categorical_accuracy: 0.7737 - val_loss: 0.4765 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.47654 to 0.47652, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4831 - categorical_accuracy: 0.7747 - val_loss: 0.4750 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.47652 to 0.47500, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4825 - categorical_accuracy: 0.7749 - val_loss: 0.4750 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.47500\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 7s 73us/step - loss: 0.4806 - categorical_accuracy: 0.7767 - val_loss: 0.4747 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.47500 to 0.47470, saving model to best_model.h5\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 6s 69us/step - loss: 0.4803 - categorical_accuracy: 0.7759 - val_loss: 0.4773 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.47470\n",
      "Epoch 15/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4795 - categorical_accuracy: 0.7764 - val_loss: 0.4726 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.47470 to 0.47262, saving model to best_model.h5\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 7s 72us/step - loss: 0.4782 - categorical_accuracy: 0.7777 - val_loss: 0.4728 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.47262\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 6s 70us/step - loss: 0.4778 - categorical_accuracy: 0.7782 - val_loss: 0.4729 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.47262\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 6s 64us/step - loss: 0.4781 - categorical_accuracy: 0.7776 - val_loss: 0.4724 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.47262 to 0.47243, saving model to best_model.h5\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4770 - categorical_accuracy: 0.7781 - val_loss: 0.4728 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.47243\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 6s 66us/step - loss: 0.4770 - categorical_accuracy: 0.7780 - val_loss: 0.4718 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.47243 to 0.47176, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 6s 64us/step - loss: 0.4762 - categorical_accuracy: 0.7784 - val_loss: 0.4718 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.47176\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 5s 58us/step - loss: 0.4757 - categorical_accuracy: 0.7787 - val_loss: 0.4713 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.47176 to 0.47130, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 5s 57us/step - loss: 0.4753 - categorical_accuracy: 0.7790 - val_loss: 0.4726 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.47130\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 5s 57us/step - loss: 0.4741 - categorical_accuracy: 0.7786 - val_loss: 0.4716 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.47130\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 5s 57us/step - loss: 0.4744 - categorical_accuracy: 0.7800 - val_loss: 0.4730 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.47130\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 5s 59us/step - loss: 0.4737 - categorical_accuracy: 0.7796 - val_loss: 0.4710 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.47130 to 0.47103, saving model to best_model.h5\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 5s 57us/step - loss: 0.4735 - categorical_accuracy: 0.7792 - val_loss: 0.4716 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.47103\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 5s 57us/step - loss: 0.4726 - categorical_accuracy: 0.7804 - val_loss: 0.4714 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.47103\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 5s 59us/step - loss: 0.4728 - categorical_accuracy: 0.7798 - val_loss: 0.4716 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.47103\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 5s 58us/step - loss: 0.4725 - categorical_accuracy: 0.7804 - val_loss: 0.4713 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.47103\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 6s 61us/step - loss: 0.4721 - categorical_accuracy: 0.7803 - val_loss: 0.4719 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.47103\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 5s 58us/step - loss: 0.4716 - categorical_accuracy: 0.7799 - val_loss: 0.4710 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.47103 to 0.47103, saving model to best_model.h5\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 5s 57us/step - loss: 0.4714 - categorical_accuracy: 0.7804 - val_loss: 0.4711 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47103\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 5s 60us/step - loss: 0.4711 - categorical_accuracy: 0.7799 - val_loss: 0.4710 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.47103\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 5s 59us/step - loss: 0.4716 - categorical_accuracy: 0.7802 - val_loss: 0.4721 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47103\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 5s 59us/step - loss: 0.4700 - categorical_accuracy: 0.7804 - val_loss: 0.4709 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.47103 to 0.47094, saving model to best_model.h5\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 5s 59us/step - loss: 0.4673 - categorical_accuracy: 0.7819 - val_loss: 0.4698 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.47094 to 0.46978, saving model to best_model.h5\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 5s 57us/step - loss: 0.4659 - categorical_accuracy: 0.7829 - val_loss: 0.4695 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.46978 to 0.46946, saving model to best_model.h5\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 5s 58us/step - loss: 0.4653 - categorical_accuracy: 0.7826 - val_loss: 0.4695 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.46946\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4652 - categorical_accuracy: 0.7823 - val_loss: 0.4695 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.46946\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 4s 48us/step - loss: 0.4649 - categorical_accuracy: 0.7829 - val_loss: 0.4694 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.46946 to 0.46942, saving model to best_model.h5\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 4s 46us/step - loss: 0.4656 - categorical_accuracy: 0.7820 - val_loss: 0.4696 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.46942\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 4s 41us/step - loss: 0.4642 - categorical_accuracy: 0.7834 - val_loss: 0.4696 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.46942\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 4s 43us/step - loss: 0.4649 - categorical_accuracy: 0.7830 - val_loss: 0.4695 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.46942\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 4s 39us/step - loss: 0.4644 - categorical_accuracy: 0.7830 - val_loss: 0.4696 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.46942\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 3s 37us/step - loss: 0.4646 - categorical_accuracy: 0.7830 - val_loss: 0.4696 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.46942\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 3s 37us/step - loss: 0.4637 - categorical_accuracy: 0.7831 - val_loss: 0.4698 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.46942\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 3s 37us/step - loss: 0.4641 - categorical_accuracy: 0.7838 - val_loss: 0.4697 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.46942\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 3s 37us/step - loss: 0.4634 - categorical_accuracy: 0.7834 - val_loss: 0.4697 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.46942\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 4s 39us/step - loss: 0.4631 - categorical_accuracy: 0.7836 - val_loss: 0.4696 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.46942\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4631 - categorical_accuracy: 0.7845 - val_loss: 0.4696 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.46942\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4627 - categorical_accuracy: 0.7839 - val_loss: 0.4696 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.46942\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4632 - categorical_accuracy: 0.7842 - val_loss: 0.4697 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.46942\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4627 - categorical_accuracy: 0.7841 - val_loss: 0.4696 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.46942\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4632 - categorical_accuracy: 0.7840 - val_loss: 0.4696 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.46942\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 5s 53us/step - loss: 0.4631 - categorical_accuracy: 0.7833 - val_loss: 0.4696 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.46942\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4634 - categorical_accuracy: 0.7837 - val_loss: 0.4697 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.46942\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 5s 53us/step - loss: 0.4640 - categorical_accuracy: 0.7836 - val_loss: 0.4697 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.46942\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4625 - categorical_accuracy: 0.7840 - val_loss: 0.4697 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.46942\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4628 - categorical_accuracy: 0.7830 - val_loss: 0.4697 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.46942\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4629 - categorical_accuracy: 0.7836 - val_loss: 0.4697 - val_categorical_accuracy: 0.7794\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.46942\n",
      "Epoch 00061: early stopping\n",
      "the 4 fold Log-Loss (NN) is 0.469417\n",
      "-----------\n",
      "Loop 2/2 Fold 5/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 6s 65us/step - loss: 0.6181 - categorical_accuracy: 0.6860 - val_loss: 0.5124 - val_categorical_accuracy: 0.7576\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51241, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.5193 - categorical_accuracy: 0.7573 - val_loss: 0.5041 - val_categorical_accuracy: 0.7588\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51241 to 0.50410, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.5048 - categorical_accuracy: 0.7644 - val_loss: 0.4930 - val_categorical_accuracy: 0.7685\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.50410 to 0.49296, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 5s 53us/step - loss: 0.4966 - categorical_accuracy: 0.7681 - val_loss: 0.4926 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49296 to 0.49260, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4917 - categorical_accuracy: 0.7708 - val_loss: 0.4853 - val_categorical_accuracy: 0.7733\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49260 to 0.48534, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 5s 57us/step - loss: 0.4882 - categorical_accuracy: 0.7722 - val_loss: 0.4843 - val_categorical_accuracy: 0.7736\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.48534 to 0.48426, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4858 - categorical_accuracy: 0.7733 - val_loss: 0.4829 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.48426 to 0.48287, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4841 - categorical_accuracy: 0.7741 - val_loss: 0.4834 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48287\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4830 - categorical_accuracy: 0.7753 - val_loss: 0.4821 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.48287 to 0.48209, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 5s 53us/step - loss: 0.4815 - categorical_accuracy: 0.7759 - val_loss: 0.4821 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.48209 to 0.48206, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4808 - categorical_accuracy: 0.7763 - val_loss: 0.4804 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.48206 to 0.48041, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4800 - categorical_accuracy: 0.7762 - val_loss: 0.4801 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.48041 to 0.48014, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 5s 56us/step - loss: 0.4785 - categorical_accuracy: 0.7765 - val_loss: 0.4799 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.48014 to 0.47987, saving model to best_model.h5\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4784 - categorical_accuracy: 0.7781 - val_loss: 0.4796 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.47987 to 0.47956, saving model to best_model.h5\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4784 - categorical_accuracy: 0.7782 - val_loss: 0.4793 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.47956 to 0.47930, saving model to best_model.h5\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 5s 56us/step - loss: 0.4769 - categorical_accuracy: 0.7782 - val_loss: 0.4794 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.47930\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 5s 53us/step - loss: 0.4763 - categorical_accuracy: 0.7782 - val_loss: 0.4792 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.47930 to 0.47921, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 5s 53us/step - loss: 0.4757 - categorical_accuracy: 0.7787 - val_loss: 0.4785 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.47921 to 0.47855, saving model to best_model.h5\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4753 - categorical_accuracy: 0.7790 - val_loss: 0.4785 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.47855 to 0.47854, saving model to best_model.h5\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 5s 56us/step - loss: 0.4746 - categorical_accuracy: 0.7787 - val_loss: 0.4783 - val_categorical_accuracy: 0.7758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: val_loss improved from 0.47854 to 0.47828, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4743 - categorical_accuracy: 0.7786 - val_loss: 0.4782 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.47828 to 0.47824, saving model to best_model.h5\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4746 - categorical_accuracy: 0.7786 - val_loss: 0.4777 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.47824 to 0.47770, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4737 - categorical_accuracy: 0.7798 - val_loss: 0.4781 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.47770\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4726 - categorical_accuracy: 0.7800 - val_loss: 0.4776 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.47770 to 0.47763, saving model to best_model.h5\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4723 - categorical_accuracy: 0.7799 - val_loss: 0.4789 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.47763\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4721 - categorical_accuracy: 0.7800 - val_loss: 0.4781 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.47763\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4712 - categorical_accuracy: 0.7798 - val_loss: 0.4773 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.47763 to 0.47730, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4712 - categorical_accuracy: 0.7804 - val_loss: 0.4771 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.47730 to 0.47709, saving model to best_model.h5\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4700 - categorical_accuracy: 0.7811 - val_loss: 0.4800 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.47709\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4704 - categorical_accuracy: 0.7813 - val_loss: 0.4796 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.47709\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 5s 53us/step - loss: 0.4698 - categorical_accuracy: 0.7819 - val_loss: 0.4772 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.47709\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 5s 56us/step - loss: 0.4698 - categorical_accuracy: 0.7811 - val_loss: 0.4777 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.47709\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 5s 53us/step - loss: 0.4691 - categorical_accuracy: 0.7805 - val_loss: 0.4779 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47709\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4687 - categorical_accuracy: 0.7815 - val_loss: 0.4766 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.47709 to 0.47662, saving model to best_model.h5\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4692 - categorical_accuracy: 0.7804 - val_loss: 0.4777 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47662\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4680 - categorical_accuracy: 0.7818 - val_loss: 0.4778 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47662\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4682 - categorical_accuracy: 0.7805 - val_loss: 0.4767 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47662\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4674 - categorical_accuracy: 0.7815 - val_loss: 0.4769 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47662\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4672 - categorical_accuracy: 0.7825 - val_loss: 0.4777 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.47662\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4657 - categorical_accuracy: 0.7831 - val_loss: 0.4782 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.47662\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4675 - categorical_accuracy: 0.7824 - val_loss: 0.4766 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.47662 to 0.47657, saving model to best_model.h5\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4656 - categorical_accuracy: 0.7822 - val_loss: 0.4761 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.47657 to 0.47615, saving model to best_model.h5\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 5s 56us/step - loss: 0.4656 - categorical_accuracy: 0.7824 - val_loss: 0.4776 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47615\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4652 - categorical_accuracy: 0.7822 - val_loss: 0.4775 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47615\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4651 - categorical_accuracy: 0.7824 - val_loss: 0.4795 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47615\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 5s 54us/step - loss: 0.4638 - categorical_accuracy: 0.7836 - val_loss: 0.4783 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47615\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 5s 56us/step - loss: 0.4645 - categorical_accuracy: 0.7828 - val_loss: 0.4773 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47615\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 5s 55us/step - loss: 0.4647 - categorical_accuracy: 0.7825 - val_loss: 0.4769 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47615\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 5s 53us/step - loss: 0.4634 - categorical_accuracy: 0.7832 - val_loss: 0.4788 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47615\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 5s 53us/step - loss: 0.4643 - categorical_accuracy: 0.7829 - val_loss: 0.4784 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47615\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 5s 51us/step - loss: 0.4630 - categorical_accuracy: 0.7818 - val_loss: 0.4775 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47615\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 3s 33us/step - loss: 0.4631 - categorical_accuracy: 0.7828 - val_loss: 0.4763 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47615\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 3s 33us/step - loss: 0.4596 - categorical_accuracy: 0.7839 - val_loss: 0.4767 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47615\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 3s 33us/step - loss: 0.4577 - categorical_accuracy: 0.7856 - val_loss: 0.4762 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47615\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 3s 33us/step - loss: 0.4575 - categorical_accuracy: 0.7854 - val_loss: 0.4764 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47615\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 3s 33us/step - loss: 0.4570 - categorical_accuracy: 0.7856 - val_loss: 0.4766 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47615\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 3s 33us/step - loss: 0.4571 - categorical_accuracy: 0.7857 - val_loss: 0.4767 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47615\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 3s 33us/step - loss: 0.4571 - categorical_accuracy: 0.7857 - val_loss: 0.4767 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47615\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 3s 34us/step - loss: 0.4563 - categorical_accuracy: 0.7867 - val_loss: 0.4767 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47615\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 3s 34us/step - loss: 0.4556 - categorical_accuracy: 0.7859 - val_loss: 0.4773 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47615\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 3s 34us/step - loss: 0.4550 - categorical_accuracy: 0.7872 - val_loss: 0.4771 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47615\n",
      "Epoch 62/250\n",
      "91457/91457 [==============================] - 3s 34us/step - loss: 0.4555 - categorical_accuracy: 0.7867 - val_loss: 0.4769 - val_categorical_accuracy: 0.7789\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47615\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 00062: early stopping\n",
      "the 5 fold Log-Loss (NN) is 0.476148\n",
      "PARTIAL: mean Log-Loss (NN) is 0.472390\n",
      "CPU times: user 3h 48min 26s, sys: 3min 55s, total: 3h 52min 21s\n",
      "Wall time: 1h 35min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Bloco para executar a rede neural a cada passada do KFold\n",
    "# Vamos realizar 2 loops com 5 kfolds e apurar a média\n",
    "loop = 2\n",
    "fold = 5\n",
    "\n",
    "# Definindo listas que serão preenchidas durante o loop for\n",
    "oof_nn = np.zeros([loop, train_y.shape[0], train_y.shape[1]])\n",
    "models_nn = []\n",
    "logloss_csv_nn = []\n",
    "\n",
    "# Treinando o modelo\n",
    "for k in range(loop):\n",
    "    kfold = KFold(fold, random_state = 42 + k, shuffle = True)\n",
    "    for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(train_y)):\n",
    "        print(\"-----------\")\n",
    "        print(f'Loop {k+1}/{loop}' + f' Fold {k_fold+1}/{fold}')\n",
    "        print(\"-----------\")\n",
    "        \n",
    "        tr_x, tr_y = train_x[tr_inds], train_y[tr_inds]\n",
    "        val_x, val_y = train_x[val_inds], train_y[val_inds]\n",
    "        \n",
    "        # Train NN\n",
    "        nn, logloss_nn = get_nn(tr_x, tr_y, val_x, val_y, shape=val_x.shape[0])\n",
    "        models_nn.append(nn)\n",
    "        print(\"the %d fold Log-Loss (NN) is %f\"%((k_fold+1), logloss_nn))\n",
    "        logloss_csv_nn.append(logloss_nn)\n",
    "        \n",
    "        #Predict OOF\n",
    "        oof_nn[k, val_inds, :] = nn.predict(val_x)\n",
    "        \n",
    "    print(\"PARTIAL: mean Log-Loss (NN) is %f\"%np.mean(logloss_csv_nn))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média log-loss:  0.472390\n",
      "Média OOF log-loss: 0.472390\n"
     ]
    }
   ],
   "source": [
    "# Verificando o resultado médio do Log Loss para cada passada do Kfold\n",
    "loss_oof_nn = []\n",
    "\n",
    "for k in range(loop):\n",
    "    loss_oof_nn.append(log_loss(train_y, oof_nn[k,...], eps=1e-15))\n",
    "    \n",
    "print(\"Média log-loss:  %f\"%np.mean(logloss_csv_nn))\n",
    "print(\"Média OOF log-loss: %f\"%np.mean(loss_oof_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apenas para acompanhar o resultado visual\n",
    "# Exibir o treinamento somente do primeiro kfold\n",
    "plt.figure(figsize=(18, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(models_nn[0].history.history[\"loss\"], \"o-\", alpha=.9, label=\"loss\")\n",
    "plt.plot(models_nn[0].history.history[\"val_loss\"], \"o-\", alpha=.9, label=\"val_loss\")\n",
    "plt.axhline(1, linestyle=\"--\", c=\"C2\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(models_nn[0].history.history[\"categorical_accuracy\"], \"o-\", alpha=.9, label=\"accuracy\")\n",
    "plt.plot(models_nn[0].history.history[\"val_categorical_accuracy\"], \"o-\", alpha=.9, label=\"val_accuracy\")\n",
    "plt.axhline(.7, linestyle=\"--\", c=\"C2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114393, 25)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar um dataset somente com as colunas mais importantes conforme Feature Selection\n",
    "new_test = teste.loc[:,best_features['Feature']]\n",
    "\n",
    "# Padronizando os dados de treino\n",
    "new_test = scaler.fit_transform(new_test)\n",
    "\n",
    "new_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao para realizar as previsoes baseado em todos os modelos do Kfold\n",
    "def predict_proba(model, x, batch_size=32, verbose=0):\n",
    "    preds = model.predict(x, batch_size, verbose)\n",
    "    if preds.min() < 0. or preds.max() > 1.:\n",
    "        warnings.warn('Network returning invalid probability values.')\n",
    "    return preds\n",
    "\n",
    "def predict(x_te, models_nn):\n",
    "    model_num_nn = len(models_nn)\n",
    "\n",
    "    for k,m in enumerate(models_nn):\n",
    "        if k==0:\n",
    "            y_pred_nn = predict_proba(m, x_te, batch_size=1024)\n",
    "        else:\n",
    "            y_pred_nn += predict_proba(m, x_te, batch_size=1024)\n",
    "            \n",
    "    y_pred_nn = y_pred_nn / model_num_nn\n",
    "    return y_pred_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35087687, 0.8501545 , 0.86869085, ..., 0.89973706, 0.86728066,\n",
       "       0.4841927 ], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realizando as previsões no dataset de teste\n",
    "test_pred = predict(new_test, models_nn)\n",
    "test_pred[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114393, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PredictedProb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.850155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.868691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.645837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.816073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  PredictedProb\n",
       "0   0       0.350877\n",
       "1   1       0.850155\n",
       "2   2       0.868691\n",
       "3   7       0.645837\n",
       "4  10       0.816073"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega o dataset de exemplo de submission e carrega as previsões das probabilidades\n",
    "#submission = pd.read_csv('/kaggle/input/competicao-dsa-machine-learning-dec-2019/sample_submission.csv')\n",
    "submission = pd.read_csv('../dataset/sample_submission.csv')\n",
    "submission['PredictedProb'] = test_pred[:,1]\n",
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera o arquivo de saída para submeter no Kaggle\n",
    "submission.to_csv('../submission/submission_nn_v1.0.2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.836056    0.000026\n",
       "0.941955    0.000026\n",
       "0.963833    0.000026\n",
       "0.933647    0.000026\n",
       "0.950567    0.000026\n",
       "              ...   \n",
       "0.926571    0.000009\n",
       "0.926571    0.000009\n",
       "0.867957    0.000009\n",
       "0.702730    0.000009\n",
       "0.795499    0.000009\n",
       "Name: PredictedProb, Length: 113252, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apenas para visualizar a distribuição das previsões\n",
    "submission['PredictedProb'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograma com as previsões\n",
    "plt.hist(submission.PredictedProb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Continua...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03f293c5aaf34fd7a50a3b2bf14dab7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0af695c1d5e24a17b3a2aebc133ddd39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6782fd7c22664c55904d66122a64843b",
        "IPY_MODEL_9163f58ecd4f4a84922abc60cc69d789"
       ],
       "layout": "IPY_MODEL_d8264eeec6af442097559d8923081902"
      }
     },
     "1c88767c318e4d86a9ab43245f5daa77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "219092655bed428f83aeb751ccaa2a4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "44521ec2de994d5186cdf7d1e445d158": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6782fd7c22664c55904d66122a64843b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c5e6acfe3f654e7ca86e4cddf862e984",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_219092655bed428f83aeb751ccaa2a4d",
       "value": 1000
      }
     },
     "9163f58ecd4f4a84922abc60cc69d789": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9a5c32201e5c447b829c604d757794df",
       "placeholder": "​",
       "style": "IPY_MODEL_b90e11ceb23b4a84a2f4908689dff66f",
       "value": " 1000/1000 [01:50&lt;00:00,  9.06it/s]"
      }
     },
     "9a5c32201e5c447b829c604d757794df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a8bfc018378d49efa1570fcb43b496f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b90e11ceb23b4a84a2f4908689dff66f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c5e6acfe3f654e7ca86e4cddf862e984": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c95734415f404758ba787c86b58e7cc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d0cb9d8667c340c4b49fb7cf32acaf7c",
        "IPY_MODEL_e1aabb08150346cc9871c9f6b3bb5d36"
       ],
       "layout": "IPY_MODEL_1c88767c318e4d86a9ab43245f5daa77"
      }
     },
     "d0cb9d8667c340c4b49fb7cf32acaf7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_44521ec2de994d5186cdf7d1e445d158",
       "max": 17000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_eec34a1f17e2477fb10e7c11419a1382",
       "value": 17000
      }
     },
     "d8264eeec6af442097559d8923081902": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1aabb08150346cc9871c9f6b3bb5d36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a8bfc018378d49efa1570fcb43b496f3",
       "placeholder": "​",
       "style": "IPY_MODEL_03f293c5aaf34fd7a50a3b2bf14dab7d",
       "value": " 17000/17000 [11:01&lt;00:00, 25.68it/s]"
      }
     },
     "eec34a1f17e2477fb10e7c11419a1382": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
