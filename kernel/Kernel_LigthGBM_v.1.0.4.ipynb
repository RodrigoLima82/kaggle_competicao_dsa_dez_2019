{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle\n",
    "## Competição DSA de Machine Learning - Dezembro 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versão 1.0.0: LB = 0.48866 CV = 0.463102\n",
    "- modelo: LightGBM (com algumas otimizações)\n",
    "- features engineering: gerado através do Auto_ViML\n",
    "\n",
    "Versão 1.0.1: LB = 0.48991 CV = 0.462946\n",
    "- modelo: LightGBM (com algumas otimizações)\n",
    "- features engineering: gerado através do Auto_ViML (com novas features)\n",
    "\n",
    "Versão 1.0.2: LB = 0.48915 CV = 0.464442\n",
    "- modelo: LightGBM (com algumas otimizações)\n",
    "- features engineering: gerado através do Auto_ViML (com agrupamento pela coluna v2)\n",
    "\n",
    "Versão 1.0.3: LB = 0.88299 CV = 0.344659\n",
    "- modelo: LightGBM (com algumas otimizações)\n",
    "- features engineering: gerado interno\n",
    "\n",
    "Versão 1.0.4: LB = ???? CV = 0.461724\n",
    "- modelo: LightGBM (com algumas otimizações)\n",
    "- features engineering: gerado através do Auto_ViML (modificado v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar os principais pacotes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from scipy.stats import mstats\n",
    "\n",
    "# Evitar que aparece os warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Seta algumas opções no Jupyter para exibição dos datasets\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# Variavel para controlar o treinamento no Kaggle\n",
    "TRAIN_OFFLINE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importa os pacotes de algoritmos\n",
    "import lightgbm as lgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "# Importa pacotes do sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando os dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao de leitura dos dados\n",
    "def read_data():\n",
    "    \n",
    "    if TRAIN_OFFLINE:\n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        train = pd.read_csv('../dataset/dataset_treino_modificado.csv')\n",
    "        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n",
    "        \n",
    "        print('Carregando arquivo dataset_teste.csv....')\n",
    "        test = pd.read_csv('../dataset/dataset_teste_modificado.csv')\n",
    "        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n",
    "        \n",
    "    else:\n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        train = pd.read_csv('/kaggle/input/competicao-dsa-machine-learning-dec-2019/dataset_treino.csv')\n",
    "        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n",
    "        \n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        test = pd.read_csv('/kaggle/input/competicao-dsa-machine-learning-dec-2019/dataset_teste.csv')\n",
    "        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando arquivo dataset_treino.csv....\n",
      "dataset_treino.csv tem 114321 linhas and 44 colunas\n",
      "Carregando arquivo dataset_teste.csv....\n",
      "dataset_teste.csv tem 114393 linhas and 51 colunas\n"
     ]
    }
   ],
   "source": [
    "# Leitura dos dados\n",
    "train, test = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test.columns[:-8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228714, 44)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Juntando os dois dataset (treino e teste)\n",
    "df = train.append(test)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Numerical features:  44\n",
      "Number of Categorical features:  0\n"
     ]
    }
   ],
   "source": [
    "# Verificar a quantidade de features numericas e categoricas\n",
    "\n",
    "numerical_feats = df.dtypes[df.dtypes != \"object\"].index\n",
    "print(\"Number of Numerical features: \", len(numerical_feats))\n",
    "\n",
    "categorical_feats = df.dtypes[df.dtypes == \"object\"].index\n",
    "print(\"Number of Categorical features: \", len(categorical_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tratando os dados outliers com Winsorize\n",
    "for col in df.columns:\n",
    "    if col in ['target']:\n",
    "        continue\n",
    "    df[col] = mstats.winsorize(df[col], limits=[0.05, 0.05])[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Função para criação de novas features, agrupando por colunas\n",
    "'''def ft(data):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    numerical_cols = data.dtypes[data.dtypes != \"object\"].index\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        if col in ['ID','target','v2']:\n",
    "            continue\n",
    "        \n",
    "        df[col + '_mean']   = data.groupby(['v2'])[col].mean()\n",
    "        df[col + '_median'] = data.groupby(['v2'])[col].median()\n",
    "        df[col + '_max']    = data.groupby(['v2'])[col].max()\n",
    "        df[col + '_min']    = data.groupby(['v2'])[col].min()\n",
    "        df[col + '_std'] = data.groupby(['v2'])[col].std()\n",
    "\n",
    "    return df'''\n",
    "    \n",
    "#new_df = ft(df)\n",
    "#new_df = new_df.reset_index()\n",
    "#new_df = pd.merge(df, new_df, on='v2',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>v100</th>\n",
       "      <th>v101</th>\n",
       "      <th>v106</th>\n",
       "      <th>v107</th>\n",
       "      <th>v108</th>\n",
       "      <th>v10_bin</th>\n",
       "      <th>v110</th>\n",
       "      <th>v111</th>\n",
       "      <th>v112</th>\n",
       "      <th>v113</th>\n",
       "      <th>v125</th>\n",
       "      <th>v129</th>\n",
       "      <th>v14_bin</th>\n",
       "      <th>v22</th>\n",
       "      <th>v24</th>\n",
       "      <th>v26_bin</th>\n",
       "      <th>v28_bin</th>\n",
       "      <th>v3</th>\n",
       "      <th>v30</th>\n",
       "      <th>v31</th>\n",
       "      <th>v34_bin</th>\n",
       "      <th>v38</th>\n",
       "      <th>v46_bin</th>\n",
       "      <th>v47</th>\n",
       "      <th>v50</th>\n",
       "      <th>v52</th>\n",
       "      <th>v55_bin</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57_bin</th>\n",
       "      <th>v58_bin</th>\n",
       "      <th>v62</th>\n",
       "      <th>v65_bin</th>\n",
       "      <th>v66</th>\n",
       "      <th>v67</th>\n",
       "      <th>v71</th>\n",
       "      <th>v72</th>\n",
       "      <th>v74</th>\n",
       "      <th>v79</th>\n",
       "      <th>v84</th>\n",
       "      <th>v85</th>\n",
       "      <th>v91</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141689</td>\n",
       "      <td>0.386152</td>\n",
       "      <td>0.495177</td>\n",
       "      <td>0</td>\n",
       "      <td>0.131094</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.338176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111270</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.598997</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.139374</td>\n",
       "      <td>0.269716</td>\n",
       "      <td>0</td>\n",
       "      <td>0.281723</td>\n",
       "      <td>0.212921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163392</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.764561</td>\n",
       "      <td>0.233377</td>\n",
       "      <td>0.407940</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135128</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0.340090</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>2</td>\n",
       "      <td>0.146414</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0.441279</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.172676</td>\n",
       "      <td>0.328590</td>\n",
       "      <td>0</td>\n",
       "      <td>0.239724</td>\n",
       "      <td>0.163506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053418</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973248</td>\n",
       "      <td>0.398321</td>\n",
       "      <td>0.612259</td>\n",
       "      <td>0</td>\n",
       "      <td>0.132526</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0.236744</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.171427</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506923</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.128673</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0</td>\n",
       "      <td>0.197568</td>\n",
       "      <td>0.170047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target      v100      v101      v106  v107      v108  v10_bin  v110  \\\n",
       "0     0.0  0.141689  0.386152  0.495177     0  0.131094       49     0   \n",
       "1     1.0  0.000000  0.000000  0.000000     0  0.000000       61     0   \n",
       "2     1.0  0.764561  0.233377  0.407940     0  0.135128       61     0   \n",
       "3     0.0  0.000000  0.000000  0.000000     1  0.000000       38     0   \n",
       "4     1.0  0.973248  0.398321  0.612259     0  0.132526       55     1   \n",
       "\n",
       "       v111  v112  v113  v125  v129  v14_bin  v22  v24  v26_bin  v28_bin  v3  \\\n",
       "0  0.338176     0     0     0     0       45    0    0      122      175   0   \n",
       "1  0.000000     0     1     1     0       35    1    1        0        0   0   \n",
       "2  0.340090     1     0     2     0       58    2    2       30      175   0   \n",
       "3  0.000000     2     0     3     0       43    3    1        0        0   0   \n",
       "4  0.236744     3    -1     4     0      137    4    0       54       69   0   \n",
       "\n",
       "   v30  v31  v34_bin  v38  v46_bin  v47       v50  v52  v55_bin  v56  v57_bin  \\\n",
       "0    0    0      141    0      251    0  0.111270    0      138    0        9   \n",
       "1    1    0      175    0        0    1  0.163392    1        0    1        0   \n",
       "2   -1    0       62    0      124    2  0.146414    0      138    2       21   \n",
       "3   -1    0       62    0        0    1  0.053418    2        0   -1        0   \n",
       "4   -1    1      120    6       15    3  0.171427    3       32    3       61   \n",
       "\n",
       "   v58_bin  v62  v65_bin  v66       v67  v71  v72  v74  v79       v84  \\\n",
       "0      106    1      128    0  0.598997    0    1    0    0  0.139374   \n",
       "1        0    1        0    1  0.000000    1    1    0    1  0.000000   \n",
       "2       54    1       37    0  0.441279    1    1    0    2  0.172676   \n",
       "3        0    2        0    2  0.000000    1    2    0    3  0.000000   \n",
       "4       22    0      151    1  0.506923    1    6    1    4  0.128673   \n",
       "\n",
       "        v85  v91       v94       v95  \n",
       "0  0.269716    0  0.281723  0.212921  \n",
       "1  0.000000    0  0.000000  0.000000  \n",
       "2  0.328590    0  0.239724  0.163506  \n",
       "3  0.000000    1  0.000000  0.000000  \n",
       "4  0.114583    0  0.197568  0.170047  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tratando as features categoricas com LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in categorical_feats:\n",
    "    le.fit(np.unique(list(new_df[col].values)))\n",
    "    new_df[col] = le.transform(new_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v106', 'v91', 'v95']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = new_df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228714, 41)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop features \n",
    "new_df = new_df.drop(new_df[to_drop], axis=1)\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>v100</th>\n",
       "      <th>v101</th>\n",
       "      <th>v107</th>\n",
       "      <th>v108</th>\n",
       "      <th>v10_bin</th>\n",
       "      <th>v110</th>\n",
       "      <th>v111</th>\n",
       "      <th>v112</th>\n",
       "      <th>v113</th>\n",
       "      <th>v125</th>\n",
       "      <th>v129</th>\n",
       "      <th>v14_bin</th>\n",
       "      <th>v22</th>\n",
       "      <th>v24</th>\n",
       "      <th>v26_bin</th>\n",
       "      <th>v28_bin</th>\n",
       "      <th>v3</th>\n",
       "      <th>v30</th>\n",
       "      <th>v31</th>\n",
       "      <th>v34_bin</th>\n",
       "      <th>v38</th>\n",
       "      <th>v46_bin</th>\n",
       "      <th>v47</th>\n",
       "      <th>v50</th>\n",
       "      <th>v52</th>\n",
       "      <th>v55_bin</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57_bin</th>\n",
       "      <th>v58_bin</th>\n",
       "      <th>v62</th>\n",
       "      <th>v65_bin</th>\n",
       "      <th>v66</th>\n",
       "      <th>v67</th>\n",
       "      <th>v71</th>\n",
       "      <th>v72</th>\n",
       "      <th>v74</th>\n",
       "      <th>v79</th>\n",
       "      <th>v84</th>\n",
       "      <th>v85</th>\n",
       "      <th>v94</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.141689</td>\n",
       "      <td>0.386152</td>\n",
       "      <td>0</td>\n",
       "      <td>0.131094</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.338176</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111270</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0.598997</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.139374</td>\n",
       "      <td>0.269716</td>\n",
       "      <td>0.281723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163392</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.764561</td>\n",
       "      <td>0.233377</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135128</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0.340090</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>2</td>\n",
       "      <td>0.146414</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0.441279</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.172676</td>\n",
       "      <td>0.328590</td>\n",
       "      <td>0.239724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053418</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973248</td>\n",
       "      <td>0.398321</td>\n",
       "      <td>0</td>\n",
       "      <td>0.132526</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0.236744</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.171427</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506923</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.128673</td>\n",
       "      <td>0.114583</td>\n",
       "      <td>0.197568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target      v100      v101  v107      v108  v10_bin  v110      v111  v112  \\\n",
       "0     0.0  0.141689  0.386152     0  0.131094       49     0  0.338176     0   \n",
       "1     1.0  0.000000  0.000000     0  0.000000       61     0  0.000000     0   \n",
       "2     1.0  0.764561  0.233377     0  0.135128       61     0  0.340090     1   \n",
       "3     0.0  0.000000  0.000000     1  0.000000       38     0  0.000000     2   \n",
       "4     1.0  0.973248  0.398321     0  0.132526       55     1  0.236744     3   \n",
       "\n",
       "   v113  v125  v129  v14_bin  v22  v24  v26_bin  v28_bin  v3  v30  v31  \\\n",
       "0     0     0     0       45    0    0      122      175   0    0    0   \n",
       "1     1     1     0       35    1    1        0        0   0    1    0   \n",
       "2     0     2     0       58    2    2       30      175   0   -1    0   \n",
       "3     0     3     0       43    3    1        0        0   0   -1    0   \n",
       "4    -1     4     0      137    4    0       54       69   0   -1    1   \n",
       "\n",
       "   v34_bin  v38  v46_bin  v47       v50  v52  v55_bin  v56  v57_bin  v58_bin  \\\n",
       "0      141    0      251    0  0.111270    0      138    0        9      106   \n",
       "1      175    0        0    1  0.163392    1        0    1        0        0   \n",
       "2       62    0      124    2  0.146414    0      138    2       21       54   \n",
       "3       62    0        0    1  0.053418    2        0   -1        0        0   \n",
       "4      120    6       15    3  0.171427    3       32    3       61       22   \n",
       "\n",
       "   v62  v65_bin  v66       v67  v71  v72  v74  v79       v84       v85  \\\n",
       "0    1      128    0  0.598997    0    1    0    0  0.139374  0.269716   \n",
       "1    1        0    1  0.000000    1    1    0    1  0.000000  0.000000   \n",
       "2    1       37    0  0.441279    1    1    0    2  0.172676  0.328590   \n",
       "3    2        0    2  0.000000    1    2    0    3  0.000000  0.000000   \n",
       "4    0      151    1  0.506923    1    6    1    4  0.128673  0.114583   \n",
       "\n",
       "        v94  \n",
       "0  0.281723  \n",
       "1  0.000000  \n",
       "2  0.239724  \n",
       "3  0.000000  \n",
       "4  0.197568  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo LigthGBM com Hyperparametros"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Buscando os melhores parametros\n",
    "# Utilizei o Hyperopt para otimizacao e o metodo Bayesian Optimization Primer\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "with open('trials.json') as file_data:\n",
    "    trial_json = json.load(file_data)\n",
    "\n",
    "data = json_normalize(trial_json)\n",
    "data.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações Gerais\n",
    "\n",
    "GENERATE_SUBMISSION_FILES = True\n",
    "SUBMISSION_SUFIX = \"_lgbm_v.1.0.4\"\n",
    "STRATIFIED_KFOLD = False\n",
    "RANDOM_SEED = np.random.seed(123)\n",
    "NUM_THREADS = 4\n",
    "NUM_FOLDS = 10\n",
    "EARLY_STOPPING = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando os melhores parametros\n",
    "LIGHTGBM_PARAMS = {'boosting_type': 'gbdt', \n",
    "                  'colsample_bytree': 0.881783, \n",
    "                  'is_unbalance': False, \n",
    "                  'learning_rate': 0.0129388, \n",
    "                  'min_child_samples': 315, \n",
    "                  'num_leaves': 139, \n",
    "                  'reg_alpha': 0.484807, \n",
    "                  'reg_lambda': 0.515065, \n",
    "                  'subsample_for_bin': 280000, \n",
    "                  'subsample': 0.635119, \n",
    "                  'n_estimators': 10000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- LIGHTGBM MODEL -------------------------\n",
    "# Funcao para processar todo o pipeline do treinamento e gerar a submissao\n",
    "def run_model(dataset, categorical_feature = None):\n",
    "    \n",
    "    # Separar o dataset de treino e teste\n",
    "    treino = dataset[dataset['target'].notnull()]\n",
    "    teste  = dataset[dataset['target'].isnull()]\n",
    "    \n",
    "    # Separando features preditoras e target\n",
    "    #X_ = treino.drop(['ID','target'], axis=1)\n",
    "    X_ = treino.drop(['target'], axis=1)\n",
    "    y_ = treino['target']\n",
    "    \n",
    "    # Aplicando a funcao SMOTE\n",
    "    # SMOTE eh um metodo de oversampling. Ele cria exemplos sinteticos da classe minoritaria ao inves de criar copias\n",
    "    #sm = SMOTE(random_state=0)\n",
    "    #X, y = sm.fit_sample(X_, y_)\n",
    "\n",
    "    X = X_.copy()\n",
    "    y = y_.copy()\n",
    "    \n",
    "    # Padronizando os dados de treino\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "\n",
    "    #del_features = ['ID','target']\n",
    "    predictors = list(X_.columns)\n",
    "    \n",
    "    print(\"Train/valid shape: {}, test shape: {}\".format(X.shape, y.shape))\n",
    "\n",
    "    # Defini o tipo de Cross-Validation\n",
    "    if not STRATIFIED_KFOLD:\n",
    "        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n",
    "    else:\n",
    "        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n",
    "\n",
    "    # Hold oof predictions, test predictions, feature importance and training/valid auc\n",
    "    oof_preds = np.zeros(X.shape[0])\n",
    "    sub_preds = np.zeros(teste.shape[0])\n",
    "    importance_df = pd.DataFrame()\n",
    "    eval_results = dict()\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, y)):\n",
    "        train_x, train_y = X[train_idx], y[train_idx]\n",
    "        valid_x, valid_y = X[valid_idx], y[valid_idx]\n",
    "\n",
    "        params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n",
    "        \n",
    "        clf = LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n",
    "        \n",
    "        if not categorical_feature:\n",
    "            clf.fit(train_x, train_y, \n",
    "                    eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                    eval_metric='logloss', \n",
    "                    verbose=100, \n",
    "                    early_stopping_rounds= EARLY_STOPPING)\n",
    "        else:\n",
    "            clf.fit(train_x, train_y, \n",
    "                    eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
    "                    eval_metric='logloss', \n",
    "                    verbose=100, \n",
    "                    early_stopping_rounds=EARLY_STOPPING,\n",
    "                    feature_name= list(X_[predictors].columns), \n",
    "                    categorical_feature= categorical_feature)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(teste.drop(['target'], axis=1), num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        # Feature importance by GAIN and SPLIT\n",
    "        fold_importance = pd.DataFrame()\n",
    "        fold_importance[\"feature\"] = predictors\n",
    "        fold_importance[\"gain\"] = clf.booster_.feature_importance(importance_type='gain')\n",
    "        fold_importance[\"split\"] = clf.booster_.feature_importance(importance_type='split')\n",
    "        importance_df = pd.concat([importance_df, fold_importance], axis=0)\n",
    "        eval_results['train_{}'.format(n_fold+1)]  = clf.evals_result_['training']['binary_logloss']\n",
    "        eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['binary_logloss']\n",
    "\n",
    "        print('Fold %2d Log Loss : %.6f' % (n_fold + 1, log_loss(valid_y, oof_preds[valid_idx])))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    print('Full Log Loss score %.6f' % log_loss(y, oof_preds))\n",
    "    teste['target'] = sub_preds.copy()\n",
    "\n",
    "    # Get the average feature importance between folds\n",
    "    mean_importance = importance_df.groupby('feature').mean().reset_index()\n",
    "    mean_importance.sort_values(by= 'gain', ascending=False, inplace=True)\n",
    "    \n",
    "    # Save feature importance, test predictions and oof predictions as csv\n",
    "    if GENERATE_SUBMISSION_FILES:\n",
    "\n",
    "        # Save submission (test data) and feature importance\n",
    "        submission = pd.read_csv('../dataset/sample_submission.csv')\n",
    "        submission['PredictedProb'] = sub_preds.copy()\n",
    "        submission.to_csv('../submission/submission{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
    "        \n",
    "        mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
    "        plt.hist(submission.PredictedProb)\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    return mean_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/valid shape: (114321, 40), test shape: (114321,)\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.471659\tvalid_1's binary_logloss: 0.487987\n",
      "[200]\ttraining's binary_logloss: 0.451571\tvalid_1's binary_logloss: 0.4756\n",
      "[300]\ttraining's binary_logloss: 0.439787\tvalid_1's binary_logloss: 0.472256\n",
      "[400]\ttraining's binary_logloss: 0.429302\tvalid_1's binary_logloss: 0.470698\n",
      "[500]\ttraining's binary_logloss: 0.419902\tvalid_1's binary_logloss: 0.470268\n",
      "[600]\ttraining's binary_logloss: 0.411226\tvalid_1's binary_logloss: 0.469805\n",
      "[700]\ttraining's binary_logloss: 0.403147\tvalid_1's binary_logloss: 0.469435\n",
      "[800]\ttraining's binary_logloss: 0.395682\tvalid_1's binary_logloss: 0.469299\n",
      "[900]\ttraining's binary_logloss: 0.388608\tvalid_1's binary_logloss: 0.469094\n",
      "[1000]\ttraining's binary_logloss: 0.381862\tvalid_1's binary_logloss: 0.468868\n",
      "[1100]\ttraining's binary_logloss: 0.375397\tvalid_1's binary_logloss: 0.468798\n",
      "[1200]\ttraining's binary_logloss: 0.369295\tvalid_1's binary_logloss: 0.468884\n",
      "[1300]\ttraining's binary_logloss: 0.363282\tvalid_1's binary_logloss: 0.468942\n",
      "[1400]\ttraining's binary_logloss: 0.357573\tvalid_1's binary_logloss: 0.468897\n",
      "[1500]\ttraining's binary_logloss: 0.352017\tvalid_1's binary_logloss: 0.468837\n",
      "[1600]\ttraining's binary_logloss: 0.34663\tvalid_1's binary_logloss: 0.468882\n",
      "[1700]\ttraining's binary_logloss: 0.341327\tvalid_1's binary_logloss: 0.468992\n",
      "[1800]\ttraining's binary_logloss: 0.336189\tvalid_1's binary_logloss: 0.46914\n",
      "[1900]\ttraining's binary_logloss: 0.331273\tvalid_1's binary_logloss: 0.469188\n",
      "[2000]\ttraining's binary_logloss: 0.326489\tvalid_1's binary_logloss: 0.46926\n",
      "[2100]\ttraining's binary_logloss: 0.321862\tvalid_1's binary_logloss: 0.469327\n",
      "Early stopping, best iteration is:\n",
      "[1119]\ttraining's binary_logloss: 0.374241\tvalid_1's binary_logloss: 0.468758\n",
      "Fold  1 Log Loss : 0.468758\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.47266\tvalid_1's binary_logloss: 0.479734\n",
      "[200]\ttraining's binary_logloss: 0.452408\tvalid_1's binary_logloss: 0.46782\n",
      "[300]\ttraining's binary_logloss: 0.440469\tvalid_1's binary_logloss: 0.464827\n",
      "[400]\ttraining's binary_logloss: 0.429791\tvalid_1's binary_logloss: 0.463211\n",
      "[500]\ttraining's binary_logloss: 0.420302\tvalid_1's binary_logloss: 0.462451\n",
      "[600]\ttraining's binary_logloss: 0.411655\tvalid_1's binary_logloss: 0.461762\n",
      "[700]\ttraining's binary_logloss: 0.403766\tvalid_1's binary_logloss: 0.461398\n",
      "[800]\ttraining's binary_logloss: 0.396251\tvalid_1's binary_logloss: 0.461237\n",
      "[900]\ttraining's binary_logloss: 0.389217\tvalid_1's binary_logloss: 0.461096\n",
      "[1000]\ttraining's binary_logloss: 0.382383\tvalid_1's binary_logloss: 0.461113\n",
      "[1100]\ttraining's binary_logloss: 0.375962\tvalid_1's binary_logloss: 0.46128\n",
      "[1200]\ttraining's binary_logloss: 0.369795\tvalid_1's binary_logloss: 0.461326\n",
      "[1300]\ttraining's binary_logloss: 0.363873\tvalid_1's binary_logloss: 0.461409\n",
      "[1400]\ttraining's binary_logloss: 0.358069\tvalid_1's binary_logloss: 0.461518\n",
      "[1500]\ttraining's binary_logloss: 0.352456\tvalid_1's binary_logloss: 0.461541\n",
      "[1600]\ttraining's binary_logloss: 0.347147\tvalid_1's binary_logloss: 0.461646\n",
      "[1700]\ttraining's binary_logloss: 0.341787\tvalid_1's binary_logloss: 0.461688\n",
      "[1800]\ttraining's binary_logloss: 0.336697\tvalid_1's binary_logloss: 0.461761\n",
      "[1900]\ttraining's binary_logloss: 0.33175\tvalid_1's binary_logloss: 0.46176\n",
      "Early stopping, best iteration is:\n",
      "[928]\ttraining's binary_logloss: 0.387268\tvalid_1's binary_logloss: 0.461006\n",
      "Fold  2 Log Loss : 0.461006\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.472638\tvalid_1's binary_logloss: 0.477426\n",
      "[200]\ttraining's binary_logloss: 0.452324\tvalid_1's binary_logloss: 0.466577\n",
      "[300]\ttraining's binary_logloss: 0.440457\tvalid_1's binary_logloss: 0.463948\n",
      "[400]\ttraining's binary_logloss: 0.429889\tvalid_1's binary_logloss: 0.462556\n",
      "[500]\ttraining's binary_logloss: 0.420404\tvalid_1's binary_logloss: 0.461666\n",
      "[600]\ttraining's binary_logloss: 0.411628\tvalid_1's binary_logloss: 0.461191\n",
      "[700]\ttraining's binary_logloss: 0.403492\tvalid_1's binary_logloss: 0.460969\n",
      "[800]\ttraining's binary_logloss: 0.395955\tvalid_1's binary_logloss: 0.460732\n",
      "[900]\ttraining's binary_logloss: 0.388783\tvalid_1's binary_logloss: 0.460619\n",
      "[1000]\ttraining's binary_logloss: 0.381972\tvalid_1's binary_logloss: 0.460341\n",
      "[1100]\ttraining's binary_logloss: 0.375521\tvalid_1's binary_logloss: 0.460173\n",
      "[1200]\ttraining's binary_logloss: 0.36928\tvalid_1's binary_logloss: 0.460177\n",
      "[1300]\ttraining's binary_logloss: 0.36335\tvalid_1's binary_logloss: 0.46025\n",
      "[1400]\ttraining's binary_logloss: 0.357586\tvalid_1's binary_logloss: 0.460351\n",
      "[1500]\ttraining's binary_logloss: 0.351981\tvalid_1's binary_logloss: 0.460515\n",
      "[1600]\ttraining's binary_logloss: 0.3466\tvalid_1's binary_logloss: 0.460595\n",
      "[1700]\ttraining's binary_logloss: 0.341339\tvalid_1's binary_logloss: 0.460493\n",
      "[1800]\ttraining's binary_logloss: 0.336302\tvalid_1's binary_logloss: 0.460629\n",
      "[1900]\ttraining's binary_logloss: 0.331341\tvalid_1's binary_logloss: 0.460936\n",
      "[2000]\ttraining's binary_logloss: 0.326558\tvalid_1's binary_logloss: 0.460991\n",
      "[2100]\ttraining's binary_logloss: 0.321826\tvalid_1's binary_logloss: 0.461197\n",
      "Early stopping, best iteration is:\n",
      "[1145]\ttraining's binary_logloss: 0.372704\tvalid_1's binary_logloss: 0.460089\n",
      "Fold  3 Log Loss : 0.460089\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.472603\tvalid_1's binary_logloss: 0.478739\n",
      "[200]\ttraining's binary_logloss: 0.452275\tvalid_1's binary_logloss: 0.467719\n",
      "[300]\ttraining's binary_logloss: 0.440479\tvalid_1's binary_logloss: 0.464999\n",
      "[400]\ttraining's binary_logloss: 0.429996\tvalid_1's binary_logloss: 0.463131\n",
      "[500]\ttraining's binary_logloss: 0.420608\tvalid_1's binary_logloss: 0.462002\n",
      "[600]\ttraining's binary_logloss: 0.412087\tvalid_1's binary_logloss: 0.461173\n",
      "[700]\ttraining's binary_logloss: 0.404056\tvalid_1's binary_logloss: 0.460538\n",
      "[800]\ttraining's binary_logloss: 0.396436\tvalid_1's binary_logloss: 0.46008\n",
      "[900]\ttraining's binary_logloss: 0.389216\tvalid_1's binary_logloss: 0.459904\n",
      "[1000]\ttraining's binary_logloss: 0.382502\tvalid_1's binary_logloss: 0.459615\n",
      "[1100]\ttraining's binary_logloss: 0.375987\tvalid_1's binary_logloss: 0.459455\n",
      "[1200]\ttraining's binary_logloss: 0.369804\tvalid_1's binary_logloss: 0.459267\n",
      "[1300]\ttraining's binary_logloss: 0.363875\tvalid_1's binary_logloss: 0.459177\n",
      "[1400]\ttraining's binary_logloss: 0.3581\tvalid_1's binary_logloss: 0.459424\n",
      "[1500]\ttraining's binary_logloss: 0.352483\tvalid_1's binary_logloss: 0.459548\n",
      "[1600]\ttraining's binary_logloss: 0.347014\tvalid_1's binary_logloss: 0.45967\n",
      "[1700]\ttraining's binary_logloss: 0.341823\tvalid_1's binary_logloss: 0.459722\n",
      "[1800]\ttraining's binary_logloss: 0.336665\tvalid_1's binary_logloss: 0.459891\n",
      "[1900]\ttraining's binary_logloss: 0.331696\tvalid_1's binary_logloss: 0.459991\n",
      "[2000]\ttraining's binary_logloss: 0.326873\tvalid_1's binary_logloss: 0.460012\n",
      "[2100]\ttraining's binary_logloss: 0.32215\tvalid_1's binary_logloss: 0.460024\n",
      "[2200]\ttraining's binary_logloss: 0.317509\tvalid_1's binary_logloss: 0.460106\n",
      "[2300]\ttraining's binary_logloss: 0.312955\tvalid_1's binary_logloss: 0.460407\n",
      "Early stopping, best iteration is:\n",
      "[1301]\ttraining's binary_logloss: 0.363817\tvalid_1's binary_logloss: 0.459172\n",
      "Fold  4 Log Loss : 0.459172\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.471843\tvalid_1's binary_logloss: 0.483532\n",
      "[200]\ttraining's binary_logloss: 0.451594\tvalid_1's binary_logloss: 0.472699\n",
      "[300]\ttraining's binary_logloss: 0.439774\tvalid_1's binary_logloss: 0.469765\n",
      "[400]\ttraining's binary_logloss: 0.42932\tvalid_1's binary_logloss: 0.468179\n",
      "[500]\ttraining's binary_logloss: 0.419934\tvalid_1's binary_logloss: 0.467053\n",
      "[600]\ttraining's binary_logloss: 0.411355\tvalid_1's binary_logloss: 0.466184\n",
      "[700]\ttraining's binary_logloss: 0.403413\tvalid_1's binary_logloss: 0.465459\n",
      "[800]\ttraining's binary_logloss: 0.396058\tvalid_1's binary_logloss: 0.464962\n",
      "[900]\ttraining's binary_logloss: 0.389066\tvalid_1's binary_logloss: 0.464567\n",
      "[1000]\ttraining's binary_logloss: 0.3824\tvalid_1's binary_logloss: 0.464477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100]\ttraining's binary_logloss: 0.375949\tvalid_1's binary_logloss: 0.46435\n",
      "[1200]\ttraining's binary_logloss: 0.369682\tvalid_1's binary_logloss: 0.464183\n",
      "[1300]\ttraining's binary_logloss: 0.363733\tvalid_1's binary_logloss: 0.464063\n",
      "[1400]\ttraining's binary_logloss: 0.357868\tvalid_1's binary_logloss: 0.463891\n",
      "[1500]\ttraining's binary_logloss: 0.352251\tvalid_1's binary_logloss: 0.463797\n",
      "[1600]\ttraining's binary_logloss: 0.346797\tvalid_1's binary_logloss: 0.46368\n",
      "[1700]\ttraining's binary_logloss: 0.34152\tvalid_1's binary_logloss: 0.4637\n",
      "[1800]\ttraining's binary_logloss: 0.336442\tvalid_1's binary_logloss: 0.46384\n",
      "[1900]\ttraining's binary_logloss: 0.33152\tvalid_1's binary_logloss: 0.463889\n",
      "[2000]\ttraining's binary_logloss: 0.32665\tvalid_1's binary_logloss: 0.463903\n",
      "[2100]\ttraining's binary_logloss: 0.321986\tvalid_1's binary_logloss: 0.464101\n",
      "[2200]\ttraining's binary_logloss: 0.317408\tvalid_1's binary_logloss: 0.464293\n",
      "[2300]\ttraining's binary_logloss: 0.312951\tvalid_1's binary_logloss: 0.464511\n",
      "[2400]\ttraining's binary_logloss: 0.308661\tvalid_1's binary_logloss: 0.464708\n",
      "[2500]\ttraining's binary_logloss: 0.304513\tvalid_1's binary_logloss: 0.465027\n",
      "Early stopping, best iteration is:\n",
      "[1535]\ttraining's binary_logloss: 0.350327\tvalid_1's binary_logloss: 0.463621\n",
      "Fold  5 Log Loss : 0.463621\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.472057\tvalid_1's binary_logloss: 0.483446\n",
      "[200]\ttraining's binary_logloss: 0.451714\tvalid_1's binary_logloss: 0.473712\n",
      "[300]\ttraining's binary_logloss: 0.439816\tvalid_1's binary_logloss: 0.471626\n",
      "[400]\ttraining's binary_logloss: 0.429412\tvalid_1's binary_logloss: 0.470342\n",
      "[500]\ttraining's binary_logloss: 0.420066\tvalid_1's binary_logloss: 0.469646\n",
      "[600]\ttraining's binary_logloss: 0.411535\tvalid_1's binary_logloss: 0.46927\n",
      "[700]\ttraining's binary_logloss: 0.403551\tvalid_1's binary_logloss: 0.468966\n",
      "[800]\ttraining's binary_logloss: 0.395984\tvalid_1's binary_logloss: 0.468706\n",
      "[900]\ttraining's binary_logloss: 0.388738\tvalid_1's binary_logloss: 0.46856\n",
      "[1000]\ttraining's binary_logloss: 0.381858\tvalid_1's binary_logloss: 0.468486\n",
      "[1100]\ttraining's binary_logloss: 0.375337\tvalid_1's binary_logloss: 0.468379\n",
      "[1200]\ttraining's binary_logloss: 0.369061\tvalid_1's binary_logloss: 0.46846\n",
      "[1300]\ttraining's binary_logloss: 0.363035\tvalid_1's binary_logloss: 0.468485\n",
      "[1400]\ttraining's binary_logloss: 0.357248\tvalid_1's binary_logloss: 0.468621\n",
      "[1500]\ttraining's binary_logloss: 0.35163\tvalid_1's binary_logloss: 0.468656\n",
      "[1600]\ttraining's binary_logloss: 0.34623\tvalid_1's binary_logloss: 0.468772\n",
      "[1700]\ttraining's binary_logloss: 0.341006\tvalid_1's binary_logloss: 0.46885\n",
      "[1800]\ttraining's binary_logloss: 0.33592\tvalid_1's binary_logloss: 0.469067\n",
      "[1900]\ttraining's binary_logloss: 0.331009\tvalid_1's binary_logloss: 0.469257\n",
      "[2000]\ttraining's binary_logloss: 0.326285\tvalid_1's binary_logloss: 0.469502\n",
      "[2100]\ttraining's binary_logloss: 0.321609\tvalid_1's binary_logloss: 0.469725\n",
      "Early stopping, best iteration is:\n",
      "[1114]\ttraining's binary_logloss: 0.374445\tvalid_1's binary_logloss: 0.468353\n",
      "Fold  6 Log Loss : 0.468353\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.47314\tvalid_1's binary_logloss: 0.475867\n",
      "[200]\ttraining's binary_logloss: 0.453036\tvalid_1's binary_logloss: 0.463126\n",
      "[300]\ttraining's binary_logloss: 0.441105\tvalid_1's binary_logloss: 0.45964\n",
      "[400]\ttraining's binary_logloss: 0.430612\tvalid_1's binary_logloss: 0.457942\n",
      "[500]\ttraining's binary_logloss: 0.421159\tvalid_1's binary_logloss: 0.45708\n",
      "[600]\ttraining's binary_logloss: 0.412535\tvalid_1's binary_logloss: 0.456507\n",
      "[700]\ttraining's binary_logloss: 0.404512\tvalid_1's binary_logloss: 0.456211\n",
      "[800]\ttraining's binary_logloss: 0.396898\tvalid_1's binary_logloss: 0.456035\n",
      "[900]\ttraining's binary_logloss: 0.389658\tvalid_1's binary_logloss: 0.455774\n",
      "[1000]\ttraining's binary_logloss: 0.382825\tvalid_1's binary_logloss: 0.455657\n",
      "[1100]\ttraining's binary_logloss: 0.376326\tvalid_1's binary_logloss: 0.455618\n",
      "[1200]\ttraining's binary_logloss: 0.369996\tvalid_1's binary_logloss: 0.455682\n",
      "[1300]\ttraining's binary_logloss: 0.363992\tvalid_1's binary_logloss: 0.455706\n",
      "[1400]\ttraining's binary_logloss: 0.358304\tvalid_1's binary_logloss: 0.455663\n",
      "[1500]\ttraining's binary_logloss: 0.352661\tvalid_1's binary_logloss: 0.455689\n",
      "[1600]\ttraining's binary_logloss: 0.34721\tvalid_1's binary_logloss: 0.455657\n",
      "[1700]\ttraining's binary_logloss: 0.342045\tvalid_1's binary_logloss: 0.455788\n",
      "[1800]\ttraining's binary_logloss: 0.337035\tvalid_1's binary_logloss: 0.456145\n",
      "[1900]\ttraining's binary_logloss: 0.332172\tvalid_1's binary_logloss: 0.456301\n",
      "[2000]\ttraining's binary_logloss: 0.327331\tvalid_1's binary_logloss: 0.456516\n",
      "Early stopping, best iteration is:\n",
      "[1081]\ttraining's binary_logloss: 0.377538\tvalid_1's binary_logloss: 0.455569\n",
      "Fold  7 Log Loss : 0.455569\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.472163\tvalid_1's binary_logloss: 0.483254\n",
      "[200]\ttraining's binary_logloss: 0.452075\tvalid_1's binary_logloss: 0.471226\n",
      "[300]\ttraining's binary_logloss: 0.440114\tvalid_1's binary_logloss: 0.468126\n",
      "[400]\ttraining's binary_logloss: 0.42965\tvalid_1's binary_logloss: 0.46617\n",
      "[500]\ttraining's binary_logloss: 0.420324\tvalid_1's binary_logloss: 0.464888\n",
      "[600]\ttraining's binary_logloss: 0.411739\tvalid_1's binary_logloss: 0.464174\n",
      "[700]\ttraining's binary_logloss: 0.403667\tvalid_1's binary_logloss: 0.463849\n",
      "[800]\ttraining's binary_logloss: 0.396175\tvalid_1's binary_logloss: 0.463343\n",
      "[900]\ttraining's binary_logloss: 0.389128\tvalid_1's binary_logloss: 0.462997\n",
      "[1000]\ttraining's binary_logloss: 0.382413\tvalid_1's binary_logloss: 0.462694\n",
      "[1100]\ttraining's binary_logloss: 0.375877\tvalid_1's binary_logloss: 0.462359\n",
      "[1200]\ttraining's binary_logloss: 0.369702\tvalid_1's binary_logloss: 0.462207\n",
      "[1300]\ttraining's binary_logloss: 0.36385\tvalid_1's binary_logloss: 0.462045\n",
      "[1400]\ttraining's binary_logloss: 0.358135\tvalid_1's binary_logloss: 0.461907\n",
      "[1500]\ttraining's binary_logloss: 0.352565\tvalid_1's binary_logloss: 0.461824\n",
      "[1600]\ttraining's binary_logloss: 0.347172\tvalid_1's binary_logloss: 0.461738\n",
      "[1700]\ttraining's binary_logloss: 0.341831\tvalid_1's binary_logloss: 0.461592\n",
      "[1800]\ttraining's binary_logloss: 0.336816\tvalid_1's binary_logloss: 0.461683\n",
      "[1900]\ttraining's binary_logloss: 0.331866\tvalid_1's binary_logloss: 0.461587\n",
      "[2000]\ttraining's binary_logloss: 0.327027\tvalid_1's binary_logloss: 0.461516\n",
      "[2100]\ttraining's binary_logloss: 0.322325\tvalid_1's binary_logloss: 0.461473\n",
      "[2200]\ttraining's binary_logloss: 0.317791\tvalid_1's binary_logloss: 0.461479\n",
      "[2300]\ttraining's binary_logloss: 0.313271\tvalid_1's binary_logloss: 0.461756\n",
      "[2400]\ttraining's binary_logloss: 0.308888\tvalid_1's binary_logloss: 0.461951\n",
      "[2500]\ttraining's binary_logloss: 0.304629\tvalid_1's binary_logloss: 0.462195\n",
      "[2600]\ttraining's binary_logloss: 0.300347\tvalid_1's binary_logloss: 0.462304\n",
      "[2700]\ttraining's binary_logloss: 0.296293\tvalid_1's binary_logloss: 0.462339\n",
      "[2800]\ttraining's binary_logloss: 0.292338\tvalid_1's binary_logloss: 0.462534\n",
      "[2900]\ttraining's binary_logloss: 0.288446\tvalid_1's binary_logloss: 0.462655\n",
      "[3000]\ttraining's binary_logloss: 0.28467\tvalid_1's binary_logloss: 0.463009\n",
      "[3100]\ttraining's binary_logloss: 0.281025\tvalid_1's binary_logloss: 0.463334\n",
      "Early stopping, best iteration is:\n",
      "[2113]\ttraining's binary_logloss: 0.321737\tvalid_1's binary_logloss: 0.461407\n",
      "Fold  8 Log Loss : 0.461407\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.472129\tvalid_1's binary_logloss: 0.483314\n",
      "[200]\ttraining's binary_logloss: 0.451776\tvalid_1's binary_logloss: 0.471789\n",
      "[300]\ttraining's binary_logloss: 0.439946\tvalid_1's binary_logloss: 0.468991\n",
      "[400]\ttraining's binary_logloss: 0.429278\tvalid_1's binary_logloss: 0.467697\n",
      "[500]\ttraining's binary_logloss: 0.419986\tvalid_1's binary_logloss: 0.466955\n",
      "[600]\ttraining's binary_logloss: 0.411501\tvalid_1's binary_logloss: 0.466476\n",
      "[700]\ttraining's binary_logloss: 0.403451\tvalid_1's binary_logloss: 0.466035\n",
      "[800]\ttraining's binary_logloss: 0.395889\tvalid_1's binary_logloss: 0.465858\n",
      "[900]\ttraining's binary_logloss: 0.388839\tvalid_1's binary_logloss: 0.465829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's binary_logloss: 0.382093\tvalid_1's binary_logloss: 0.465762\n",
      "[1100]\ttraining's binary_logloss: 0.375642\tvalid_1's binary_logloss: 0.465734\n",
      "[1200]\ttraining's binary_logloss: 0.369362\tvalid_1's binary_logloss: 0.465671\n",
      "[1300]\ttraining's binary_logloss: 0.363405\tvalid_1's binary_logloss: 0.465787\n",
      "[1400]\ttraining's binary_logloss: 0.357626\tvalid_1's binary_logloss: 0.465925\n",
      "[1500]\ttraining's binary_logloss: 0.352087\tvalid_1's binary_logloss: 0.466184\n",
      "[1600]\ttraining's binary_logloss: 0.346694\tvalid_1's binary_logloss: 0.466503\n",
      "[1700]\ttraining's binary_logloss: 0.341516\tvalid_1's binary_logloss: 0.466767\n",
      "[1800]\ttraining's binary_logloss: 0.336367\tvalid_1's binary_logloss: 0.467035\n",
      "[1900]\ttraining's binary_logloss: 0.331317\tvalid_1's binary_logloss: 0.46715\n",
      "Early stopping, best iteration is:\n",
      "[937]\ttraining's binary_logloss: 0.386309\tvalid_1's binary_logloss: 0.465657\n",
      "Fold  9 Log Loss : 0.465657\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[100]\ttraining's binary_logloss: 0.473207\tvalid_1's binary_logloss: 0.475871\n",
      "[200]\ttraining's binary_logloss: 0.452971\tvalid_1's binary_logloss: 0.463114\n",
      "[300]\ttraining's binary_logloss: 0.44122\tvalid_1's binary_logloss: 0.459635\n",
      "[400]\ttraining's binary_logloss: 0.430668\tvalid_1's binary_logloss: 0.457535\n",
      "[500]\ttraining's binary_logloss: 0.421382\tvalid_1's binary_logloss: 0.456302\n",
      "[600]\ttraining's binary_logloss: 0.412825\tvalid_1's binary_logloss: 0.455489\n",
      "[700]\ttraining's binary_logloss: 0.404864\tvalid_1's binary_logloss: 0.455178\n",
      "[800]\ttraining's binary_logloss: 0.397325\tvalid_1's binary_logloss: 0.454698\n",
      "[900]\ttraining's binary_logloss: 0.390129\tvalid_1's binary_logloss: 0.454446\n",
      "[1000]\ttraining's binary_logloss: 0.383408\tvalid_1's binary_logloss: 0.454209\n",
      "[1100]\ttraining's binary_logloss: 0.376842\tvalid_1's binary_logloss: 0.454006\n",
      "[1200]\ttraining's binary_logloss: 0.370585\tvalid_1's binary_logloss: 0.453845\n",
      "[1300]\ttraining's binary_logloss: 0.364645\tvalid_1's binary_logloss: 0.453799\n",
      "[1400]\ttraining's binary_logloss: 0.35883\tvalid_1's binary_logloss: 0.453816\n",
      "[1500]\ttraining's binary_logloss: 0.35328\tvalid_1's binary_logloss: 0.453696\n",
      "[1600]\ttraining's binary_logloss: 0.347681\tvalid_1's binary_logloss: 0.453658\n",
      "[1700]\ttraining's binary_logloss: 0.342424\tvalid_1's binary_logloss: 0.453698\n",
      "[1800]\ttraining's binary_logloss: 0.337346\tvalid_1's binary_logloss: 0.453905\n",
      "[1900]\ttraining's binary_logloss: 0.332293\tvalid_1's binary_logloss: 0.454069\n",
      "[2000]\ttraining's binary_logloss: 0.327438\tvalid_1's binary_logloss: 0.454271\n",
      "[2100]\ttraining's binary_logloss: 0.322724\tvalid_1's binary_logloss: 0.454316\n",
      "[2200]\ttraining's binary_logloss: 0.318221\tvalid_1's binary_logloss: 0.454423\n",
      "[2300]\ttraining's binary_logloss: 0.313798\tvalid_1's binary_logloss: 0.454667\n",
      "[2400]\ttraining's binary_logloss: 0.309521\tvalid_1's binary_logloss: 0.454819\n",
      "[2500]\ttraining's binary_logloss: 0.305285\tvalid_1's binary_logloss: 0.455041\n",
      "[2600]\ttraining's binary_logloss: 0.301133\tvalid_1's binary_logloss: 0.455173\n",
      "Early stopping, best iteration is:\n",
      "[1649]\ttraining's binary_logloss: 0.345064\tvalid_1's binary_logloss: 0.453607\n",
      "Fold 10 Log Loss : 0.453607\n",
      "Full Log Loss score 0.461724\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>gain</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>v50</td>\n",
       "      <td>375668.418239</td>\n",
       "      <td>14730.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>v66</td>\n",
       "      <td>106722.038591</td>\n",
       "      <td>2950.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>v56</td>\n",
       "      <td>67105.880259</td>\n",
       "      <td>8955.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>v31</td>\n",
       "      <td>66532.364682</td>\n",
       "      <td>482.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>v22</td>\n",
       "      <td>62836.764023</td>\n",
       "      <td>13525.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>v34_bin</td>\n",
       "      <td>61505.875440</td>\n",
       "      <td>7959.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>v110</td>\n",
       "      <td>47994.083201</td>\n",
       "      <td>1022.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>v125</td>\n",
       "      <td>46232.333444</td>\n",
       "      <td>9738.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>v10_bin</td>\n",
       "      <td>46169.867209</td>\n",
       "      <td>8152.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>v14_bin</td>\n",
       "      <td>36342.509342</td>\n",
       "      <td>7387.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>v94</td>\n",
       "      <td>31406.216483</td>\n",
       "      <td>6708.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>v67</td>\n",
       "      <td>31204.610666</td>\n",
       "      <td>6436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>v52</td>\n",
       "      <td>30266.512428</td>\n",
       "      <td>6478.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>v108</td>\n",
       "      <td>29432.219475</td>\n",
       "      <td>5957.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>v100</td>\n",
       "      <td>29405.643538</td>\n",
       "      <td>5715.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>v85</td>\n",
       "      <td>28387.533428</td>\n",
       "      <td>5893.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>v113</td>\n",
       "      <td>27910.287515</td>\n",
       "      <td>4901.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>v101</td>\n",
       "      <td>26848.301956</td>\n",
       "      <td>5501.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>v24</td>\n",
       "      <td>26672.273498</td>\n",
       "      <td>3410.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>v84</td>\n",
       "      <td>26378.552111</td>\n",
       "      <td>5410.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>v112</td>\n",
       "      <td>26116.345676</td>\n",
       "      <td>5598.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>v111</td>\n",
       "      <td>25785.674778</td>\n",
       "      <td>5289.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>v46_bin</td>\n",
       "      <td>20175.278911</td>\n",
       "      <td>4266.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>v107</td>\n",
       "      <td>18984.351736</td>\n",
       "      <td>4059.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>v79</td>\n",
       "      <td>18938.443449</td>\n",
       "      <td>2908.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>v30</td>\n",
       "      <td>16583.672655</td>\n",
       "      <td>2964.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>v26_bin</td>\n",
       "      <td>14086.264038</td>\n",
       "      <td>2854.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>v57_bin</td>\n",
       "      <td>12557.876709</td>\n",
       "      <td>2474.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>v47</td>\n",
       "      <td>12236.878144</td>\n",
       "      <td>1331.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>v55_bin</td>\n",
       "      <td>12065.522263</td>\n",
       "      <td>2413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>v129</td>\n",
       "      <td>11185.422213</td>\n",
       "      <td>887.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>v28_bin</td>\n",
       "      <td>11120.083844</td>\n",
       "      <td>2417.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>v71</td>\n",
       "      <td>10324.383050</td>\n",
       "      <td>1676.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>v58_bin</td>\n",
       "      <td>9729.105735</td>\n",
       "      <td>2552.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>v65_bin</td>\n",
       "      <td>9160.482593</td>\n",
       "      <td>1830.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>v72</td>\n",
       "      <td>8951.705810</td>\n",
       "      <td>1745.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>v38</td>\n",
       "      <td>5664.219248</td>\n",
       "      <td>655.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>v62</td>\n",
       "      <td>4675.005113</td>\n",
       "      <td>873.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>v74</td>\n",
       "      <td>782.431167</td>\n",
       "      <td>197.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>v3</td>\n",
       "      <td>66.206098</td>\n",
       "      <td>10.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature           gain    split\n",
       "23      v50  375668.418239  14730.1\n",
       "31      v66  106722.038591   2950.9\n",
       "26      v56   67105.880259   8955.5\n",
       "18      v31   66532.364682    482.0\n",
       "12      v22   62836.764023  13525.1\n",
       "19  v34_bin   61505.875440   7959.1\n",
       "5      v110   47994.083201   1022.9\n",
       "9      v125   46232.333444   9738.1\n",
       "4   v10_bin   46169.867209   8152.6\n",
       "11  v14_bin   36342.509342   7387.6\n",
       "39      v94   31406.216483   6708.3\n",
       "32      v67   31204.610666   6436.0\n",
       "24      v52   30266.512428   6478.7\n",
       "3      v108   29432.219475   5957.5\n",
       "0      v100   29405.643538   5715.2\n",
       "38      v85   28387.533428   5893.1\n",
       "8      v113   27910.287515   4901.7\n",
       "1      v101   26848.301956   5501.5\n",
       "13      v24   26672.273498   3410.5\n",
       "37      v84   26378.552111   5410.1\n",
       "7      v112   26116.345676   5598.5\n",
       "6      v111   25785.674778   5289.8\n",
       "21  v46_bin   20175.278911   4266.3\n",
       "2      v107   18984.351736   4059.6\n",
       "36      v79   18938.443449   2908.3\n",
       "17      v30   16583.672655   2964.2\n",
       "14  v26_bin   14086.264038   2854.5\n",
       "27  v57_bin   12557.876709   2474.6\n",
       "22      v47   12236.878144   1331.1\n",
       "25  v55_bin   12065.522263   2413.0\n",
       "10     v129   11185.422213    887.1\n",
       "15  v28_bin   11120.083844   2417.7\n",
       "33      v71   10324.383050   1676.0\n",
       "28  v58_bin    9729.105735   2552.7\n",
       "30  v65_bin    9160.482593   1830.6\n",
       "34      v72    8951.705810   1745.1\n",
       "20      v38    5664.219248    655.9\n",
       "29      v62    4675.005113    873.6\n",
       "35      v74     782.431167    197.6\n",
       "16       v3      66.206098     10.9"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03f293c5aaf34fd7a50a3b2bf14dab7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0af695c1d5e24a17b3a2aebc133ddd39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6782fd7c22664c55904d66122a64843b",
        "IPY_MODEL_9163f58ecd4f4a84922abc60cc69d789"
       ],
       "layout": "IPY_MODEL_d8264eeec6af442097559d8923081902"
      }
     },
     "1c88767c318e4d86a9ab43245f5daa77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "219092655bed428f83aeb751ccaa2a4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "44521ec2de994d5186cdf7d1e445d158": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6782fd7c22664c55904d66122a64843b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c5e6acfe3f654e7ca86e4cddf862e984",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_219092655bed428f83aeb751ccaa2a4d",
       "value": 1000
      }
     },
     "9163f58ecd4f4a84922abc60cc69d789": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9a5c32201e5c447b829c604d757794df",
       "placeholder": "​",
       "style": "IPY_MODEL_b90e11ceb23b4a84a2f4908689dff66f",
       "value": " 1000/1000 [01:50&lt;00:00,  9.06it/s]"
      }
     },
     "9a5c32201e5c447b829c604d757794df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a8bfc018378d49efa1570fcb43b496f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b90e11ceb23b4a84a2f4908689dff66f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c5e6acfe3f654e7ca86e4cddf862e984": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c95734415f404758ba787c86b58e7cc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d0cb9d8667c340c4b49fb7cf32acaf7c",
        "IPY_MODEL_e1aabb08150346cc9871c9f6b3bb5d36"
       ],
       "layout": "IPY_MODEL_1c88767c318e4d86a9ab43245f5daa77"
      }
     },
     "d0cb9d8667c340c4b49fb7cf32acaf7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_44521ec2de994d5186cdf7d1e445d158",
       "max": 17000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_eec34a1f17e2477fb10e7c11419a1382",
       "value": 17000
      }
     },
     "d8264eeec6af442097559d8923081902": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1aabb08150346cc9871c9f6b3bb5d36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a8bfc018378d49efa1570fcb43b496f3",
       "placeholder": "​",
       "style": "IPY_MODEL_03f293c5aaf34fd7a50a3b2bf14dab7d",
       "value": " 17000/17000 [11:01&lt;00:00, 25.68it/s]"
      }
     },
     "eec34a1f17e2477fb10e7c11419a1382": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
