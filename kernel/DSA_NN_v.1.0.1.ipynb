{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle\n",
    "## Competição DSA de Machine Learning - Dezembro 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versão 1.0.0: LB = 0.50744 / CV = ???\n",
    "- modelo: NN com 3 camadas\n",
    "- features categoricas: removido\n",
    "- dados missing: atribuído o valor medio\n",
    "- feature selection: 25\n",
    "\n",
    "Versão 1.0.1: LB = 0.52913 / CV = 0.475503\n",
    "- modelo: NN com 4 camadas\n",
    "- dados missing: removido colunas com mais de 40% de NA e as demais -999\n",
    "- features categoricas: label encoder\n",
    "- feature engineering: usando pacote Boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar os principais pacotes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "import random as rd\n",
    "import os\n",
    "import codecs\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from numba import jit\n",
    "from collections import Counter\n",
    "import copy\n",
    "from typing import Any\n",
    "\n",
    "seed = 12345\n",
    "np.random.seed(seed)\n",
    "rd.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Evitar que aparece os warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Seta algumas opções no Jupyter para exibição dos datasets\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# Variavel para controlar o treinamento no Kaggle\n",
    "TRAIN_OFFLINE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa os pacotes de algoritmos\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb \n",
    "\n",
    "# Importa os pacotes de algoritmos de redes neurais (Keras)\n",
    "import keras\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda,BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import Callback,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "#from keras_radam import RAdam\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Importa pacotes do sklearn\n",
    "from sklearn import preprocessing\n",
    "import sklearn.metrics as mtr\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, log_loss, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, StandardScaler\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler as SS\n",
    "from sklearn.model_selection import train_test_split as TTS\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau as RLRP\n",
    "from keras.callbacks import EarlyStopping as ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregando os dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \n",
    "    if TRAIN_OFFLINE:\n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        train = pd.read_csv('../dataset/dataset_treino.csv')\n",
    "        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n",
    "\n",
    "        print('Carregando arquivo dataset_teste.csv....')\n",
    "        test = pd.read_csv('../dataset/dataset_teste.csv')\n",
    "        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n",
    "\n",
    "        \n",
    "    else:\n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        train = pd.read_csv('/kaggle/input/competicao-dsa-machine-learning-dec-2019/dataset_treino.csv')\n",
    "        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n",
    "        \n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        test = pd.read_csv('/kaggle/input/competicao-dsa-machine-learning-dec-2019/dataset_teste.csv')\n",
    "        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando arquivo dataset_treino.csv....\n",
      "dataset_treino.csv tem 114321 linhas and 133 colunas\n",
      "Carregando arquivo dataset_teste.csv....\n",
      "dataset_teste.csv tem 114393 linhas and 132 colunas\n"
     ]
    }
   ],
   "source": [
    "# Leitura dos dados\n",
    "train, test = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114321, 31), (114393, 30))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removendo todas as colunas com mais de 40% de dados missing\n",
    "train = train[train.columns[train.isnull().mean() <= 0.4]]\n",
    "test  = test[test.columns[test.isnull().mean() <= 0.4]]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoder nas features categoricas\n",
    "for c in train.columns[train.dtypes == 'object']:\n",
    "    train[c] = train[c].factorize()[0]\n",
    "    \n",
    "for c in test.columns[test.dtypes == 'object']:\n",
    "    test[c] = test[c].factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preenche os dados missing com -999\n",
    "train.fillna(-999,inplace=True)\n",
    "test.fillna(-999,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: \t1 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t29\n",
      "Rejected: \t0\n",
      "Iteration: \t2 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t29\n",
      "Rejected: \t0\n",
      "Iteration: \t3 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t29\n",
      "Rejected: \t0\n",
      "Iteration: \t4 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t29\n",
      "Rejected: \t0\n",
      "Iteration: \t5 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t29\n",
      "Rejected: \t0\n",
      "Iteration: \t6 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t29\n",
      "Rejected: \t0\n",
      "Iteration: \t7 / 100\n",
      "Confirmed: \t0\n",
      "Tentative: \t29\n",
      "Rejected: \t0\n",
      "Iteration: \t8 / 100\n",
      "Confirmed: \t22\n",
      "Tentative: \t1\n",
      "Rejected: \t6\n",
      "Iteration: \t9 / 100\n",
      "Confirmed: \t22\n",
      "Tentative: \t1\n",
      "Rejected: \t6\n",
      "Iteration: \t10 / 100\n",
      "Confirmed: \t22\n",
      "Tentative: \t1\n",
      "Rejected: \t6\n",
      "Iteration: \t11 / 100\n",
      "Confirmed: \t22\n",
      "Tentative: \t1\n",
      "Rejected: \t6\n",
      "Iteration: \t12 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t0\n",
      "Rejected: \t6\n",
      "\n",
      "\n",
      "BorutaPy finished running.\n",
      "\n",
      "Iteration: \t13 / 100\n",
      "Confirmed: \t23\n",
      "Tentative: \t0\n",
      "Rejected: \t6\n"
     ]
    }
   ],
   "source": [
    "from boruta import BorutaPy\n",
    "\n",
    "# Separando features preditoras e target\n",
    "X = train.drop(['ID', 'target'], axis=1)\n",
    "y = train['target']\n",
    "\n",
    "X = X.values\n",
    "y = y.values\n",
    "y = y.ravel()\n",
    "\n",
    "# define random forest classifier, with utilising all cores and\n",
    "# sampling in proportion to y labels\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# define Boruta feature selection method\n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n",
    "\n",
    "# find all relevant features - 5 features should be selected\n",
    "feat_selector.fit(X, y)\n",
    "\n",
    "# check selected features - first 5 features are selected\n",
    "feat_selector.support_\n",
    "\n",
    "# check ranking of features\n",
    "feat_selector.ranking_\n",
    "\n",
    "# call transform() on X to filter it down to selected features\n",
    "X_filtered = feat_selector.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114321, 23)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Criar e avaliar alguns algoritmos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114321, 23), (114321, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separando features preditoras e target\n",
    "train_x = X_filtered.copy()\n",
    "train_y = train['target']\n",
    "train_y = to_categorical(train_y)\n",
    "\n",
    "# Padronizando os dados\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Algoritmo Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn(x_tr,y_tr,x_val,y_val,shape):\n",
    "    K.clear_session()\n",
    "    \n",
    "    inp = Input(shape = (x_tr.shape[1],))\n",
    "\n",
    "    x = Dense(256, input_dim=x_tr.shape[1], activation='relu')(inp)\n",
    "    x = Dropout(0.625)(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.625)(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(.25)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    out = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inp,out)\n",
    "    \n",
    "    model.compile(optimizer = optimizers.adam(learning_rate=0.1, beta_1=0.9, beta_2=0.99),\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['categorical_accuracy'])\n",
    "     \n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                       mode='min',\n",
    "                       restore_best_weights=True, \n",
    "                       verbose=1, \n",
    "                       patience=20)\n",
    "\n",
    "    mc = ModelCheckpoint('best_model.h5',\n",
    "                         monitor='val_loss',\n",
    "                         mode='min',\n",
    "                         save_best_only=True, \n",
    "                         verbose=1, \n",
    "                         save_weights_only=True)\n",
    "\n",
    "    rl = ReduceLROnPlateau(monitor='val_loss', \n",
    "                           factor=0.1, \n",
    "                           patience=10, \n",
    "                           verbose=1, \n",
    "                           epsilon=1e-4, \n",
    "                           mode='min')\n",
    "\n",
    "    model.fit(x_tr, y_tr,\n",
    "              validation_data=[x_val, y_val],\n",
    "              callbacks=[es,mc,rl],\n",
    "              epochs=250, \n",
    "              batch_size=1024,\n",
    "              verbose=1,\n",
    "              #class_weight=class_weight_y,\n",
    "              shuffle=True)\n",
    "    \n",
    "    model.load_weights(\"best_model.h5\")\n",
    "    \n",
    "    y_pred = model.predict(x_val)\n",
    "    y_valid = y_val\n",
    "             \n",
    "    logloss = log_loss(y_valid, y_pred, eps=1e-15)\n",
    "\n",
    "    return model, logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Loop 1/2 Fold 1/5\n",
      "-----------\n",
      "WARNING:tensorflow:Large dropout rate: 0.625 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.625 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 91456 samples, validate on 22865 samples\n",
      "Epoch 1/250\n",
      "91456/91456 [==============================] - 8s 85us/step - loss: 0.5367 - categorical_accuracy: 0.7554 - val_loss: 0.7020 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70205, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91456/91456 [==============================] - 2s 22us/step - loss: 0.5218 - categorical_accuracy: 0.7597 - val_loss: 0.6152 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70205 to 0.61520, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91456/91456 [==============================] - 2s 21us/step - loss: 0.5111 - categorical_accuracy: 0.7598 - val_loss: 0.5293 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61520 to 0.52927, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91456/91456 [==============================] - 2s 21us/step - loss: 0.5050 - categorical_accuracy: 0.7607 - val_loss: 0.5640 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52927\n",
      "Epoch 5/250\n",
      "91456/91456 [==============================] - 2s 20us/step - loss: 0.5051 - categorical_accuracy: 0.7608 - val_loss: 0.5668 - val_categorical_accuracy: 0.7676\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.52927\n",
      "Epoch 6/250\n",
      "91456/91456 [==============================] - 2s 20us/step - loss: 0.5050 - categorical_accuracy: 0.7593 - val_loss: 0.5267 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.52927 to 0.52675, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "91456/91456 [==============================] - 2s 21us/step - loss: 0.5022 - categorical_accuracy: 0.7612 - val_loss: 0.7148 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.52675\n",
      "Epoch 8/250\n",
      "91456/91456 [==============================] - 2s 20us/step - loss: 0.5022 - categorical_accuracy: 0.7611 - val_loss: 0.5403 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.52675\n",
      "Epoch 9/250\n",
      "91456/91456 [==============================] - 2s 22us/step - loss: 0.4993 - categorical_accuracy: 0.7623 - val_loss: 0.5956 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.52675\n",
      "Epoch 10/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4988 - categorical_accuracy: 0.7641 - val_loss: 0.5220 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.52675 to 0.52205, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4984 - categorical_accuracy: 0.7634 - val_loss: 0.5132 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.52205 to 0.51316, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4979 - categorical_accuracy: 0.7647 - val_loss: 0.4902 - val_categorical_accuracy: 0.7655\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.51316 to 0.49015, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4982 - categorical_accuracy: 0.7648 - val_loss: 0.4988 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49015\n",
      "Epoch 14/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4955 - categorical_accuracy: 0.7657 - val_loss: 0.4921 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.49015\n",
      "Epoch 15/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4986 - categorical_accuracy: 0.7617 - val_loss: 0.5121 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.49015\n",
      "Epoch 16/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4952 - categorical_accuracy: 0.7663 - val_loss: 0.4901 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.49015 to 0.49008, saving model to best_model.h5\n",
      "Epoch 17/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4954 - categorical_accuracy: 0.7663 - val_loss: 0.5068 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.49008\n",
      "Epoch 18/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4961 - categorical_accuracy: 0.7664 - val_loss: 0.4782 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.49008 to 0.47820, saving model to best_model.h5\n",
      "Epoch 19/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4945 - categorical_accuracy: 0.7678 - val_loss: 0.5182 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.47820\n",
      "Epoch 20/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4939 - categorical_accuracy: 0.7681 - val_loss: 0.4797 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.47820\n",
      "Epoch 21/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4960 - categorical_accuracy: 0.7657 - val_loss: 0.4864 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.47820\n",
      "Epoch 22/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4952 - categorical_accuracy: 0.7661 - val_loss: 0.4847 - val_categorical_accuracy: 0.7687\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.47820\n",
      "Epoch 23/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4934 - categorical_accuracy: 0.7677 - val_loss: 0.4797 - val_categorical_accuracy: 0.7710\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.47820\n",
      "Epoch 24/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4923 - categorical_accuracy: 0.7680 - val_loss: 0.4772 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.47820 to 0.47718, saving model to best_model.h5\n",
      "Epoch 25/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4939 - categorical_accuracy: 0.7675 - val_loss: 0.4879 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.47718\n",
      "Epoch 26/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4927 - categorical_accuracy: 0.7678 - val_loss: 0.4799 - val_categorical_accuracy: 0.7705\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.47718\n",
      "Epoch 27/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4923 - categorical_accuracy: 0.7682 - val_loss: 0.4864 - val_categorical_accuracy: 0.7654\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.47718\n",
      "Epoch 28/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4936 - categorical_accuracy: 0.7682 - val_loss: 0.4778 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.47718\n",
      "Epoch 29/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4912 - categorical_accuracy: 0.7679 - val_loss: 0.4825 - val_categorical_accuracy: 0.7649\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.47718\n",
      "Epoch 30/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4916 - categorical_accuracy: 0.7683 - val_loss: 0.4753 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.47718 to 0.47525, saving model to best_model.h5\n",
      "Epoch 31/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4915 - categorical_accuracy: 0.7683 - val_loss: 0.4803 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.47525\n",
      "Epoch 32/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4905 - categorical_accuracy: 0.7696 - val_loss: 0.4767 - val_categorical_accuracy: 0.7805\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.47525\n",
      "Epoch 33/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4944 - categorical_accuracy: 0.7670 - val_loss: 0.4838 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47525\n",
      "Epoch 34/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4897 - categorical_accuracy: 0.7686 - val_loss: 0.4774 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.47525\n",
      "Epoch 35/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4901 - categorical_accuracy: 0.7701 - val_loss: 0.4794 - val_categorical_accuracy: 0.7813\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47525\n",
      "Epoch 36/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4925 - categorical_accuracy: 0.7685 - val_loss: 0.4872 - val_categorical_accuracy: 0.7717\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47525\n",
      "Epoch 37/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4901 - categorical_accuracy: 0.7706 - val_loss: 0.4771 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47525\n",
      "Epoch 38/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4900 - categorical_accuracy: 0.7698 - val_loss: 0.4804 - val_categorical_accuracy: 0.7682\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47525\n",
      "Epoch 39/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4910 - categorical_accuracy: 0.7699 - val_loss: 0.4746 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.47525 to 0.47463, saving model to best_model.h5\n",
      "Epoch 40/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4881 - categorical_accuracy: 0.7704 - val_loss: 0.4740 - val_categorical_accuracy: 0.7808\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.47463 to 0.47404, saving model to best_model.h5\n",
      "Epoch 41/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4897 - categorical_accuracy: 0.7692 - val_loss: 0.4964 - val_categorical_accuracy: 0.7672\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47404\n",
      "Epoch 42/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4895 - categorical_accuracy: 0.7701 - val_loss: 0.4787 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47404\n",
      "Epoch 43/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4887 - categorical_accuracy: 0.7700 - val_loss: 0.4747 - val_categorical_accuracy: 0.7819\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47404\n",
      "Epoch 44/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4881 - categorical_accuracy: 0.7704 - val_loss: 0.4800 - val_categorical_accuracy: 0.7725\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47404\n",
      "Epoch 45/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4889 - categorical_accuracy: 0.7706 - val_loss: 0.4758 - val_categorical_accuracy: 0.7816\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47404\n",
      "Epoch 46/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4889 - categorical_accuracy: 0.7711 - val_loss: 0.4833 - val_categorical_accuracy: 0.7708\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47404\n",
      "Epoch 47/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4888 - categorical_accuracy: 0.7706 - val_loss: 0.4898 - val_categorical_accuracy: 0.7653\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47404\n",
      "Epoch 48/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4871 - categorical_accuracy: 0.7716 - val_loss: 0.4777 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47404\n",
      "Epoch 49/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4886 - categorical_accuracy: 0.7713 - val_loss: 0.4806 - val_categorical_accuracy: 0.7621\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47404\n",
      "Epoch 50/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4894 - categorical_accuracy: 0.7703 - val_loss: 0.4803 - val_categorical_accuracy: 0.7637\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47404\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 51/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4843 - categorical_accuracy: 0.7739 - val_loss: 0.4733 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.47404 to 0.47328, saving model to best_model.h5\n",
      "Epoch 52/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4832 - categorical_accuracy: 0.7751 - val_loss: 0.4745 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47328\n",
      "Epoch 53/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4818 - categorical_accuracy: 0.7748 - val_loss: 0.4748 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47328\n",
      "Epoch 54/250\n",
      "91456/91456 [==============================] - 2s 18us/step - loss: 0.4813 - categorical_accuracy: 0.7754 - val_loss: 0.4727 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.47328 to 0.47266, saving model to best_model.h5\n",
      "Epoch 55/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4829 - categorical_accuracy: 0.7739 - val_loss: 0.4747 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47266\n",
      "Epoch 56/250\n",
      "91456/91456 [==============================] - 1s 13us/step - loss: 0.4812 - categorical_accuracy: 0.7742 - val_loss: 0.4833 - val_categorical_accuracy: 0.7688\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47266\n",
      "Epoch 57/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4816 - categorical_accuracy: 0.7745 - val_loss: 0.4741 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47266\n",
      "Epoch 58/250\n",
      "91456/91456 [==============================] - 1s 16us/step - loss: 0.4815 - categorical_accuracy: 0.7750 - val_loss: 0.4724 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.47266 to 0.47242, saving model to best_model.h5\n",
      "Epoch 59/250\n",
      "91456/91456 [==============================] - 2s 17us/step - loss: 0.4811 - categorical_accuracy: 0.7760 - val_loss: 0.4724 - val_categorical_accuracy: 0.7817\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.47242 to 0.47242, saving model to best_model.h5\n",
      "Epoch 60/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4809 - categorical_accuracy: 0.7758 - val_loss: 0.4708 - val_categorical_accuracy: 0.7818\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.47242 to 0.47079, saving model to best_model.h5\n",
      "Epoch 61/250\n",
      "91456/91456 [==============================] - 1s 16us/step - loss: 0.4809 - categorical_accuracy: 0.7762 - val_loss: 0.4712 - val_categorical_accuracy: 0.7816\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47079\n",
      "Epoch 62/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4804 - categorical_accuracy: 0.7761 - val_loss: 0.4715 - val_categorical_accuracy: 0.7808\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47079\n",
      "Epoch 63/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4808 - categorical_accuracy: 0.7758 - val_loss: 0.4702 - val_categorical_accuracy: 0.7814\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.47079 to 0.47015, saving model to best_model.h5\n",
      "Epoch 64/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4803 - categorical_accuracy: 0.7757 - val_loss: 0.4745 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47015\n",
      "Epoch 65/250\n",
      "91456/91456 [==============================] - 1s 16us/step - loss: 0.4794 - categorical_accuracy: 0.7764 - val_loss: 0.4737 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47015\n",
      "Epoch 66/250\n",
      "91456/91456 [==============================] - 1s 16us/step - loss: 0.4797 - categorical_accuracy: 0.7759 - val_loss: 0.4701 - val_categorical_accuracy: 0.7819\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.47015 to 0.47008, saving model to best_model.h5\n",
      "Epoch 67/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4800 - categorical_accuracy: 0.7765 - val_loss: 0.4688 - val_categorical_accuracy: 0.7824\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.47008 to 0.46878, saving model to best_model.h5\n",
      "Epoch 68/250\n",
      "91456/91456 [==============================] - 1s 16us/step - loss: 0.4806 - categorical_accuracy: 0.7752 - val_loss: 0.4739 - val_categorical_accuracy: 0.7776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00068: val_loss did not improve from 0.46878\n",
      "Epoch 69/250\n",
      "91456/91456 [==============================] - 1s 16us/step - loss: 0.4795 - categorical_accuracy: 0.7766 - val_loss: 0.4708 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.46878\n",
      "Epoch 70/250\n",
      "91456/91456 [==============================] - 1s 16us/step - loss: 0.4811 - categorical_accuracy: 0.7762 - val_loss: 0.4734 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.46878\n",
      "Epoch 71/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4797 - categorical_accuracy: 0.7753 - val_loss: 0.4735 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.46878\n",
      "Epoch 72/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4803 - categorical_accuracy: 0.7770 - val_loss: 0.4701 - val_categorical_accuracy: 0.7817\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.46878\n",
      "Epoch 73/250\n",
      "91456/91456 [==============================] - 1s 16us/step - loss: 0.4797 - categorical_accuracy: 0.7767 - val_loss: 0.4724 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.46878\n",
      "Epoch 74/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4791 - categorical_accuracy: 0.7770 - val_loss: 0.4726 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.46878\n",
      "Epoch 75/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4794 - categorical_accuracy: 0.7762 - val_loss: 0.4737 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.46878\n",
      "Epoch 76/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4795 - categorical_accuracy: 0.7769 - val_loss: 0.4711 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.46878\n",
      "Epoch 77/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4799 - categorical_accuracy: 0.7757 - val_loss: 0.4699 - val_categorical_accuracy: 0.7829\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.46878\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 78/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4796 - categorical_accuracy: 0.7761 - val_loss: 0.4711 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.46878\n",
      "Epoch 79/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4788 - categorical_accuracy: 0.7762 - val_loss: 0.4716 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.46878\n",
      "Epoch 80/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4775 - categorical_accuracy: 0.7769 - val_loss: 0.4710 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.46878\n",
      "Epoch 81/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4795 - categorical_accuracy: 0.7768 - val_loss: 0.4717 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.46878\n",
      "Epoch 82/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4782 - categorical_accuracy: 0.7771 - val_loss: 0.4712 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.46878\n",
      "Epoch 83/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4790 - categorical_accuracy: 0.7769 - val_loss: 0.4715 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.46878\n",
      "Epoch 84/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4778 - categorical_accuracy: 0.7767 - val_loss: 0.4709 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.46878\n",
      "Epoch 85/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4787 - categorical_accuracy: 0.7776 - val_loss: 0.4714 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.46878\n",
      "Epoch 86/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4773 - categorical_accuracy: 0.7776 - val_loss: 0.4710 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.46878\n",
      "Epoch 87/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4783 - categorical_accuracy: 0.7766 - val_loss: 0.4711 - val_categorical_accuracy: 0.7796\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.46878\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 00087: early stopping\n",
      "the 1 fold Log-Loss (NN) is 0.468781\n",
      "-----------\n",
      "Loop 1/2 Fold 2/5\n",
      "-----------\n",
      "WARNING:tensorflow:Large dropout rate: 0.625 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 3s 31us/step - loss: 0.5371 - categorical_accuracy: 0.7566 - val_loss: 0.6581 - val_categorical_accuracy: 0.7579\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65811, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.5182 - categorical_accuracy: 0.7608 - val_loss: 0.5612 - val_categorical_accuracy: 0.7579\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65811 to 0.56115, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.5105 - categorical_accuracy: 0.7608 - val_loss: 0.5235 - val_categorical_accuracy: 0.7579\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56115 to 0.52349, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.5076 - categorical_accuracy: 0.7610 - val_loss: 0.5078 - val_categorical_accuracy: 0.7602\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52349 to 0.50784, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.5019 - categorical_accuracy: 0.7618 - val_loss: 0.5409 - val_categorical_accuracy: 0.7579\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.50784\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.5008 - categorical_accuracy: 0.7629 - val_loss: 0.5867 - val_categorical_accuracy: 0.7579\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.50784\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.5000 - categorical_accuracy: 0.7633 - val_loss: 0.5136 - val_categorical_accuracy: 0.7580\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.50784\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4980 - categorical_accuracy: 0.7640 - val_loss: 0.5065 - val_categorical_accuracy: 0.7682\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.50784 to 0.50652, saving model to best_model.h5\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4984 - categorical_accuracy: 0.7637 - val_loss: 0.4940 - val_categorical_accuracy: 0.7703\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50652 to 0.49403, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4988 - categorical_accuracy: 0.7647 - val_loss: 0.5030 - val_categorical_accuracy: 0.7579\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.49403\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4957 - categorical_accuracy: 0.7657 - val_loss: 0.4913 - val_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.49403 to 0.49131, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4934 - categorical_accuracy: 0.7666 - val_loss: 0.4981 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.49131\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4947 - categorical_accuracy: 0.7663 - val_loss: 0.5036 - val_categorical_accuracy: 0.7621\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49131\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4940 - categorical_accuracy: 0.7679 - val_loss: 0.4989 - val_categorical_accuracy: 0.7726\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.49131\n",
      "Epoch 15/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4929 - categorical_accuracy: 0.7681 - val_loss: 0.5213 - val_categorical_accuracy: 0.7579\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.49131\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4929 - categorical_accuracy: 0.7671 - val_loss: 0.4951 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.49131\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4929 - categorical_accuracy: 0.7671 - val_loss: 0.4995 - val_categorical_accuracy: 0.7672\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.49131\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4943 - categorical_accuracy: 0.7679 - val_loss: 0.5024 - val_categorical_accuracy: 0.7657\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.49131\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4935 - categorical_accuracy: 0.7693 - val_loss: 0.4945 - val_categorical_accuracy: 0.7579\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.49131\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4910 - categorical_accuracy: 0.7694 - val_loss: 0.5086 - val_categorical_accuracy: 0.7592\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.49131\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4907 - categorical_accuracy: 0.7679 - val_loss: 0.4906 - val_categorical_accuracy: 0.7728\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.49131 to 0.49060, saving model to best_model.h5\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4944 - categorical_accuracy: 0.7671 - val_loss: 0.4961 - val_categorical_accuracy: 0.7685\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.49060\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4900 - categorical_accuracy: 0.7699 - val_loss: 0.5183 - val_categorical_accuracy: 0.7579\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.49060\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4914 - categorical_accuracy: 0.7688 - val_loss: 0.4974 - val_categorical_accuracy: 0.7635\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.49060\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4911 - categorical_accuracy: 0.7686 - val_loss: 0.4909 - val_categorical_accuracy: 0.7709\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.49060\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4908 - categorical_accuracy: 0.7691 - val_loss: 0.5005 - val_categorical_accuracy: 0.7650\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.49060\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4893 - categorical_accuracy: 0.7704 - val_loss: 0.4919 - val_categorical_accuracy: 0.7732\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.49060\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4888 - categorical_accuracy: 0.7698 - val_loss: 0.4905 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.49060 to 0.49054, saving model to best_model.h5\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4894 - categorical_accuracy: 0.7698 - val_loss: 0.4924 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.49054\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4894 - categorical_accuracy: 0.7701 - val_loss: 0.5100 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.49054\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4924 - categorical_accuracy: 0.7682 - val_loss: 0.4925 - val_categorical_accuracy: 0.7626\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.49054\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4871 - categorical_accuracy: 0.7705 - val_loss: 0.4897 - val_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.49054 to 0.48968, saving model to best_model.h5\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4844 - categorical_accuracy: 0.7723 - val_loss: 0.4890 - val_categorical_accuracy: 0.7675\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.48968 to 0.48896, saving model to best_model.h5\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4839 - categorical_accuracy: 0.7723 - val_loss: 0.4891 - val_categorical_accuracy: 0.7687\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.48896\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4836 - categorical_accuracy: 0.7737 - val_loss: 0.4885 - val_categorical_accuracy: 0.7699\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.48896 to 0.48848, saving model to best_model.h5\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4832 - categorical_accuracy: 0.7729 - val_loss: 0.4893 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.48848\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4833 - categorical_accuracy: 0.7742 - val_loss: 0.4911 - val_categorical_accuracy: 0.7709\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.48848\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4822 - categorical_accuracy: 0.7745 - val_loss: 0.4934 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.48848\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4832 - categorical_accuracy: 0.7732 - val_loss: 0.4947 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.48848\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4817 - categorical_accuracy: 0.7745 - val_loss: 0.4998 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.48848\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4817 - categorical_accuracy: 0.7746 - val_loss: 0.5064 - val_categorical_accuracy: 0.7652\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.48848\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4815 - categorical_accuracy: 0.7756 - val_loss: 0.4961 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.48848\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4810 - categorical_accuracy: 0.7762 - val_loss: 0.4991 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.48848\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4815 - categorical_accuracy: 0.7751 - val_loss: 0.5024 - val_categorical_accuracy: 0.7734\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.48848\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4808 - categorical_accuracy: 0.7759 - val_loss: 0.5056 - val_categorical_accuracy: 0.7689\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.48848\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4802 - categorical_accuracy: 0.7759 - val_loss: 0.4990 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.48848\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4808 - categorical_accuracy: 0.7765 - val_loss: 0.5015 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.48848\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4798 - categorical_accuracy: 0.7762 - val_loss: 0.5065 - val_categorical_accuracy: 0.7726\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.48848\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4798 - categorical_accuracy: 0.7761 - val_loss: 0.5092 - val_categorical_accuracy: 0.7714\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.48848\n",
      "Epoch 50/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4805 - categorical_accuracy: 0.7764 - val_loss: 0.5019 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.48848\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4791 - categorical_accuracy: 0.7765 - val_loss: 0.5064 - val_categorical_accuracy: 0.7718\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.48848\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4808 - categorical_accuracy: 0.7757 - val_loss: 0.5099 - val_categorical_accuracy: 0.7729\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.48848\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4801 - categorical_accuracy: 0.7758 - val_loss: 0.5129 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.48848\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4795 - categorical_accuracy: 0.7760 - val_loss: 0.4917 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.48848\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4790 - categorical_accuracy: 0.7770 - val_loss: 0.4948 - val_categorical_accuracy: 0.7735\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.48848\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 00055: early stopping\n",
      "the 2 fold Log-Loss (NN) is 0.487584\n",
      "-----------\n",
      "Loop 1/2 Fold 3/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 2s 26us/step - loss: 0.5361 - categorical_accuracy: 0.7550 - val_loss: 0.8120 - val_categorical_accuracy: 0.7625\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.81202, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.5191 - categorical_accuracy: 0.7597 - val_loss: 0.5421 - val_categorical_accuracy: 0.7623\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.81202 to 0.54211, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.5095 - categorical_accuracy: 0.7600 - val_loss: 0.5441 - val_categorical_accuracy: 0.7625\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54211\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.5062 - categorical_accuracy: 0.7601 - val_loss: 0.6037 - val_categorical_accuracy: 0.7623\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.54211\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.5020 - categorical_accuracy: 0.7613 - val_loss: 0.5383 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54211 to 0.53825, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.5034 - categorical_accuracy: 0.7613 - val_loss: 0.5141 - val_categorical_accuracy: 0.7625\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.53825 to 0.51413, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.5002 - categorical_accuracy: 0.7623 - val_loss: 0.4863 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51413 to 0.48634, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4982 - categorical_accuracy: 0.7629 - val_loss: 0.5410 - val_categorical_accuracy: 0.7681\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48634\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.5005 - categorical_accuracy: 0.7615 - val_loss: 0.5350 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48634\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.5004 - categorical_accuracy: 0.7616 - val_loss: 0.5226 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48634\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4983 - categorical_accuracy: 0.7633 - val_loss: 0.4968 - val_categorical_accuracy: 0.7613\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48634\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4982 - categorical_accuracy: 0.7625 - val_loss: 0.4865 - val_categorical_accuracy: 0.7627\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.48634\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4981 - categorical_accuracy: 0.7626 - val_loss: 0.4873 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.48634\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4944 - categorical_accuracy: 0.7659 - val_loss: 0.4849 - val_categorical_accuracy: 0.7732\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.48634 to 0.48494, saving model to best_model.h5\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4975 - categorical_accuracy: 0.7659 - val_loss: 0.4972 - val_categorical_accuracy: 0.7656\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.48494\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4928 - categorical_accuracy: 0.7667 - val_loss: 0.4870 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.48494\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4932 - categorical_accuracy: 0.7676 - val_loss: 0.5022 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.48494\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4932 - categorical_accuracy: 0.7662 - val_loss: 0.4882 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.48494\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4925 - categorical_accuracy: 0.7670 - val_loss: 0.4859 - val_categorical_accuracy: 0.7626\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.48494\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4936 - categorical_accuracy: 0.7678 - val_loss: 0.4884 - val_categorical_accuracy: 0.7664\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.48494\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4917 - categorical_accuracy: 0.7688 - val_loss: 0.4830 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.48494 to 0.48298, saving model to best_model.h5\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4930 - categorical_accuracy: 0.7694 - val_loss: 0.4904 - val_categorical_accuracy: 0.7708\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.48298\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4918 - categorical_accuracy: 0.7696 - val_loss: 0.4855 - val_categorical_accuracy: 0.7662\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.48298\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4898 - categorical_accuracy: 0.7697 - val_loss: 0.4894 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48298\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4908 - categorical_accuracy: 0.7690 - val_loss: 0.4884 - val_categorical_accuracy: 0.7653\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.48298\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4928 - categorical_accuracy: 0.7682 - val_loss: 0.4870 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.48298\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4900 - categorical_accuracy: 0.7696 - val_loss: 0.4811 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.48298 to 0.48108, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4899 - categorical_accuracy: 0.7705 - val_loss: 0.4877 - val_categorical_accuracy: 0.7702\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.48108\n",
      "Epoch 29/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4922 - categorical_accuracy: 0.7688 - val_loss: 0.4823 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.48108\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4889 - categorical_accuracy: 0.7704 - val_loss: 0.4780 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.48108 to 0.47796, saving model to best_model.h5\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4890 - categorical_accuracy: 0.7698 - val_loss: 0.4872 - val_categorical_accuracy: 0.7710\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.47796\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4895 - categorical_accuracy: 0.7713 - val_loss: 0.4804 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.47796\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4917 - categorical_accuracy: 0.7681 - val_loss: 0.5163 - val_categorical_accuracy: 0.7629\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47796\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4886 - categorical_accuracy: 0.7714 - val_loss: 0.4813 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.47796\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4892 - categorical_accuracy: 0.7689 - val_loss: 0.4867 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47796\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4901 - categorical_accuracy: 0.7703 - val_loss: 0.4918 - val_categorical_accuracy: 0.7644\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47796\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4877 - categorical_accuracy: 0.7718 - val_loss: 0.4782 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47796\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4897 - categorical_accuracy: 0.7694 - val_loss: 0.4868 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47796\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4889 - categorical_accuracy: 0.7700 - val_loss: 0.4811 - val_categorical_accuracy: 0.7725\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.47796\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4869 - categorical_accuracy: 0.7718 - val_loss: 0.4795 - val_categorical_accuracy: 0.7736\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.47796\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4829 - categorical_accuracy: 0.7740 - val_loss: 0.4795 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47796\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4821 - categorical_accuracy: 0.7747 - val_loss: 0.4800 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47796\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4819 - categorical_accuracy: 0.7745 - val_loss: 0.4808 - val_categorical_accuracy: 0.7732\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47796\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4813 - categorical_accuracy: 0.7751 - val_loss: 0.4786 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47796\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4814 - categorical_accuracy: 0.7746 - val_loss: 0.4772 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.47796 to 0.47724, saving model to best_model.h5\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4811 - categorical_accuracy: 0.7755 - val_loss: 0.4786 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47724\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4806 - categorical_accuracy: 0.7759 - val_loss: 0.4785 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47724\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4807 - categorical_accuracy: 0.7751 - val_loss: 0.4787 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47724\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4804 - categorical_accuracy: 0.7758 - val_loss: 0.4769 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.47724 to 0.47690, saving model to best_model.h5\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4802 - categorical_accuracy: 0.7757 - val_loss: 0.4794 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47690\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4800 - categorical_accuracy: 0.7759 - val_loss: 0.4770 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47690\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4801 - categorical_accuracy: 0.7762 - val_loss: 0.4809 - val_categorical_accuracy: 0.7717\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47690\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4806 - categorical_accuracy: 0.7771 - val_loss: 0.4786 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47690\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4793 - categorical_accuracy: 0.7770 - val_loss: 0.4796 - val_categorical_accuracy: 0.7719\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47690\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4785 - categorical_accuracy: 0.7768 - val_loss: 0.4797 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47690\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4790 - categorical_accuracy: 0.7775 - val_loss: 0.4763 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.47690 to 0.47630, saving model to best_model.h5\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4791 - categorical_accuracy: 0.7773 - val_loss: 0.4814 - val_categorical_accuracy: 0.7719\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47630\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4791 - categorical_accuracy: 0.7775 - val_loss: 0.4761 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.47630 to 0.47610, saving model to best_model.h5\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4791 - categorical_accuracy: 0.7763 - val_loss: 0.4756 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.47610 to 0.47560, saving model to best_model.h5\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4790 - categorical_accuracy: 0.7769 - val_loss: 0.4784 - val_categorical_accuracy: 0.7732\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47560\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4785 - categorical_accuracy: 0.7761 - val_loss: 0.4761 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47560\n",
      "Epoch 62/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4788 - categorical_accuracy: 0.7774 - val_loss: 0.4761 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47560\n",
      "Epoch 63/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4790 - categorical_accuracy: 0.7764 - val_loss: 0.4763 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47560\n",
      "Epoch 64/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4779 - categorical_accuracy: 0.7773 - val_loss: 0.4772 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47560\n",
      "Epoch 65/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4786 - categorical_accuracy: 0.7773 - val_loss: 0.4829 - val_categorical_accuracy: 0.7716\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47560\n",
      "Epoch 66/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4783 - categorical_accuracy: 0.7762 - val_loss: 0.4765 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47560\n",
      "Epoch 67/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4782 - categorical_accuracy: 0.7770 - val_loss: 0.4759 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.47560\n",
      "Epoch 68/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4789 - categorical_accuracy: 0.7774 - val_loss: 0.4759 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.47560\n",
      "Epoch 69/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4786 - categorical_accuracy: 0.7778 - val_loss: 0.4778 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47560\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 70/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4783 - categorical_accuracy: 0.7775 - val_loss: 0.4767 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47560\n",
      "Epoch 71/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4777 - categorical_accuracy: 0.7772 - val_loss: 0.4757 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47560\n",
      "Epoch 72/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4781 - categorical_accuracy: 0.7774 - val_loss: 0.4759 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.47560\n",
      "Epoch 73/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4779 - categorical_accuracy: 0.7775 - val_loss: 0.4767 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47560\n",
      "Epoch 74/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4773 - categorical_accuracy: 0.7778 - val_loss: 0.4755 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.47560 to 0.47549, saving model to best_model.h5\n",
      "Epoch 75/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4781 - categorical_accuracy: 0.7783 - val_loss: 0.4756 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.47549\n",
      "Epoch 76/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4775 - categorical_accuracy: 0.7772 - val_loss: 0.4755 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.47549\n",
      "Epoch 77/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4778 - categorical_accuracy: 0.7776 - val_loss: 0.4760 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.47549\n",
      "Epoch 78/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4771 - categorical_accuracy: 0.7778 - val_loss: 0.4759 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.47549\n",
      "Epoch 79/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4772 - categorical_accuracy: 0.7782 - val_loss: 0.4763 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.47549\n",
      "Epoch 80/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4774 - categorical_accuracy: 0.7774 - val_loss: 0.4757 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.47549\n",
      "Epoch 81/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4774 - categorical_accuracy: 0.7778 - val_loss: 0.4762 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.47549\n",
      "Epoch 82/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4774 - categorical_accuracy: 0.7773 - val_loss: 0.4757 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.47549\n",
      "Epoch 83/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4773 - categorical_accuracy: 0.7772 - val_loss: 0.4753 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.47549 to 0.47534, saving model to best_model.h5\n",
      "Epoch 84/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4784 - categorical_accuracy: 0.7768 - val_loss: 0.4756 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.47534\n",
      "Epoch 85/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4775 - categorical_accuracy: 0.7778 - val_loss: 0.4757 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.47534\n",
      "Epoch 86/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4777 - categorical_accuracy: 0.7774 - val_loss: 0.4756 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.47534\n",
      "Epoch 87/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4771 - categorical_accuracy: 0.7774 - val_loss: 0.4757 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.47534\n",
      "Epoch 88/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4769 - categorical_accuracy: 0.7776 - val_loss: 0.4759 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.47534\n",
      "Epoch 89/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4783 - categorical_accuracy: 0.7778 - val_loss: 0.4756 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.47534\n",
      "Epoch 90/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4777 - categorical_accuracy: 0.7771 - val_loss: 0.4762 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.47534\n",
      "Epoch 91/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4777 - categorical_accuracy: 0.7773 - val_loss: 0.4757 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.47534\n",
      "Epoch 92/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4774 - categorical_accuracy: 0.7773 - val_loss: 0.4755 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.47534\n",
      "Epoch 93/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4776 - categorical_accuracy: 0.7772 - val_loss: 0.4754 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.47534\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 94/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4770 - categorical_accuracy: 0.7772 - val_loss: 0.4756 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.47534\n",
      "Epoch 95/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4769 - categorical_accuracy: 0.7776 - val_loss: 0.4756 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.47534\n",
      "Epoch 96/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4778 - categorical_accuracy: 0.7772 - val_loss: 0.4755 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.47534\n",
      "Epoch 97/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4772 - categorical_accuracy: 0.7774 - val_loss: 0.4755 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.47534\n",
      "Epoch 98/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4779 - categorical_accuracy: 0.7777 - val_loss: 0.4757 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.47534\n",
      "Epoch 99/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4773 - categorical_accuracy: 0.7774 - val_loss: 0.4755 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.47534\n",
      "Epoch 100/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4777 - categorical_accuracy: 0.7773 - val_loss: 0.4756 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.47534\n",
      "Epoch 101/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4773 - categorical_accuracy: 0.7771 - val_loss: 0.4757 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.47534\n",
      "Epoch 102/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4770 - categorical_accuracy: 0.7772 - val_loss: 0.4759 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.47534\n",
      "Epoch 103/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4774 - categorical_accuracy: 0.7770 - val_loss: 0.4756 - val_categorical_accuracy: 0.7761\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.47534\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 00103: early stopping\n",
      "the 3 fold Log-Loss (NN) is 0.475339\n",
      "-----------\n",
      "Loop 1/2 Fold 4/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 3s 29us/step - loss: 0.5470 - categorical_accuracy: 0.7555 - val_loss: 0.7113 - val_categorical_accuracy: 0.7608\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.71134, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.5203 - categorical_accuracy: 0.7612 - val_loss: 0.5300 - val_categorical_accuracy: 0.7608\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.71134 to 0.53002, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.5107 - categorical_accuracy: 0.7611 - val_loss: 0.5227 - val_categorical_accuracy: 0.7553\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53002 to 0.52272, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.5057 - categorical_accuracy: 0.7601 - val_loss: 0.5281 - val_categorical_accuracy: 0.7608\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.52272\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.5025 - categorical_accuracy: 0.7619 - val_loss: 0.5431 - val_categorical_accuracy: 0.7608\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.52272\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.5022 - categorical_accuracy: 0.7618 - val_loss: 0.5560 - val_categorical_accuracy: 0.7608\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.52272\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.5006 - categorical_accuracy: 0.7619 - val_loss: 0.4873 - val_categorical_accuracy: 0.7716\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.52272 to 0.48730, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4993 - categorical_accuracy: 0.7630 - val_loss: 0.5020 - val_categorical_accuracy: 0.7608\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48730\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4983 - categorical_accuracy: 0.7643 - val_loss: 0.5263 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48730\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4989 - categorical_accuracy: 0.7635 - val_loss: 0.5296 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48730\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4988 - categorical_accuracy: 0.7635 - val_loss: 0.5103 - val_categorical_accuracy: 0.7696\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48730\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4963 - categorical_accuracy: 0.7648 - val_loss: 0.4900 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.48730\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4962 - categorical_accuracy: 0.7642 - val_loss: 0.4968 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.48730\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4962 - categorical_accuracy: 0.7655 - val_loss: 0.5030 - val_categorical_accuracy: 0.7608\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.48730\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4950 - categorical_accuracy: 0.7673 - val_loss: 0.4878 - val_categorical_accuracy: 0.7712\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.48730\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4945 - categorical_accuracy: 0.7667 - val_loss: 0.4917 - val_categorical_accuracy: 0.7708\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.48730\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4938 - categorical_accuracy: 0.7678 - val_loss: 0.4827 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.48730 to 0.48271, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4936 - categorical_accuracy: 0.7665 - val_loss: 0.5033 - val_categorical_accuracy: 0.7720\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.48271\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4921 - categorical_accuracy: 0.7681 - val_loss: 0.4877 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.48271\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4950 - categorical_accuracy: 0.7665 - val_loss: 0.4872 - val_categorical_accuracy: 0.7612\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.48271\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4946 - categorical_accuracy: 0.7651 - val_loss: 0.4825 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.48271 to 0.48251, saving model to best_model.h5\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4960 - categorical_accuracy: 0.7660 - val_loss: 0.4904 - val_categorical_accuracy: 0.7610\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.48251\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4920 - categorical_accuracy: 0.7674 - val_loss: 0.4922 - val_categorical_accuracy: 0.7618\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.48251\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4923 - categorical_accuracy: 0.7662 - val_loss: 0.4956 - val_categorical_accuracy: 0.7712\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48251\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4930 - categorical_accuracy: 0.7669 - val_loss: 0.4868 - val_categorical_accuracy: 0.7608\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.48251\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4933 - categorical_accuracy: 0.7674 - val_loss: 0.4837 - val_categorical_accuracy: 0.7653\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.48251\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4948 - categorical_accuracy: 0.7667 - val_loss: 0.4801 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.48251 to 0.48007, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4918 - categorical_accuracy: 0.7678 - val_loss: 0.4915 - val_categorical_accuracy: 0.7643\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.48007\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4904 - categorical_accuracy: 0.7688 - val_loss: 0.4813 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.48007\n",
      "Epoch 30/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4907 - categorical_accuracy: 0.7696 - val_loss: 0.5011 - val_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.48007\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4905 - categorical_accuracy: 0.7698 - val_loss: 0.4840 - val_categorical_accuracy: 0.7662\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.48007\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4910 - categorical_accuracy: 0.7703 - val_loss: 0.4812 - val_categorical_accuracy: 0.7717\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.48007\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4900 - categorical_accuracy: 0.7695 - val_loss: 0.4893 - val_categorical_accuracy: 0.7646\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.48007\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4897 - categorical_accuracy: 0.7697 - val_loss: 0.4774 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.48007 to 0.47744, saving model to best_model.h5\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4894 - categorical_accuracy: 0.7707 - val_loss: 0.4854 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47744\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4891 - categorical_accuracy: 0.7703 - val_loss: 0.4927 - val_categorical_accuracy: 0.7657\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47744\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4892 - categorical_accuracy: 0.7704 - val_loss: 0.4797 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47744\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4882 - categorical_accuracy: 0.7702 - val_loss: 0.4876 - val_categorical_accuracy: 0.7655\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47744\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4888 - categorical_accuracy: 0.7716 - val_loss: 0.4856 - val_categorical_accuracy: 0.7687\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.47744\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4878 - categorical_accuracy: 0.7718 - val_loss: 0.4853 - val_categorical_accuracy: 0.7717\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.47744\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4885 - categorical_accuracy: 0.7711 - val_loss: 0.4771 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.47744 to 0.47708, saving model to best_model.h5\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4883 - categorical_accuracy: 0.7706 - val_loss: 0.4767 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.47708 to 0.47667, saving model to best_model.h5\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4904 - categorical_accuracy: 0.7701 - val_loss: 0.4820 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47667\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4879 - categorical_accuracy: 0.7710 - val_loss: 0.4858 - val_categorical_accuracy: 0.7661\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47667\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4888 - categorical_accuracy: 0.7709 - val_loss: 0.4839 - val_categorical_accuracy: 0.7652\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47667\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4870 - categorical_accuracy: 0.7723 - val_loss: 0.4800 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47667\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4866 - categorical_accuracy: 0.7723 - val_loss: 0.4820 - val_categorical_accuracy: 0.7693\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47667\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4873 - categorical_accuracy: 0.7730 - val_loss: 0.4771 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47667\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4878 - categorical_accuracy: 0.7712 - val_loss: 0.4779 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47667\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4858 - categorical_accuracy: 0.7722 - val_loss: 0.4736 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.47667 to 0.47363, saving model to best_model.h5\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4853 - categorical_accuracy: 0.7727 - val_loss: 0.4748 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47363\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4885 - categorical_accuracy: 0.7719 - val_loss: 0.4933 - val_categorical_accuracy: 0.7608\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47363\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4866 - categorical_accuracy: 0.7727 - val_loss: 0.4837 - val_categorical_accuracy: 0.7683\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47363\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4862 - categorical_accuracy: 0.7720 - val_loss: 0.4833 - val_categorical_accuracy: 0.7720\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47363\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4855 - categorical_accuracy: 0.7727 - val_loss: 0.4740 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47363\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4855 - categorical_accuracy: 0.7723 - val_loss: 0.5074 - val_categorical_accuracy: 0.7710\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47363\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4860 - categorical_accuracy: 0.7725 - val_loss: 0.4818 - val_categorical_accuracy: 0.7702\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47363\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4862 - categorical_accuracy: 0.7722 - val_loss: 0.4795 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47363\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4857 - categorical_accuracy: 0.7725 - val_loss: 0.4824 - val_categorical_accuracy: 0.7691\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47363\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4869 - categorical_accuracy: 0.7722 - val_loss: 0.4800 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47363\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4826 - categorical_accuracy: 0.7739 - val_loss: 0.4739 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47363\n",
      "Epoch 62/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4807 - categorical_accuracy: 0.7752 - val_loss: 0.4785 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47363\n",
      "Epoch 63/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4797 - categorical_accuracy: 0.7755 - val_loss: 0.4786 - val_categorical_accuracy: 0.7738\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47363\n",
      "Epoch 64/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4807 - categorical_accuracy: 0.7757 - val_loss: 0.4766 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47363\n",
      "Epoch 65/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4796 - categorical_accuracy: 0.7762 - val_loss: 0.4755 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47363\n",
      "Epoch 66/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4796 - categorical_accuracy: 0.7757 - val_loss: 0.4743 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47363\n",
      "Epoch 67/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4795 - categorical_accuracy: 0.7763 - val_loss: 0.4744 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.47363\n",
      "Epoch 68/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4789 - categorical_accuracy: 0.7768 - val_loss: 0.4758 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.47363\n",
      "Epoch 69/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4790 - categorical_accuracy: 0.7762 - val_loss: 0.4783 - val_categorical_accuracy: 0.7708\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47363\n",
      "Epoch 70/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4782 - categorical_accuracy: 0.7771 - val_loss: 0.4726 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.47363 to 0.47259, saving model to best_model.h5\n",
      "Epoch 71/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4789 - categorical_accuracy: 0.7761 - val_loss: 0.4761 - val_categorical_accuracy: 0.7728\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47259\n",
      "Epoch 72/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4784 - categorical_accuracy: 0.7767 - val_loss: 0.4779 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.47259\n",
      "Epoch 73/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4781 - categorical_accuracy: 0.7766 - val_loss: 0.4758 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47259\n",
      "Epoch 74/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4777 - categorical_accuracy: 0.7767 - val_loss: 0.4735 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.47259\n",
      "Epoch 75/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4781 - categorical_accuracy: 0.7775 - val_loss: 0.4769 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.47259\n",
      "Epoch 76/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4778 - categorical_accuracy: 0.7764 - val_loss: 0.4723 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.47259 to 0.47230, saving model to best_model.h5\n",
      "Epoch 77/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4784 - categorical_accuracy: 0.7768 - val_loss: 0.4761 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.47230\n",
      "Epoch 78/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4785 - categorical_accuracy: 0.7766 - val_loss: 0.4725 - val_categorical_accuracy: 0.7818\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.47230\n",
      "Epoch 79/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4784 - categorical_accuracy: 0.7774 - val_loss: 0.4717 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.47230 to 0.47174, saving model to best_model.h5\n",
      "Epoch 80/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4773 - categorical_accuracy: 0.7774 - val_loss: 0.4754 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.47174\n",
      "Epoch 81/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4777 - categorical_accuracy: 0.7780 - val_loss: 0.4725 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.47174\n",
      "Epoch 82/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4776 - categorical_accuracy: 0.7769 - val_loss: 0.4739 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.47174\n",
      "Epoch 83/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4773 - categorical_accuracy: 0.7775 - val_loss: 0.4723 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.47174\n",
      "Epoch 84/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4789 - categorical_accuracy: 0.7773 - val_loss: 0.4744 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.47174\n",
      "Epoch 85/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4774 - categorical_accuracy: 0.7774 - val_loss: 0.4741 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.47174\n",
      "Epoch 86/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4780 - categorical_accuracy: 0.7769 - val_loss: 0.4755 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.47174\n",
      "Epoch 87/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4778 - categorical_accuracy: 0.7769 - val_loss: 0.4726 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.47174\n",
      "Epoch 88/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4780 - categorical_accuracy: 0.7768 - val_loss: 0.4711 - val_categorical_accuracy: 0.7817\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.47174 to 0.47107, saving model to best_model.h5\n",
      "Epoch 89/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4777 - categorical_accuracy: 0.7775 - val_loss: 0.4741 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.47107\n",
      "Epoch 90/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4767 - categorical_accuracy: 0.7781 - val_loss: 0.4740 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.47107\n",
      "Epoch 91/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4768 - categorical_accuracy: 0.7767 - val_loss: 0.4737 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.47107\n",
      "Epoch 92/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4774 - categorical_accuracy: 0.7770 - val_loss: 0.4743 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.47107\n",
      "Epoch 93/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4771 - categorical_accuracy: 0.7779 - val_loss: 0.4733 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.47107\n",
      "Epoch 94/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4775 - categorical_accuracy: 0.7766 - val_loss: 0.4737 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.47107\n",
      "Epoch 95/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4769 - categorical_accuracy: 0.7769 - val_loss: 0.4750 - val_categorical_accuracy: 0.7731\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.47107\n",
      "Epoch 96/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4770 - categorical_accuracy: 0.7779 - val_loss: 0.4742 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.47107\n",
      "Epoch 97/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4779 - categorical_accuracy: 0.7771 - val_loss: 0.4747 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.47107\n",
      "Epoch 98/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4772 - categorical_accuracy: 0.7784 - val_loss: 0.4745 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.47107\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 99/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4763 - categorical_accuracy: 0.7771 - val_loss: 0.4727 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.47107\n",
      "Epoch 100/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4757 - categorical_accuracy: 0.7775 - val_loss: 0.4726 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.47107\n",
      "Epoch 101/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4758 - categorical_accuracy: 0.7780 - val_loss: 0.4726 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.47107\n",
      "Epoch 102/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4764 - categorical_accuracy: 0.7782 - val_loss: 0.4727 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.47107\n",
      "Epoch 103/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4766 - categorical_accuracy: 0.7781 - val_loss: 0.4724 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.47107\n",
      "Epoch 104/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4766 - categorical_accuracy: 0.7775 - val_loss: 0.4730 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.47107\n",
      "Epoch 105/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4766 - categorical_accuracy: 0.7775 - val_loss: 0.4730 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.47107\n",
      "Epoch 106/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4770 - categorical_accuracy: 0.7773 - val_loss: 0.4728 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.47107\n",
      "Epoch 107/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4764 - categorical_accuracy: 0.7771 - val_loss: 0.4734 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.47107\n",
      "Epoch 108/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4755 - categorical_accuracy: 0.7784 - val_loss: 0.4727 - val_categorical_accuracy: 0.7773\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.47107\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 00108: early stopping\n",
      "the 4 fold Log-Loss (NN) is 0.471068\n",
      "-----------\n",
      "Loop 1/2 Fold 5/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 3s 28us/step - loss: 0.5380 - categorical_accuracy: 0.7556 - val_loss: 0.6350 - val_categorical_accuracy: 0.7630\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63503, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.5174 - categorical_accuracy: 0.7604 - val_loss: 0.5862 - val_categorical_accuracy: 0.7629\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63503 to 0.58620, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.5089 - categorical_accuracy: 0.7599 - val_loss: 0.5937 - val_categorical_accuracy: 0.7630\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.58620\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.5054 - categorical_accuracy: 0.7603 - val_loss: 0.5449 - val_categorical_accuracy: 0.7630\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.58620 to 0.54486, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.5024 - categorical_accuracy: 0.7608 - val_loss: 0.6262 - val_categorical_accuracy: 0.7630\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54486\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.5012 - categorical_accuracy: 0.7624 - val_loss: 0.5201 - val_categorical_accuracy: 0.7666\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.54486 to 0.52009, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.5013 - categorical_accuracy: 0.7623 - val_loss: 0.5032 - val_categorical_accuracy: 0.7545\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.52009 to 0.50323, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4991 - categorical_accuracy: 0.7630 - val_loss: 0.5293 - val_categorical_accuracy: 0.7675\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.50323\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 2s 18us/step - loss: 0.4979 - categorical_accuracy: 0.7641 - val_loss: 0.4885 - val_categorical_accuracy: 0.7630\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50323 to 0.48852, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4976 - categorical_accuracy: 0.7646 - val_loss: 0.5026 - val_categorical_accuracy: 0.7692\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48852\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.5008 - categorical_accuracy: 0.7622 - val_loss: 0.5160 - val_categorical_accuracy: 0.7663\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48852\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4955 - categorical_accuracy: 0.7650 - val_loss: 0.5149 - val_categorical_accuracy: 0.7629\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.48852\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4955 - categorical_accuracy: 0.7655 - val_loss: 0.5061 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.48852\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4952 - categorical_accuracy: 0.7657 - val_loss: 0.4957 - val_categorical_accuracy: 0.7740\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.48852\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4940 - categorical_accuracy: 0.7661 - val_loss: 0.5424 - val_categorical_accuracy: 0.7629\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.48852\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4952 - categorical_accuracy: 0.7664 - val_loss: 0.4918 - val_categorical_accuracy: 0.7689\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.48852\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4920 - categorical_accuracy: 0.7688 - val_loss: 0.4877 - val_categorical_accuracy: 0.7687\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.48852 to 0.48773, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4948 - categorical_accuracy: 0.7654 - val_loss: 0.4898 - val_categorical_accuracy: 0.7728\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.48773\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4917 - categorical_accuracy: 0.7678 - val_loss: 0.4838 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.48773 to 0.48380, saving model to best_model.h5\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 2s 18us/step - loss: 0.4915 - categorical_accuracy: 0.7674 - val_loss: 0.4832 - val_categorical_accuracy: 0.7717\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.48380 to 0.48317, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4918 - categorical_accuracy: 0.7685 - val_loss: 0.4811 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.48317 to 0.48108, saving model to best_model.h5\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4924 - categorical_accuracy: 0.7679 - val_loss: 0.4850 - val_categorical_accuracy: 0.7736\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.48108\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 2s 18us/step - loss: 0.4929 - categorical_accuracy: 0.7677 - val_loss: 0.4868 - val_categorical_accuracy: 0.7711\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.48108\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4927 - categorical_accuracy: 0.7679 - val_loss: 0.4911 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48108\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 2s 18us/step - loss: 0.4939 - categorical_accuracy: 0.7669 - val_loss: 0.4845 - val_categorical_accuracy: 0.7729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00025: val_loss did not improve from 0.48108\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4915 - categorical_accuracy: 0.7687 - val_loss: 0.4867 - val_categorical_accuracy: 0.7684\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.48108\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4914 - categorical_accuracy: 0.7686 - val_loss: 0.4803 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.48108 to 0.48034, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4914 - categorical_accuracy: 0.7699 - val_loss: 0.4969 - val_categorical_accuracy: 0.7556\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.48034\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4893 - categorical_accuracy: 0.7695 - val_loss: 0.4958 - val_categorical_accuracy: 0.7630\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.48034\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 2s 16us/step - loss: 0.4875 - categorical_accuracy: 0.7706 - val_loss: 0.4835 - val_categorical_accuracy: 0.7728\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.48034\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4902 - categorical_accuracy: 0.7688 - val_loss: 0.4786 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.48034 to 0.47855, saving model to best_model.h5\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4895 - categorical_accuracy: 0.7694 - val_loss: 0.4789 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.47855\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 2s 16us/step - loss: 0.4885 - categorical_accuracy: 0.7704 - val_loss: 0.4806 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47855\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4903 - categorical_accuracy: 0.7701 - val_loss: 0.4839 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.47855\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4887 - categorical_accuracy: 0.7701 - val_loss: 0.4786 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47855\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4879 - categorical_accuracy: 0.7719 - val_loss: 0.4991 - val_categorical_accuracy: 0.7630\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47855\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4900 - categorical_accuracy: 0.7714 - val_loss: 0.4791 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47855\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4877 - categorical_accuracy: 0.7725 - val_loss: 0.4807 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47855\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4890 - categorical_accuracy: 0.7716 - val_loss: 0.4826 - val_categorical_accuracy: 0.7636\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.47855\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4882 - categorical_accuracy: 0.7711 - val_loss: 0.4798 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.47855\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4872 - categorical_accuracy: 0.7712 - val_loss: 0.4875 - val_categorical_accuracy: 0.7643\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47855\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4843 - categorical_accuracy: 0.7726 - val_loss: 0.4794 - val_categorical_accuracy: 0.7737\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47855\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4829 - categorical_accuracy: 0.7735 - val_loss: 0.4796 - val_categorical_accuracy: 0.7737\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47855\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4828 - categorical_accuracy: 0.7745 - val_loss: 0.4791 - val_categorical_accuracy: 0.7733\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47855\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4817 - categorical_accuracy: 0.7748 - val_loss: 0.4800 - val_categorical_accuracy: 0.7721\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47855\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4827 - categorical_accuracy: 0.7746 - val_loss: 0.4789 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47855\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4823 - categorical_accuracy: 0.7754 - val_loss: 0.4773 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.47855 to 0.47730, saving model to best_model.h5\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4810 - categorical_accuracy: 0.7761 - val_loss: 0.4770 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.47730 to 0.47704, saving model to best_model.h5\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4802 - categorical_accuracy: 0.7758 - val_loss: 0.4790 - val_categorical_accuracy: 0.7731\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47704\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4800 - categorical_accuracy: 0.7759 - val_loss: 0.4782 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47704\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4809 - categorical_accuracy: 0.7758 - val_loss: 0.4782 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47704\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4810 - categorical_accuracy: 0.7759 - val_loss: 0.4783 - val_categorical_accuracy: 0.7737\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47704\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4802 - categorical_accuracy: 0.7769 - val_loss: 0.4772 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47704\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4799 - categorical_accuracy: 0.7773 - val_loss: 0.4763 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.47704 to 0.47625, saving model to best_model.h5\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4810 - categorical_accuracy: 0.7771 - val_loss: 0.4789 - val_categorical_accuracy: 0.7731\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47625\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4789 - categorical_accuracy: 0.7773 - val_loss: 0.4771 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47625\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4800 - categorical_accuracy: 0.7762 - val_loss: 0.4780 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47625\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4794 - categorical_accuracy: 0.7768 - val_loss: 0.4780 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47625\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4794 - categorical_accuracy: 0.7769 - val_loss: 0.4767 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47625\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4801 - categorical_accuracy: 0.7765 - val_loss: 0.4771 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47625\n",
      "Epoch 61/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4789 - categorical_accuracy: 0.7779 - val_loss: 0.4792 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47625\n",
      "Epoch 62/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4793 - categorical_accuracy: 0.7774 - val_loss: 0.4774 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47625\n",
      "Epoch 63/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4788 - categorical_accuracy: 0.7783 - val_loss: 0.4754 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.47625 to 0.47544, saving model to best_model.h5\n",
      "Epoch 64/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4788 - categorical_accuracy: 0.7773 - val_loss: 0.4772 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47544\n",
      "Epoch 65/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4785 - categorical_accuracy: 0.7772 - val_loss: 0.4784 - val_categorical_accuracy: 0.7736\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47544\n",
      "Epoch 66/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4785 - categorical_accuracy: 0.7773 - val_loss: 0.4762 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47544\n",
      "Epoch 67/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4787 - categorical_accuracy: 0.7776 - val_loss: 0.4768 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.47544\n",
      "Epoch 68/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4791 - categorical_accuracy: 0.7770 - val_loss: 0.4757 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.47544\n",
      "Epoch 69/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4790 - categorical_accuracy: 0.7780 - val_loss: 0.4816 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47544\n",
      "Epoch 70/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4788 - categorical_accuracy: 0.7777 - val_loss: 0.4765 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47544\n",
      "Epoch 71/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4790 - categorical_accuracy: 0.7771 - val_loss: 0.4768 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47544\n",
      "Epoch 72/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4785 - categorical_accuracy: 0.7773 - val_loss: 0.4769 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.47544\n",
      "Epoch 73/250\n",
      "91457/91457 [==============================] - 2s 16us/step - loss: 0.4785 - categorical_accuracy: 0.7778 - val_loss: 0.4773 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47544\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 74/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4784 - categorical_accuracy: 0.7773 - val_loss: 0.4760 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.47544\n",
      "Epoch 75/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4783 - categorical_accuracy: 0.7777 - val_loss: 0.4760 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.47544\n",
      "Epoch 76/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4783 - categorical_accuracy: 0.7775 - val_loss: 0.4758 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.47544\n",
      "Epoch 77/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4780 - categorical_accuracy: 0.7776 - val_loss: 0.4756 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.47544\n",
      "Epoch 78/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4772 - categorical_accuracy: 0.7779 - val_loss: 0.4760 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.47544\n",
      "Epoch 79/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4778 - categorical_accuracy: 0.7778 - val_loss: 0.4758 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.47544\n",
      "Epoch 80/250\n",
      "91457/91457 [==============================] - 2s 16us/step - loss: 0.4779 - categorical_accuracy: 0.7780 - val_loss: 0.4758 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.47544\n",
      "Epoch 81/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4775 - categorical_accuracy: 0.7783 - val_loss: 0.4759 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.47544\n",
      "Epoch 82/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4776 - categorical_accuracy: 0.7785 - val_loss: 0.4761 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.47544\n",
      "Epoch 83/250\n",
      "91457/91457 [==============================] - 2s 16us/step - loss: 0.4775 - categorical_accuracy: 0.7786 - val_loss: 0.4764 - val_categorical_accuracy: 0.7764\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.47544\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 00083: early stopping\n",
      "the 5 fold Log-Loss (NN) is 0.475440\n",
      "PARTIAL: mean Log-Loss (NN) is 0.475642\n",
      "-----------\n",
      "Loop 2/2 Fold 1/5\n",
      "-----------\n",
      "Train on 91456 samples, validate on 22865 samples\n",
      "Epoch 1/250\n",
      "91456/91456 [==============================] - 3s 33us/step - loss: 0.5429 - categorical_accuracy: 0.7556 - val_loss: 0.8489 - val_categorical_accuracy: 0.7614\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.84894, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.5198 - categorical_accuracy: 0.7610 - val_loss: 0.5561 - val_categorical_accuracy: 0.7614\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.84894 to 0.55607, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.5119 - categorical_accuracy: 0.7608 - val_loss: 0.5194 - val_categorical_accuracy: 0.7614\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55607 to 0.51942, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.5054 - categorical_accuracy: 0.7602 - val_loss: 0.5057 - val_categorical_accuracy: 0.7614\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.51942 to 0.50569, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.5024 - categorical_accuracy: 0.7601 - val_loss: 0.5464 - val_categorical_accuracy: 0.7614\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.50569\n",
      "Epoch 6/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.5022 - categorical_accuracy: 0.7602 - val_loss: 0.5665 - val_categorical_accuracy: 0.7614\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.50569\n",
      "Epoch 7/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4990 - categorical_accuracy: 0.7623 - val_loss: 0.5720 - val_categorical_accuracy: 0.7614\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.50569\n",
      "Epoch 8/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4980 - categorical_accuracy: 0.7621 - val_loss: 0.6039 - val_categorical_accuracy: 0.7619\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.50569\n",
      "Epoch 9/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4971 - categorical_accuracy: 0.7633 - val_loss: 0.5285 - val_categorical_accuracy: 0.7716\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.50569\n",
      "Epoch 10/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4972 - categorical_accuracy: 0.7648 - val_loss: 0.6248 - val_categorical_accuracy: 0.7614\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.50569\n",
      "Epoch 11/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4958 - categorical_accuracy: 0.7638 - val_loss: 0.5079 - val_categorical_accuracy: 0.7738\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.50569\n",
      "Epoch 12/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4956 - categorical_accuracy: 0.7646 - val_loss: 0.5179 - val_categorical_accuracy: 0.7637\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.50569\n",
      "Epoch 13/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4963 - categorical_accuracy: 0.7652 - val_loss: 0.4914 - val_categorical_accuracy: 0.7690\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.50569 to 0.49144, saving model to best_model.h5\n",
      "Epoch 14/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4943 - categorical_accuracy: 0.7647 - val_loss: 0.5537 - val_categorical_accuracy: 0.7614\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.49144\n",
      "Epoch 15/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4947 - categorical_accuracy: 0.7654 - val_loss: 0.4924 - val_categorical_accuracy: 0.7726\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.49144\n",
      "Epoch 16/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4930 - categorical_accuracy: 0.7655 - val_loss: 0.4929 - val_categorical_accuracy: 0.7658\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.49144\n",
      "Epoch 17/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4920 - categorical_accuracy: 0.7667 - val_loss: 0.5024 - val_categorical_accuracy: 0.7642\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.49144\n",
      "Epoch 18/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4936 - categorical_accuracy: 0.7670 - val_loss: 0.4962 - val_categorical_accuracy: 0.7658\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.49144\n",
      "Epoch 19/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4921 - categorical_accuracy: 0.7674 - val_loss: 0.4995 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.49144\n",
      "Epoch 20/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4928 - categorical_accuracy: 0.7668 - val_loss: 0.4882 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.49144 to 0.48823, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4938 - categorical_accuracy: 0.7665 - val_loss: 0.4916 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.48823\n",
      "Epoch 22/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4917 - categorical_accuracy: 0.7684 - val_loss: 0.4908 - val_categorical_accuracy: 0.7710\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.48823\n",
      "Epoch 23/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4901 - categorical_accuracy: 0.7674 - val_loss: 0.4983 - val_categorical_accuracy: 0.7693\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.48823\n",
      "Epoch 24/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4902 - categorical_accuracy: 0.7682 - val_loss: 0.4861 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.48823 to 0.48605, saving model to best_model.h5\n",
      "Epoch 25/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4902 - categorical_accuracy: 0.7687 - val_loss: 0.4914 - val_categorical_accuracy: 0.7639\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.48605\n",
      "Epoch 26/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4890 - categorical_accuracy: 0.7703 - val_loss: 0.4864 - val_categorical_accuracy: 0.7710\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.48605\n",
      "Epoch 27/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4889 - categorical_accuracy: 0.7702 - val_loss: 0.4856 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.48605 to 0.48564, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4890 - categorical_accuracy: 0.7704 - val_loss: 0.4926 - val_categorical_accuracy: 0.7615\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.48564\n",
      "Epoch 29/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4887 - categorical_accuracy: 0.7696 - val_loss: 0.5144 - val_categorical_accuracy: 0.7671\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.48564\n",
      "Epoch 30/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4893 - categorical_accuracy: 0.7708 - val_loss: 0.4884 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.48564\n",
      "Epoch 31/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4903 - categorical_accuracy: 0.7693 - val_loss: 0.4967 - val_categorical_accuracy: 0.7661\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.48564\n",
      "Epoch 32/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4896 - categorical_accuracy: 0.7680 - val_loss: 0.4925 - val_categorical_accuracy: 0.7694\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.48564\n",
      "Epoch 33/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4881 - categorical_accuracy: 0.7699 - val_loss: 0.4950 - val_categorical_accuracy: 0.7614\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.48564\n",
      "Epoch 34/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4860 - categorical_accuracy: 0.7723 - val_loss: 0.4839 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.48564 to 0.48386, saving model to best_model.h5\n",
      "Epoch 35/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4870 - categorical_accuracy: 0.7711 - val_loss: 0.4922 - val_categorical_accuracy: 0.7682\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.48386\n",
      "Epoch 36/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4883 - categorical_accuracy: 0.7705 - val_loss: 0.4922 - val_categorical_accuracy: 0.7656\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.48386\n",
      "Epoch 37/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4879 - categorical_accuracy: 0.7698 - val_loss: 0.4887 - val_categorical_accuracy: 0.7712\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.48386\n",
      "Epoch 38/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4878 - categorical_accuracy: 0.7722 - val_loss: 0.5165 - val_categorical_accuracy: 0.7291\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.48386\n",
      "Epoch 39/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4898 - categorical_accuracy: 0.7685 - val_loss: 0.4928 - val_categorical_accuracy: 0.7714\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.48386\n",
      "Epoch 40/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4856 - categorical_accuracy: 0.7708 - val_loss: 0.4831 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.48386 to 0.48308, saving model to best_model.h5\n",
      "Epoch 41/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4854 - categorical_accuracy: 0.7725 - val_loss: 0.4981 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.48308\n",
      "Epoch 42/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4869 - categorical_accuracy: 0.7706 - val_loss: 0.4948 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.48308\n",
      "Epoch 43/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4872 - categorical_accuracy: 0.7716 - val_loss: 0.4889 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.48308\n",
      "Epoch 44/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4857 - categorical_accuracy: 0.7724 - val_loss: 0.4898 - val_categorical_accuracy: 0.7726\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.48308\n",
      "Epoch 45/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4856 - categorical_accuracy: 0.7721 - val_loss: 0.4843 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.48308\n",
      "Epoch 46/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4854 - categorical_accuracy: 0.7711 - val_loss: 0.4908 - val_categorical_accuracy: 0.7616\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.48308\n",
      "Epoch 47/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4864 - categorical_accuracy: 0.7708 - val_loss: 0.4994 - val_categorical_accuracy: 0.7696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00047: val_loss did not improve from 0.48308\n",
      "Epoch 48/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4863 - categorical_accuracy: 0.7713 - val_loss: 0.4878 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.48308\n",
      "Epoch 49/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4854 - categorical_accuracy: 0.7720 - val_loss: 0.4918 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.48308\n",
      "Epoch 50/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4866 - categorical_accuracy: 0.7712 - val_loss: 0.4918 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.48308\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 51/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4809 - categorical_accuracy: 0.7748 - val_loss: 0.4821 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.48308 to 0.48211, saving model to best_model.h5\n",
      "Epoch 52/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4794 - categorical_accuracy: 0.7755 - val_loss: 0.4811 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.48211 to 0.48106, saving model to best_model.h5\n",
      "Epoch 53/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4801 - categorical_accuracy: 0.7751 - val_loss: 0.4812 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.48106\n",
      "Epoch 54/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4788 - categorical_accuracy: 0.7760 - val_loss: 0.4832 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.48106\n",
      "Epoch 55/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4789 - categorical_accuracy: 0.7755 - val_loss: 0.4820 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.48106\n",
      "Epoch 56/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4787 - categorical_accuracy: 0.7761 - val_loss: 0.4828 - val_categorical_accuracy: 0.7729\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.48106\n",
      "Epoch 57/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4786 - categorical_accuracy: 0.7764 - val_loss: 0.4810 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.48106 to 0.48103, saving model to best_model.h5\n",
      "Epoch 58/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4780 - categorical_accuracy: 0.7757 - val_loss: 0.4798 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.48103 to 0.47983, saving model to best_model.h5\n",
      "Epoch 59/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4783 - categorical_accuracy: 0.7763 - val_loss: 0.4811 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47983\n",
      "Epoch 60/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4785 - categorical_accuracy: 0.7766 - val_loss: 0.4802 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47983\n",
      "Epoch 61/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4775 - categorical_accuracy: 0.7773 - val_loss: 0.4806 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47983\n",
      "Epoch 62/250\n",
      "91456/91456 [==============================] - 1s 14us/step - loss: 0.4780 - categorical_accuracy: 0.7757 - val_loss: 0.4802 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47983\n",
      "Epoch 63/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4780 - categorical_accuracy: 0.7764 - val_loss: 0.4824 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47983\n",
      "Epoch 64/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4772 - categorical_accuracy: 0.7763 - val_loss: 0.4813 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47983\n",
      "Epoch 65/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4781 - categorical_accuracy: 0.7768 - val_loss: 0.4827 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47983\n",
      "Epoch 66/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4777 - categorical_accuracy: 0.7765 - val_loss: 0.4813 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47983\n",
      "Epoch 67/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4772 - categorical_accuracy: 0.7772 - val_loss: 0.4796 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.47983 to 0.47960, saving model to best_model.h5\n",
      "Epoch 68/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4774 - categorical_accuracy: 0.7767 - val_loss: 0.4819 - val_categorical_accuracy: 0.7733\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.47960\n",
      "Epoch 69/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4780 - categorical_accuracy: 0.7771 - val_loss: 0.4802 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47960\n",
      "Epoch 70/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4771 - categorical_accuracy: 0.7772 - val_loss: 0.4798 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47960\n",
      "Epoch 71/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4765 - categorical_accuracy: 0.7769 - val_loss: 0.4803 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47960\n",
      "Epoch 72/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4758 - categorical_accuracy: 0.7777 - val_loss: 0.4824 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.47960\n",
      "Epoch 73/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4772 - categorical_accuracy: 0.7769 - val_loss: 0.4817 - val_categorical_accuracy: 0.7740\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47960\n",
      "Epoch 74/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4770 - categorical_accuracy: 0.7768 - val_loss: 0.4789 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.47960 to 0.47892, saving model to best_model.h5\n",
      "Epoch 75/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4772 - categorical_accuracy: 0.7770 - val_loss: 0.4807 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.47892\n",
      "Epoch 76/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4770 - categorical_accuracy: 0.7774 - val_loss: 0.4814 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.47892\n",
      "Epoch 77/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4762 - categorical_accuracy: 0.7784 - val_loss: 0.4794 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.47892\n",
      "Epoch 78/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4762 - categorical_accuracy: 0.7770 - val_loss: 0.4788 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.47892 to 0.47883, saving model to best_model.h5\n",
      "Epoch 79/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4765 - categorical_accuracy: 0.7771 - val_loss: 0.4802 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.47883\n",
      "Epoch 80/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4765 - categorical_accuracy: 0.7776 - val_loss: 0.4811 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.47883\n",
      "Epoch 81/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4768 - categorical_accuracy: 0.7764 - val_loss: 0.4799 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.47883\n",
      "Epoch 82/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4763 - categorical_accuracy: 0.7782 - val_loss: 0.4792 - val_categorical_accuracy: 0.7774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00082: val_loss did not improve from 0.47883\n",
      "Epoch 83/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4766 - categorical_accuracy: 0.7769 - val_loss: 0.4820 - val_categorical_accuracy: 0.7728\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.47883\n",
      "Epoch 84/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4763 - categorical_accuracy: 0.7778 - val_loss: 0.4834 - val_categorical_accuracy: 0.7712\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.47883\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 85/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4752 - categorical_accuracy: 0.7784 - val_loss: 0.4804 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.47883\n",
      "Epoch 86/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4759 - categorical_accuracy: 0.7772 - val_loss: 0.4798 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.47883\n",
      "Epoch 87/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4754 - categorical_accuracy: 0.7771 - val_loss: 0.4795 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.47883\n",
      "Epoch 88/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4759 - categorical_accuracy: 0.7773 - val_loss: 0.4799 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.47883\n",
      "Epoch 89/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4753 - categorical_accuracy: 0.7774 - val_loss: 0.4797 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.47883\n",
      "Epoch 90/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4755 - categorical_accuracy: 0.7773 - val_loss: 0.4799 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.47883\n",
      "Epoch 91/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4751 - categorical_accuracy: 0.7779 - val_loss: 0.4800 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.47883\n",
      "Epoch 92/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4758 - categorical_accuracy: 0.7775 - val_loss: 0.4801 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.47883\n",
      "Epoch 93/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4755 - categorical_accuracy: 0.7768 - val_loss: 0.4797 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.47883\n",
      "Epoch 94/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4761 - categorical_accuracy: 0.7769 - val_loss: 0.4800 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.47883\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 95/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4757 - categorical_accuracy: 0.7781 - val_loss: 0.4800 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.47883\n",
      "Epoch 96/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4765 - categorical_accuracy: 0.7764 - val_loss: 0.4799 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.47883\n",
      "Epoch 97/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4764 - categorical_accuracy: 0.7770 - val_loss: 0.4798 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.47883\n",
      "Epoch 98/250\n",
      "91456/91456 [==============================] - 1s 15us/step - loss: 0.4757 - categorical_accuracy: 0.7779 - val_loss: 0.4797 - val_categorical_accuracy: 0.7765\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.47883\n",
      "Epoch 00098: early stopping\n",
      "the 1 fold Log-Loss (NN) is 0.478828\n",
      "-----------\n",
      "Loop 2/2 Fold 2/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 3s 32us/step - loss: 0.5383 - categorical_accuracy: 0.7553 - val_loss: 0.6750 - val_categorical_accuracy: 0.7647\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67503, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.5182 - categorical_accuracy: 0.7590 - val_loss: 0.5117 - val_categorical_accuracy: 0.7654\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67503 to 0.51168, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.5103 - categorical_accuracy: 0.7592 - val_loss: 0.4986 - val_categorical_accuracy: 0.7654\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.51168 to 0.49858, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.5056 - categorical_accuracy: 0.7602 - val_loss: 0.5898 - val_categorical_accuracy: 0.7654\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49858\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.5055 - categorical_accuracy: 0.7601 - val_loss: 0.5430 - val_categorical_accuracy: 0.7709\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49858\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.5036 - categorical_accuracy: 0.7610 - val_loss: 0.5317 - val_categorical_accuracy: 0.7652\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49858\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.5023 - categorical_accuracy: 0.7605 - val_loss: 0.5097 - val_categorical_accuracy: 0.7654\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.49858\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.5005 - categorical_accuracy: 0.7623 - val_loss: 0.5103 - val_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.49858\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4989 - categorical_accuracy: 0.7628 - val_loss: 0.4840 - val_categorical_accuracy: 0.7699\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.49858 to 0.48404, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4981 - categorical_accuracy: 0.7657 - val_loss: 0.5031 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48404\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4967 - categorical_accuracy: 0.7654 - val_loss: 0.4877 - val_categorical_accuracy: 0.7711\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48404\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4967 - categorical_accuracy: 0.7642 - val_loss: 0.4797 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.48404 to 0.47969, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4976 - categorical_accuracy: 0.7643 - val_loss: 0.4767 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.47969 to 0.47672, saving model to best_model.h5\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4957 - categorical_accuracy: 0.7667 - val_loss: 0.4877 - val_categorical_accuracy: 0.7706\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.47672\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4938 - categorical_accuracy: 0.7672 - val_loss: 0.4917 - val_categorical_accuracy: 0.7808\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.47672\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4947 - categorical_accuracy: 0.7667 - val_loss: 0.4769 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.47672\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4942 - categorical_accuracy: 0.7664 - val_loss: 0.4884 - val_categorical_accuracy: 0.7653\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.47672\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4949 - categorical_accuracy: 0.7666 - val_loss: 0.4769 - val_categorical_accuracy: 0.7787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: val_loss did not improve from 0.47672\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4941 - categorical_accuracy: 0.7675 - val_loss: 0.4808 - val_categorical_accuracy: 0.7697\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.47672\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4931 - categorical_accuracy: 0.7674 - val_loss: 0.4785 - val_categorical_accuracy: 0.7808\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.47672\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4925 - categorical_accuracy: 0.7670 - val_loss: 0.4975 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.47672\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4926 - categorical_accuracy: 0.7685 - val_loss: 0.4872 - val_categorical_accuracy: 0.7654\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.47672\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4914 - categorical_accuracy: 0.7688 - val_loss: 0.4778 - val_categorical_accuracy: 0.7832\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.47672\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4877 - categorical_accuracy: 0.7710 - val_loss: 0.4756 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.47672 to 0.47559, saving model to best_model.h5\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4858 - categorical_accuracy: 0.7719 - val_loss: 0.4790 - val_categorical_accuracy: 0.7729\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.47559\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4840 - categorical_accuracy: 0.7739 - val_loss: 0.4756 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.47559 to 0.47558, saving model to best_model.h5\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4854 - categorical_accuracy: 0.7738 - val_loss: 0.4757 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.47558\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4841 - categorical_accuracy: 0.7732 - val_loss: 0.4783 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.47558\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4839 - categorical_accuracy: 0.7739 - val_loss: 0.4762 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.47558\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4830 - categorical_accuracy: 0.7740 - val_loss: 0.4763 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.47558\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4827 - categorical_accuracy: 0.7748 - val_loss: 0.4742 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.47558 to 0.47418, saving model to best_model.h5\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4825 - categorical_accuracy: 0.7745 - val_loss: 0.4729 - val_categorical_accuracy: 0.7815\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.47418 to 0.47291, saving model to best_model.h5\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4828 - categorical_accuracy: 0.7751 - val_loss: 0.4748 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47291\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4827 - categorical_accuracy: 0.7752 - val_loss: 0.4754 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.47291\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4818 - categorical_accuracy: 0.7751 - val_loss: 0.4759 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47291\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4822 - categorical_accuracy: 0.7753 - val_loss: 0.4737 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47291\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4820 - categorical_accuracy: 0.7757 - val_loss: 0.4741 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47291\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4819 - categorical_accuracy: 0.7752 - val_loss: 0.4740 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47291\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4812 - categorical_accuracy: 0.7762 - val_loss: 0.4735 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.47291\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4821 - categorical_accuracy: 0.7757 - val_loss: 0.4727 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.47291 to 0.47269, saving model to best_model.h5\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4810 - categorical_accuracy: 0.7757 - val_loss: 0.4748 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47269\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4815 - categorical_accuracy: 0.7760 - val_loss: 0.4758 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47269\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4817 - categorical_accuracy: 0.7755 - val_loss: 0.4745 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47269\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4815 - categorical_accuracy: 0.7764 - val_loss: 0.4732 - val_categorical_accuracy: 0.7810\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47269\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4813 - categorical_accuracy: 0.7759 - val_loss: 0.4743 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47269\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4807 - categorical_accuracy: 0.7772 - val_loss: 0.4739 - val_categorical_accuracy: 0.7822\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47269\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4812 - categorical_accuracy: 0.7747 - val_loss: 0.4746 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47269\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4806 - categorical_accuracy: 0.7755 - val_loss: 0.4724 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.47269 to 0.47243, saving model to best_model.h5\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4807 - categorical_accuracy: 0.7760 - val_loss: 0.4739 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47243\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4814 - categorical_accuracy: 0.7754 - val_loss: 0.4718 - val_categorical_accuracy: 0.7811\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.47243 to 0.47181, saving model to best_model.h5\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4800 - categorical_accuracy: 0.7762 - val_loss: 0.4720 - val_categorical_accuracy: 0.7806\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47181\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4804 - categorical_accuracy: 0.7765 - val_loss: 0.4713 - val_categorical_accuracy: 0.7818\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.47181 to 0.47128, saving model to best_model.h5\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4810 - categorical_accuracy: 0.7760 - val_loss: 0.4725 - val_categorical_accuracy: 0.7803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47128\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4799 - categorical_accuracy: 0.7767 - val_loss: 0.4744 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47128\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4802 - categorical_accuracy: 0.7766 - val_loss: 0.4719 - val_categorical_accuracy: 0.7805\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47128\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4803 - categorical_accuracy: 0.7760 - val_loss: 0.4734 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47128\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4802 - categorical_accuracy: 0.7764 - val_loss: 0.4738 - val_categorical_accuracy: 0.7811\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47128\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4805 - categorical_accuracy: 0.7765 - val_loss: 0.4735 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47128\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4800 - categorical_accuracy: 0.7765 - val_loss: 0.4728 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47128\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4797 - categorical_accuracy: 0.7766 - val_loss: 0.4729 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47128\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4795 - categorical_accuracy: 0.7771 - val_loss: 0.4730 - val_categorical_accuracy: 0.7811\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47128\n",
      "Epoch 62/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4797 - categorical_accuracy: 0.7766 - val_loss: 0.4751 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47128\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 63/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4796 - categorical_accuracy: 0.7768 - val_loss: 0.4719 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47128\n",
      "Epoch 64/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4794 - categorical_accuracy: 0.7767 - val_loss: 0.4719 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47128\n",
      "Epoch 65/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4787 - categorical_accuracy: 0.7766 - val_loss: 0.4720 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47128\n",
      "Epoch 66/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4786 - categorical_accuracy: 0.7774 - val_loss: 0.4710 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.47128 to 0.47103, saving model to best_model.h5\n",
      "Epoch 67/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4790 - categorical_accuracy: 0.7773 - val_loss: 0.4711 - val_categorical_accuracy: 0.7806\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.47103\n",
      "Epoch 68/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4792 - categorical_accuracy: 0.7768 - val_loss: 0.4706 - val_categorical_accuracy: 0.7807\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.47103 to 0.47062, saving model to best_model.h5\n",
      "Epoch 69/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4790 - categorical_accuracy: 0.7779 - val_loss: 0.4712 - val_categorical_accuracy: 0.7807\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47062\n",
      "Epoch 70/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4791 - categorical_accuracy: 0.7775 - val_loss: 0.4711 - val_categorical_accuracy: 0.7808\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47062\n",
      "Epoch 71/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4784 - categorical_accuracy: 0.7775 - val_loss: 0.4710 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47062\n",
      "Epoch 72/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4794 - categorical_accuracy: 0.7770 - val_loss: 0.4712 - val_categorical_accuracy: 0.7807\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.47062\n",
      "Epoch 73/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4788 - categorical_accuracy: 0.7777 - val_loss: 0.4710 - val_categorical_accuracy: 0.7807\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47062\n",
      "Epoch 74/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4782 - categorical_accuracy: 0.7780 - val_loss: 0.4707 - val_categorical_accuracy: 0.7813\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.47062\n",
      "Epoch 75/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4781 - categorical_accuracy: 0.7771 - val_loss: 0.4712 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.47062\n",
      "Epoch 76/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4783 - categorical_accuracy: 0.7775 - val_loss: 0.4710 - val_categorical_accuracy: 0.7808\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.47062\n",
      "Epoch 77/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4783 - categorical_accuracy: 0.7769 - val_loss: 0.4713 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.47062\n",
      "Epoch 78/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4788 - categorical_accuracy: 0.7771 - val_loss: 0.4710 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.47062\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 79/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4781 - categorical_accuracy: 0.7779 - val_loss: 0.4711 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.47062\n",
      "Epoch 80/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4784 - categorical_accuracy: 0.7775 - val_loss: 0.4712 - val_categorical_accuracy: 0.7805\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.47062\n",
      "Epoch 81/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4790 - categorical_accuracy: 0.7773 - val_loss: 0.4711 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.47062\n",
      "Epoch 82/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4786 - categorical_accuracy: 0.7773 - val_loss: 0.4712 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.47062\n",
      "Epoch 83/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4785 - categorical_accuracy: 0.7772 - val_loss: 0.4712 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.47062\n",
      "Epoch 84/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4786 - categorical_accuracy: 0.7770 - val_loss: 0.4711 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.47062\n",
      "Epoch 85/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4788 - categorical_accuracy: 0.7775 - val_loss: 0.4710 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.47062\n",
      "Epoch 86/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4786 - categorical_accuracy: 0.7767 - val_loss: 0.4711 - val_categorical_accuracy: 0.7805\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.47062\n",
      "Epoch 87/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4782 - categorical_accuracy: 0.7770 - val_loss: 0.4712 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.47062\n",
      "Epoch 88/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4783 - categorical_accuracy: 0.7779 - val_loss: 0.4711 - val_categorical_accuracy: 0.7802\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.47062\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 00088: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 2 fold Log-Loss (NN) is 0.470616\n",
      "-----------\n",
      "Loop 2/2 Fold 3/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 3s 32us/step - loss: 0.5407 - categorical_accuracy: 0.7579 - val_loss: 0.6812 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.68124, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.5201 - categorical_accuracy: 0.7617 - val_loss: 0.5840 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.68124 to 0.58396, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.5111 - categorical_accuracy: 0.7613 - val_loss: 0.6101 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.58396\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.5060 - categorical_accuracy: 0.7610 - val_loss: 0.5400 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.58396 to 0.54001, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.5042 - categorical_accuracy: 0.7613 - val_loss: 0.5360 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54001 to 0.53601, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.5031 - categorical_accuracy: 0.7613 - val_loss: 0.5143 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.53601 to 0.51431, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.5009 - categorical_accuracy: 0.7630 - val_loss: 0.4960 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.51431 to 0.49597, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4997 - categorical_accuracy: 0.7634 - val_loss: 0.5158 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.49597\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4995 - categorical_accuracy: 0.7634 - val_loss: 0.5139 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.49597\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4975 - categorical_accuracy: 0.7642 - val_loss: 0.4938 - val_categorical_accuracy: 0.7701\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.49597 to 0.49381, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4969 - categorical_accuracy: 0.7665 - val_loss: 0.4907 - val_categorical_accuracy: 0.7732\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.49381 to 0.49071, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4974 - categorical_accuracy: 0.7642 - val_loss: 0.4872 - val_categorical_accuracy: 0.7653\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.49071 to 0.48722, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4950 - categorical_accuracy: 0.7659 - val_loss: 0.5043 - val_categorical_accuracy: 0.7630\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.48722\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4954 - categorical_accuracy: 0.7664 - val_loss: 0.5070 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.48722\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4952 - categorical_accuracy: 0.7654 - val_loss: 0.4910 - val_categorical_accuracy: 0.7657\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.48722\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4947 - categorical_accuracy: 0.7661 - val_loss: 0.5048 - val_categorical_accuracy: 0.7714\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.48722\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4933 - categorical_accuracy: 0.7676 - val_loss: 0.4848 - val_categorical_accuracy: 0.7686\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.48722 to 0.48481, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 2s 17us/step - loss: 0.4946 - categorical_accuracy: 0.7669 - val_loss: 0.5031 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.48481\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4927 - categorical_accuracy: 0.7671 - val_loss: 0.4838 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.48481 to 0.48378, saving model to best_model.h5\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4928 - categorical_accuracy: 0.7667 - val_loss: 0.4850 - val_categorical_accuracy: 0.7642\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.48378\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4927 - categorical_accuracy: 0.7675 - val_loss: 0.4935 - val_categorical_accuracy: 0.7431\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.48378\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4912 - categorical_accuracy: 0.7694 - val_loss: 0.4834 - val_categorical_accuracy: 0.7706\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.48378 to 0.48338, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4919 - categorical_accuracy: 0.7666 - val_loss: 0.4830 - val_categorical_accuracy: 0.7689\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.48338 to 0.48295, saving model to best_model.h5\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4910 - categorical_accuracy: 0.7686 - val_loss: 0.4834 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48295\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4900 - categorical_accuracy: 0.7687 - val_loss: 0.4813 - val_categorical_accuracy: 0.7731\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.48295 to 0.48129, saving model to best_model.h5\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4919 - categorical_accuracy: 0.7685 - val_loss: 0.4851 - val_categorical_accuracy: 0.7609\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.48129\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4899 - categorical_accuracy: 0.7695 - val_loss: 0.4986 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.48129\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4904 - categorical_accuracy: 0.7693 - val_loss: 0.4820 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.48129\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4889 - categorical_accuracy: 0.7695 - val_loss: 0.4879 - val_categorical_accuracy: 0.7628\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.48129\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4910 - categorical_accuracy: 0.7681 - val_loss: 0.4800 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.48129 to 0.47995, saving model to best_model.h5\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4894 - categorical_accuracy: 0.7693 - val_loss: 0.4964 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.47995\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4894 - categorical_accuracy: 0.7704 - val_loss: 0.4818 - val_categorical_accuracy: 0.7705\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.47995\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4886 - categorical_accuracy: 0.7702 - val_loss: 0.4871 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47995\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4891 - categorical_accuracy: 0.7690 - val_loss: 0.4781 - val_categorical_accuracy: 0.7733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: val_loss improved from 0.47995 to 0.47813, saving model to best_model.h5\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4896 - categorical_accuracy: 0.7716 - val_loss: 0.4999 - val_categorical_accuracy: 0.7594\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47813\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4888 - categorical_accuracy: 0.7699 - val_loss: 0.4822 - val_categorical_accuracy: 0.7654\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47813\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4891 - categorical_accuracy: 0.7691 - val_loss: 0.4852 - val_categorical_accuracy: 0.7703\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47813\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4882 - categorical_accuracy: 0.7693 - val_loss: 0.4803 - val_categorical_accuracy: 0.7731\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47813\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4881 - categorical_accuracy: 0.7702 - val_loss: 0.4873 - val_categorical_accuracy: 0.7573\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.47813\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4904 - categorical_accuracy: 0.7694 - val_loss: 0.4797 - val_categorical_accuracy: 0.7721\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.47813\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4883 - categorical_accuracy: 0.7688 - val_loss: 0.4782 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47813\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4882 - categorical_accuracy: 0.7706 - val_loss: 0.4822 - val_categorical_accuracy: 0.7738\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47813\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4869 - categorical_accuracy: 0.7719 - val_loss: 0.5013 - val_categorical_accuracy: 0.7593\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47813\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4899 - categorical_accuracy: 0.7690 - val_loss: 0.4826 - val_categorical_accuracy: 0.7651\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47813\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4842 - categorical_accuracy: 0.7725 - val_loss: 0.4807 - val_categorical_accuracy: 0.7701\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47813\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 1s 16us/step - loss: 0.4826 - categorical_accuracy: 0.7740 - val_loss: 0.4846 - val_categorical_accuracy: 0.7679\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47813\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4821 - categorical_accuracy: 0.7738 - val_loss: 0.4776 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.47813 to 0.47756, saving model to best_model.h5\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4814 - categorical_accuracy: 0.7735 - val_loss: 0.4807 - val_categorical_accuracy: 0.7696\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47756\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4813 - categorical_accuracy: 0.7742 - val_loss: 0.4809 - val_categorical_accuracy: 0.7703\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47756\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4818 - categorical_accuracy: 0.7736 - val_loss: 0.4786 - val_categorical_accuracy: 0.7732\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47756\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4803 - categorical_accuracy: 0.7747 - val_loss: 0.4797 - val_categorical_accuracy: 0.7705\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47756\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4804 - categorical_accuracy: 0.7752 - val_loss: 0.4793 - val_categorical_accuracy: 0.7716\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47756\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4809 - categorical_accuracy: 0.7741 - val_loss: 0.4804 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47756\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4799 - categorical_accuracy: 0.7744 - val_loss: 0.4797 - val_categorical_accuracy: 0.7702\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47756\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4800 - categorical_accuracy: 0.7739 - val_loss: 0.4782 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47756\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4798 - categorical_accuracy: 0.7728 - val_loss: 0.4814 - val_categorical_accuracy: 0.7677\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47756\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4804 - categorical_accuracy: 0.7750 - val_loss: 0.4822 - val_categorical_accuracy: 0.7679\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47756\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4800 - categorical_accuracy: 0.7742 - val_loss: 0.4784 - val_categorical_accuracy: 0.7736\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47756\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4799 - categorical_accuracy: 0.7741 - val_loss: 0.4785 - val_categorical_accuracy: 0.7729\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47756\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4793 - categorical_accuracy: 0.7745 - val_loss: 0.4785 - val_categorical_accuracy: 0.7729\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47756\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4795 - categorical_accuracy: 0.7742 - val_loss: 0.4790 - val_categorical_accuracy: 0.7711\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47756\n",
      "Epoch 62/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4800 - categorical_accuracy: 0.7736 - val_loss: 0.4785 - val_categorical_accuracy: 0.7737\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47756\n",
      "Epoch 63/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4797 - categorical_accuracy: 0.7744 - val_loss: 0.4781 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47756\n",
      "Epoch 64/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.4788 - categorical_accuracy: 0.7753 - val_loss: 0.4788 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47756\n",
      "Epoch 65/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4797 - categorical_accuracy: 0.7741 - val_loss: 0.4784 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47756\n",
      "Epoch 66/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4795 - categorical_accuracy: 0.7744 - val_loss: 0.4780 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47756\n",
      "Epoch 67/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4788 - categorical_accuracy: 0.7748 - val_loss: 0.4773 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.47756 to 0.47726, saving model to best_model.h5\n",
      "Epoch 68/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4788 - categorical_accuracy: 0.7754 - val_loss: 0.4781 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.47726\n",
      "Epoch 69/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4796 - categorical_accuracy: 0.7743 - val_loss: 0.4771 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.47726 to 0.47710, saving model to best_model.h5\n",
      "Epoch 70/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4793 - categorical_accuracy: 0.7749 - val_loss: 0.4773 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47710\n",
      "Epoch 71/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4791 - categorical_accuracy: 0.7746 - val_loss: 0.4778 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47710\n",
      "Epoch 72/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4794 - categorical_accuracy: 0.7752 - val_loss: 0.4779 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.47710\n",
      "Epoch 73/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4794 - categorical_accuracy: 0.7746 - val_loss: 0.4777 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47710\n",
      "Epoch 74/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4799 - categorical_accuracy: 0.7742 - val_loss: 0.4782 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.47710\n",
      "Epoch 75/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4794 - categorical_accuracy: 0.7740 - val_loss: 0.4777 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.47710\n",
      "Epoch 76/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4790 - categorical_accuracy: 0.7749 - val_loss: 0.4780 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.47710\n",
      "Epoch 77/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4792 - categorical_accuracy: 0.7748 - val_loss: 0.4778 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.47710\n",
      "Epoch 78/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4792 - categorical_accuracy: 0.7735 - val_loss: 0.4771 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.47710\n",
      "Epoch 79/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4791 - categorical_accuracy: 0.7744 - val_loss: 0.4775 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.47710\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 80/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4787 - categorical_accuracy: 0.7741 - val_loss: 0.4775 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.47710\n",
      "Epoch 81/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4789 - categorical_accuracy: 0.7740 - val_loss: 0.4776 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.47710\n",
      "Epoch 82/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4790 - categorical_accuracy: 0.7740 - val_loss: 0.4776 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.47710\n",
      "Epoch 83/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4797 - categorical_accuracy: 0.7742 - val_loss: 0.4776 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.47710\n",
      "Epoch 84/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4786 - categorical_accuracy: 0.7752 - val_loss: 0.4777 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.47710\n",
      "Epoch 85/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4792 - categorical_accuracy: 0.7747 - val_loss: 0.4776 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.47710\n",
      "Epoch 86/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4804 - categorical_accuracy: 0.7747 - val_loss: 0.4777 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.47710\n",
      "Epoch 87/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4794 - categorical_accuracy: 0.7745 - val_loss: 0.4775 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.47710\n",
      "Epoch 88/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4790 - categorical_accuracy: 0.7750 - val_loss: 0.4775 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.47710\n",
      "Epoch 89/250\n",
      "91457/91457 [==============================] - 1s 15us/step - loss: 0.4788 - categorical_accuracy: 0.7747 - val_loss: 0.4775 - val_categorical_accuracy: 0.7756\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.47710\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "Epoch 00089: early stopping\n",
      "the 3 fold Log-Loss (NN) is 0.477096\n",
      "-----------\n",
      "Loop 2/2 Fold 4/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 3s 27us/step - loss: 0.5387 - categorical_accuracy: 0.7559 - val_loss: 0.7863 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78627, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.5178 - categorical_accuracy: 0.7608 - val_loss: 0.5439 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78627 to 0.54394, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.5117 - categorical_accuracy: 0.7597 - val_loss: 0.6792 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.54394\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.5059 - categorical_accuracy: 0.7601 - val_loss: 0.5375 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54394 to 0.53746, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.5038 - categorical_accuracy: 0.7594 - val_loss: 0.5199 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.53746 to 0.51993, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.5023 - categorical_accuracy: 0.7610 - val_loss: 0.5265 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51993\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 1s 14us/step - loss: 0.5008 - categorical_accuracy: 0.7624 - val_loss: 0.5392 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51993\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4989 - categorical_accuracy: 0.7630 - val_loss: 0.5038 - val_categorical_accuracy: 0.7638\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51993 to 0.50380, saving model to best_model.h5\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4990 - categorical_accuracy: 0.7640 - val_loss: 0.4953 - val_categorical_accuracy: 0.7635\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50380 to 0.49525, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4964 - categorical_accuracy: 0.7656 - val_loss: 0.4860 - val_categorical_accuracy: 0.7706\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.49525 to 0.48597, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4961 - categorical_accuracy: 0.7644 - val_loss: 0.4865 - val_categorical_accuracy: 0.7692\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48597\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4971 - categorical_accuracy: 0.7654 - val_loss: 0.4979 - val_categorical_accuracy: 0.7496\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.48597\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4970 - categorical_accuracy: 0.7645 - val_loss: 0.4795 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.48597 to 0.47949, saving model to best_model.h5\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4935 - categorical_accuracy: 0.7677 - val_loss: 0.4818 - val_categorical_accuracy: 0.7729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00014: val_loss did not improve from 0.47949\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4947 - categorical_accuracy: 0.7666 - val_loss: 0.5093 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.47949\n",
      "Epoch 16/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4945 - categorical_accuracy: 0.7669 - val_loss: 0.5213 - val_categorical_accuracy: 0.7673\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.47949\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4944 - categorical_accuracy: 0.7672 - val_loss: 0.4957 - val_categorical_accuracy: 0.7718\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.47949\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4923 - categorical_accuracy: 0.7687 - val_loss: 0.4868 - val_categorical_accuracy: 0.7705\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.47949\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4936 - categorical_accuracy: 0.7687 - val_loss: 0.5056 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.47949\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4937 - categorical_accuracy: 0.7672 - val_loss: 0.4824 - val_categorical_accuracy: 0.7696\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.47949\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4925 - categorical_accuracy: 0.7680 - val_loss: 0.4800 - val_categorical_accuracy: 0.7739\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.47949\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4933 - categorical_accuracy: 0.7684 - val_loss: 0.4811 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.47949\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4916 - categorical_accuracy: 0.7685 - val_loss: 0.4775 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.47949 to 0.47753, saving model to best_model.h5\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4924 - categorical_accuracy: 0.7689 - val_loss: 0.4962 - val_categorical_accuracy: 0.7624\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.47753\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4919 - categorical_accuracy: 0.7671 - val_loss: 0.4952 - val_categorical_accuracy: 0.7680\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.47753\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4928 - categorical_accuracy: 0.7679 - val_loss: 0.4763 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.47753 to 0.47634, saving model to best_model.h5\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4901 - categorical_accuracy: 0.7698 - val_loss: 0.4824 - val_categorical_accuracy: 0.7703\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.47634\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4902 - categorical_accuracy: 0.7693 - val_loss: 0.4848 - val_categorical_accuracy: 0.7690\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.47634\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4904 - categorical_accuracy: 0.7700 - val_loss: 0.4859 - val_categorical_accuracy: 0.7686\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.47634\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4923 - categorical_accuracy: 0.7691 - val_loss: 0.4816 - val_categorical_accuracy: 0.7661\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.47634\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4909 - categorical_accuracy: 0.7700 - val_loss: 0.4852 - val_categorical_accuracy: 0.7695\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.47634\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4904 - categorical_accuracy: 0.7717 - val_loss: 0.4798 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.47634\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4890 - categorical_accuracy: 0.7708 - val_loss: 0.4801 - val_categorical_accuracy: 0.7726\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47634\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4896 - categorical_accuracy: 0.7711 - val_loss: 0.4910 - val_categorical_accuracy: 0.7636\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.47634\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4893 - categorical_accuracy: 0.7711 - val_loss: 0.4777 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47634\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4902 - categorical_accuracy: 0.7698 - val_loss: 0.4794 - val_categorical_accuracy: 0.7733\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47634\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4859 - categorical_accuracy: 0.7733 - val_loss: 0.4763 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.47634 to 0.47630, saving model to best_model.h5\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4839 - categorical_accuracy: 0.7741 - val_loss: 0.4795 - val_categorical_accuracy: 0.7739\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47630\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4827 - categorical_accuracy: 0.7748 - val_loss: 0.4755 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.47630 to 0.47554, saving model to best_model.h5\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4834 - categorical_accuracy: 0.7751 - val_loss: 0.4762 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.47554\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4824 - categorical_accuracy: 0.7767 - val_loss: 0.4782 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47554\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4820 - categorical_accuracy: 0.7762 - val_loss: 0.4770 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47554\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4817 - categorical_accuracy: 0.7768 - val_loss: 0.4752 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.47554 to 0.47523, saving model to best_model.h5\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4822 - categorical_accuracy: 0.7767 - val_loss: 0.4766 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47523\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4825 - categorical_accuracy: 0.7756 - val_loss: 0.4742 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.47523 to 0.47424, saving model to best_model.h5\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4816 - categorical_accuracy: 0.7764 - val_loss: 0.4759 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47424\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4816 - categorical_accuracy: 0.7758 - val_loss: 0.4754 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47424\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4812 - categorical_accuracy: 0.7763 - val_loss: 0.4733 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.47424 to 0.47330, saving model to best_model.h5\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4812 - categorical_accuracy: 0.7762 - val_loss: 0.4763 - val_categorical_accuracy: 0.7753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47330\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4811 - categorical_accuracy: 0.7768 - val_loss: 0.4748 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47330\n",
      "Epoch 51/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4813 - categorical_accuracy: 0.7765 - val_loss: 0.4747 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47330\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4812 - categorical_accuracy: 0.7764 - val_loss: 0.4756 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47330\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4806 - categorical_accuracy: 0.7766 - val_loss: 0.4760 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47330\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4811 - categorical_accuracy: 0.7760 - val_loss: 0.4750 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47330\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4810 - categorical_accuracy: 0.7762 - val_loss: 0.4745 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47330\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4806 - categorical_accuracy: 0.7767 - val_loss: 0.4759 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47330\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4803 - categorical_accuracy: 0.7759 - val_loss: 0.4770 - val_categorical_accuracy: 0.7740\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47330\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4812 - categorical_accuracy: 0.7763 - val_loss: 0.4747 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47330\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4800 - categorical_accuracy: 0.7769 - val_loss: 0.4746 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47330\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4797 - categorical_accuracy: 0.7767 - val_loss: 0.4744 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47330\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4793 - categorical_accuracy: 0.7768 - val_loss: 0.4742 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47330\n",
      "Epoch 62/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4795 - categorical_accuracy: 0.7767 - val_loss: 0.4742 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47330\n",
      "Epoch 63/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4793 - categorical_accuracy: 0.7778 - val_loss: 0.4739 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47330\n",
      "Epoch 64/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4801 - categorical_accuracy: 0.7769 - val_loss: 0.4744 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47330\n",
      "Epoch 65/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4795 - categorical_accuracy: 0.7770 - val_loss: 0.4739 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47330\n",
      "Epoch 66/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4792 - categorical_accuracy: 0.7773 - val_loss: 0.4745 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47330\n",
      "Epoch 67/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4800 - categorical_accuracy: 0.7772 - val_loss: 0.4742 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.47330\n",
      "Epoch 68/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4798 - categorical_accuracy: 0.7765 - val_loss: 0.4746 - val_categorical_accuracy: 0.7765\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.47330\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 00068: early stopping\n",
      "the 4 fold Log-Loss (NN) is 0.473300\n",
      "-----------\n",
      "Loop 2/2 Fold 5/5\n",
      "-----------\n",
      "Train on 91457 samples, validate on 22864 samples\n",
      "Epoch 1/250\n",
      "91457/91457 [==============================] - 2s 23us/step - loss: 0.5369 - categorical_accuracy: 0.7578 - val_loss: 0.7365 - val_categorical_accuracy: 0.7574\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73647, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.5159 - categorical_accuracy: 0.7614 - val_loss: 0.5191 - val_categorical_accuracy: 0.7574\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73647 to 0.51908, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.5073 - categorical_accuracy: 0.7613 - val_loss: 0.5621 - val_categorical_accuracy: 0.7574\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.51908\n",
      "Epoch 4/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.5062 - categorical_accuracy: 0.7614 - val_loss: 0.6143 - val_categorical_accuracy: 0.7574\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51908\n",
      "Epoch 5/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.5017 - categorical_accuracy: 0.7626 - val_loss: 0.5294 - val_categorical_accuracy: 0.7574\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51908\n",
      "Epoch 6/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.5008 - categorical_accuracy: 0.7620 - val_loss: 0.5414 - val_categorical_accuracy: 0.7574\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51908\n",
      "Epoch 7/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.5002 - categorical_accuracy: 0.7632 - val_loss: 0.5750 - val_categorical_accuracy: 0.7574\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51908\n",
      "Epoch 8/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4970 - categorical_accuracy: 0.7649 - val_loss: 0.5394 - val_categorical_accuracy: 0.7574\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51908\n",
      "Epoch 9/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4963 - categorical_accuracy: 0.7642 - val_loss: 0.4968 - val_categorical_accuracy: 0.7712\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.51908 to 0.49675, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4960 - categorical_accuracy: 0.7650 - val_loss: 0.5105 - val_categorical_accuracy: 0.7621\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.49675\n",
      "Epoch 11/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4974 - categorical_accuracy: 0.7652 - val_loss: 0.4986 - val_categorical_accuracy: 0.7729\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.49675\n",
      "Epoch 12/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4951 - categorical_accuracy: 0.7652 - val_loss: 0.4927 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.49675 to 0.49270, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4935 - categorical_accuracy: 0.7671 - val_loss: 0.5015 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49270\n",
      "Epoch 14/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4927 - categorical_accuracy: 0.7682 - val_loss: 0.5091 - val_categorical_accuracy: 0.7681\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.49270\n",
      "Epoch 15/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4940 - categorical_accuracy: 0.7671 - val_loss: 0.5117 - val_categorical_accuracy: 0.7605\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.49270\n",
      "Epoch 16/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4951 - categorical_accuracy: 0.7664 - val_loss: 0.4890 - val_categorical_accuracy: 0.7722\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.49270 to 0.48899, saving model to best_model.h5\n",
      "Epoch 17/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4927 - categorical_accuracy: 0.7672 - val_loss: 0.4875 - val_categorical_accuracy: 0.7719\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.48899 to 0.48752, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4916 - categorical_accuracy: 0.7683 - val_loss: 0.4953 - val_categorical_accuracy: 0.7574\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.48752\n",
      "Epoch 19/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4902 - categorical_accuracy: 0.7694 - val_loss: 0.4957 - val_categorical_accuracy: 0.7649\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.48752\n",
      "Epoch 20/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4906 - categorical_accuracy: 0.7700 - val_loss: 0.4859 - val_categorical_accuracy: 0.7722\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.48752 to 0.48585, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4915 - categorical_accuracy: 0.7698 - val_loss: 0.4999 - val_categorical_accuracy: 0.7597\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.48585\n",
      "Epoch 22/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4915 - categorical_accuracy: 0.7677 - val_loss: 0.4946 - val_categorical_accuracy: 0.7575\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.48585\n",
      "Epoch 23/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4903 - categorical_accuracy: 0.7690 - val_loss: 0.4873 - val_categorical_accuracy: 0.7697\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.48585\n",
      "Epoch 24/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4892 - categorical_accuracy: 0.7707 - val_loss: 0.4850 - val_categorical_accuracy: 0.7702\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.48585 to 0.48503, saving model to best_model.h5\n",
      "Epoch 25/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4889 - categorical_accuracy: 0.7705 - val_loss: 0.5005 - val_categorical_accuracy: 0.7579\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.48503\n",
      "Epoch 26/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4910 - categorical_accuracy: 0.7695 - val_loss: 0.4862 - val_categorical_accuracy: 0.7654\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.48503\n",
      "Epoch 27/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4904 - categorical_accuracy: 0.7698 - val_loss: 0.4838 - val_categorical_accuracy: 0.7711\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.48503 to 0.48375, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4889 - categorical_accuracy: 0.7708 - val_loss: 0.4843 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.48375\n",
      "Epoch 29/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4884 - categorical_accuracy: 0.7707 - val_loss: 0.4859 - val_categorical_accuracy: 0.7728\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.48375\n",
      "Epoch 30/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4889 - categorical_accuracy: 0.7728 - val_loss: 0.4921 - val_categorical_accuracy: 0.7645\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.48375\n",
      "Epoch 31/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4887 - categorical_accuracy: 0.7712 - val_loss: 0.4945 - val_categorical_accuracy: 0.7640\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.48375\n",
      "Epoch 32/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4877 - categorical_accuracy: 0.7713 - val_loss: 0.4818 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.48375 to 0.48179, saving model to best_model.h5\n",
      "Epoch 33/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4866 - categorical_accuracy: 0.7721 - val_loss: 0.4877 - val_categorical_accuracy: 0.7710\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.48179\n",
      "Epoch 34/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4872 - categorical_accuracy: 0.7721 - val_loss: 0.4871 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.48179\n",
      "Epoch 35/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4872 - categorical_accuracy: 0.7714 - val_loss: 0.4816 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.48179 to 0.48155, saving model to best_model.h5\n",
      "Epoch 36/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4888 - categorical_accuracy: 0.7714 - val_loss: 0.4852 - val_categorical_accuracy: 0.7728\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.48155\n",
      "Epoch 37/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4863 - categorical_accuracy: 0.7718 - val_loss: 0.4805 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.48155 to 0.48051, saving model to best_model.h5\n",
      "Epoch 38/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4872 - categorical_accuracy: 0.7719 - val_loss: 0.4871 - val_categorical_accuracy: 0.7676\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.48051\n",
      "Epoch 39/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4872 - categorical_accuracy: 0.7716 - val_loss: 0.4937 - val_categorical_accuracy: 0.7606\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.48051\n",
      "Epoch 40/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4880 - categorical_accuracy: 0.7715 - val_loss: 0.4892 - val_categorical_accuracy: 0.7574\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.48051\n",
      "Epoch 41/250\n",
      "91457/91457 [==============================] - 1s 13us/step - loss: 0.4868 - categorical_accuracy: 0.7714 - val_loss: 0.4926 - val_categorical_accuracy: 0.7664\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.48051\n",
      "Epoch 42/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4858 - categorical_accuracy: 0.7733 - val_loss: 0.5027 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.48051\n",
      "Epoch 43/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4880 - categorical_accuracy: 0.7718 - val_loss: 0.4795 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.48051 to 0.47946, saving model to best_model.h5\n",
      "Epoch 44/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4860 - categorical_accuracy: 0.7728 - val_loss: 0.4805 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47946\n",
      "Epoch 45/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4855 - categorical_accuracy: 0.7734 - val_loss: 0.4907 - val_categorical_accuracy: 0.7678\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47946\n",
      "Epoch 46/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4856 - categorical_accuracy: 0.7724 - val_loss: 0.4803 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47946\n",
      "Epoch 47/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4867 - categorical_accuracy: 0.7721 - val_loss: 0.4882 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47946\n",
      "Epoch 48/250\n",
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4866 - categorical_accuracy: 0.7732 - val_loss: 0.4886 - val_categorical_accuracy: 0.7611\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47946\n",
      "Epoch 49/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4844 - categorical_accuracy: 0.7731 - val_loss: 0.4877 - val_categorical_accuracy: 0.7693\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47946\n",
      "Epoch 50/250\n",
      "91457/91457 [==============================] - 1s 12us/step - loss: 0.4839 - categorical_accuracy: 0.7734 - val_loss: 0.4823 - val_categorical_accuracy: 0.7699\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47946\n",
      "Epoch 51/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 1s 11us/step - loss: 0.4855 - categorical_accuracy: 0.7739 - val_loss: 0.4864 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47946\n",
      "Epoch 52/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4850 - categorical_accuracy: 0.7731 - val_loss: 0.4847 - val_categorical_accuracy: 0.7678\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47946\n",
      "Epoch 53/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4855 - categorical_accuracy: 0.7720 - val_loss: 0.4830 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47946\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.010000000149011612.\n",
      "Epoch 54/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4812 - categorical_accuracy: 0.7744 - val_loss: 0.4785 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.47946 to 0.47851, saving model to best_model.h5\n",
      "Epoch 55/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4804 - categorical_accuracy: 0.7756 - val_loss: 0.4791 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47851\n",
      "Epoch 56/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4796 - categorical_accuracy: 0.7765 - val_loss: 0.4815 - val_categorical_accuracy: 0.7709\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47851\n",
      "Epoch 57/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4791 - categorical_accuracy: 0.7762 - val_loss: 0.4832 - val_categorical_accuracy: 0.7685\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47851\n",
      "Epoch 58/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4789 - categorical_accuracy: 0.7749 - val_loss: 0.4784 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.47851 to 0.47843, saving model to best_model.h5\n",
      "Epoch 59/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4790 - categorical_accuracy: 0.7760 - val_loss: 0.4805 - val_categorical_accuracy: 0.7706\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47843\n",
      "Epoch 60/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4781 - categorical_accuracy: 0.7765 - val_loss: 0.4797 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47843\n",
      "Epoch 61/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4784 - categorical_accuracy: 0.7776 - val_loss: 0.4782 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.47843 to 0.47823, saving model to best_model.h5\n",
      "Epoch 62/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4777 - categorical_accuracy: 0.7770 - val_loss: 0.4819 - val_categorical_accuracy: 0.7686\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47823\n",
      "Epoch 63/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4772 - categorical_accuracy: 0.7767 - val_loss: 0.4792 - val_categorical_accuracy: 0.7731\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47823\n",
      "Epoch 64/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4772 - categorical_accuracy: 0.7772 - val_loss: 0.4816 - val_categorical_accuracy: 0.7702\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47823\n",
      "Epoch 65/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4773 - categorical_accuracy: 0.7770 - val_loss: 0.4840 - val_categorical_accuracy: 0.7685\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47823\n",
      "Epoch 66/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4778 - categorical_accuracy: 0.7772 - val_loss: 0.4798 - val_categorical_accuracy: 0.7732\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47823\n",
      "Epoch 67/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4774 - categorical_accuracy: 0.7773 - val_loss: 0.4780 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.47823 to 0.47798, saving model to best_model.h5\n",
      "Epoch 68/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4777 - categorical_accuracy: 0.7780 - val_loss: 0.4777 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.47798 to 0.47771, saving model to best_model.h5\n",
      "Epoch 69/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4777 - categorical_accuracy: 0.7780 - val_loss: 0.4806 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47771\n",
      "Epoch 70/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4776 - categorical_accuracy: 0.7781 - val_loss: 0.4800 - val_categorical_accuracy: 0.7719\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47771\n",
      "Epoch 71/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4773 - categorical_accuracy: 0.7781 - val_loss: 0.4788 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47771\n",
      "Epoch 72/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4772 - categorical_accuracy: 0.7776 - val_loss: 0.4793 - val_categorical_accuracy: 0.7721\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.47771\n",
      "Epoch 73/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4774 - categorical_accuracy: 0.7772 - val_loss: 0.4820 - val_categorical_accuracy: 0.7685\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47771\n",
      "Epoch 74/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4769 - categorical_accuracy: 0.7770 - val_loss: 0.4805 - val_categorical_accuracy: 0.7691\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.47771\n",
      "Epoch 75/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4766 - categorical_accuracy: 0.7775 - val_loss: 0.4770 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.47771 to 0.47697, saving model to best_model.h5\n",
      "Epoch 76/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4773 - categorical_accuracy: 0.7781 - val_loss: 0.4803 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.47697\n",
      "Epoch 77/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4765 - categorical_accuracy: 0.7780 - val_loss: 0.4784 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.47697\n",
      "Epoch 78/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4769 - categorical_accuracy: 0.7776 - val_loss: 0.4780 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.47697\n",
      "Epoch 79/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4774 - categorical_accuracy: 0.7779 - val_loss: 0.4791 - val_categorical_accuracy: 0.7721\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.47697\n",
      "Epoch 80/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4770 - categorical_accuracy: 0.7772 - val_loss: 0.4791 - val_categorical_accuracy: 0.7721\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.47697\n",
      "Epoch 81/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4769 - categorical_accuracy: 0.7783 - val_loss: 0.4801 - val_categorical_accuracy: 0.7699\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.47697\n",
      "Epoch 82/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4764 - categorical_accuracy: 0.7780 - val_loss: 0.4796 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.47697\n",
      "Epoch 83/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4769 - categorical_accuracy: 0.7773 - val_loss: 0.4809 - val_categorical_accuracy: 0.7684\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.47697\n",
      "Epoch 84/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4758 - categorical_accuracy: 0.7780 - val_loss: 0.4784 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.47697\n",
      "Epoch 85/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4769 - categorical_accuracy: 0.7774 - val_loss: 0.4783 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.47697\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 86/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4754 - categorical_accuracy: 0.7784 - val_loss: 0.4780 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.47697\n",
      "Epoch 87/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4765 - categorical_accuracy: 0.7776 - val_loss: 0.4785 - val_categorical_accuracy: 0.7734\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.47697\n",
      "Epoch 88/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4764 - categorical_accuracy: 0.7776 - val_loss: 0.4783 - val_categorical_accuracy: 0.7738\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.47697\n",
      "Epoch 89/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4752 - categorical_accuracy: 0.7782 - val_loss: 0.4780 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.47697\n",
      "Epoch 90/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4758 - categorical_accuracy: 0.7780 - val_loss: 0.4783 - val_categorical_accuracy: 0.7731\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.47697\n",
      "Epoch 91/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4754 - categorical_accuracy: 0.7791 - val_loss: 0.4778 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.47697\n",
      "Epoch 92/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4753 - categorical_accuracy: 0.7783 - val_loss: 0.4780 - val_categorical_accuracy: 0.7737\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.47697\n",
      "Epoch 93/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4760 - categorical_accuracy: 0.7776 - val_loss: 0.4783 - val_categorical_accuracy: 0.7729\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.47697\n",
      "Epoch 94/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4756 - categorical_accuracy: 0.7786 - val_loss: 0.4783 - val_categorical_accuracy: 0.7728\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.47697\n",
      "Epoch 95/250\n",
      "91457/91457 [==============================] - 1s 9us/step - loss: 0.4757 - categorical_accuracy: 0.7789 - val_loss: 0.4777 - val_categorical_accuracy: 0.7744\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.47697\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "Epoch 00095: early stopping\n",
      "the 5 fold Log-Loss (NN) is 0.476974\n",
      "PARTIAL: mean Log-Loss (NN) is 0.475503\n",
      "CPU times: user 1h 8min 42s, sys: 3min 30s, total: 1h 12min 12s\n",
      "Wall time: 20min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "loop = 2\n",
    "fold = 5\n",
    "\n",
    "oof_nn = np.zeros([loop, train_y.shape[0], train_y.shape[1]])\n",
    "models_nn = []\n",
    "logloss_csv_nn = []\n",
    "\n",
    "#class_weight_y = class_weight.compute_class_weight('balanced',np.unique(train_y), train_y)\n",
    "\n",
    "for k in range(loop):\n",
    "    kfold = KFold(fold, random_state = 42 + k, shuffle = True)\n",
    "    for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(train_y)):\n",
    "        print(\"-----------\")\n",
    "        print(f'Loop {k+1}/{loop}' + f' Fold {k_fold+1}/{fold}')\n",
    "        print(\"-----------\")\n",
    "        \n",
    "        tr_x, tr_y = train_x[tr_inds], train_y[tr_inds]\n",
    "        val_x, val_y = train_x[val_inds], train_y[val_inds]\n",
    "        \n",
    "        # Train NN\n",
    "        nn, logloss_nn = get_nn(tr_x, tr_y, val_x, val_y, shape=val_x.shape[0])\n",
    "        models_nn.append(nn)\n",
    "        print(\"the %d fold Log-Loss (NN) is %f\"%((k_fold+1), logloss_nn))\n",
    "        logloss_csv_nn.append(logloss_nn)\n",
    "        \n",
    "        #Predict OOF\n",
    "        oof_nn[k, val_inds, :] = nn.predict(val_x)\n",
    "        \n",
    "    print(\"PARTIAL: mean Log-Loss (NN) is %f\"%np.mean(logloss_csv_nn))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_oof_nn = []\n",
    "\n",
    "for k in range(loop):\n",
    "    loss_oof_nn.append(log_loss(train_y, oof_nn[k,...], eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean log-loss (NN) is 0.475503\n",
      "mean OOF log-loss (NN) is 0.475503\n"
     ]
    }
   ],
   "source": [
    "print(\"mean log-loss (NN) is %f\"%np.mean(logloss_csv_nn))\n",
    "print(\"mean OOF log-loss (NN) is %f\"%np.mean(loss_oof_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(models_nn[0].history.history[\"loss\"], \"o-\", alpha=.9, label=\"loss\")\n",
    "plt.plot(models_nn[0].history.history[\"val_loss\"], \"o-\", alpha=.9, label=\"val_loss\")\n",
    "plt.axhline(1, linestyle=\"--\", c=\"C2\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(models_nn[0].history.history[\"categorical_accuracy\"], \"o-\", alpha=.9, label=\"accuracy\")\n",
    "plt.plot(models_nn[0].history.history[\"val_categorical_accuracy\"], \"o-\", alpha=.9, label=\"val_accuracy\")\n",
    "plt.axhline(.7, linestyle=\"--\", c=\"C2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114393, 23)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparando os dados de teste\n",
    "new_test = test.drop(['ID'], axis=1).values\n",
    "test_filtered = feat_selector.transform(new_test)\n",
    "test_filtered = scaler.fit_transform(test_filtered)\n",
    "test_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao para realizar as previsoes baseado em todos os modelos do Kfold\n",
    "def predict(x_te, models_nn):\n",
    "    \n",
    "    model_num_nn = len(models_nn)\n",
    "\n",
    "    for k,m in enumerate(models_nn):\n",
    "        if k==0:\n",
    "            y_pred_nn = predict_proba(m, x_te, batch_size=1024, verbose=1)\n",
    "        else:\n",
    "            y_pred_nn += predict_proba(m, x_te, batch_size=1024, verbose=1)\n",
    "            \n",
    "    y_pred_nn = y_pred_nn / model_num_nn\n",
    "    \n",
    "    return y_pred_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114393/114393 [==============================] - 0s 3us/step\n",
      "114393/114393 [==============================] - 0s 2us/step\n",
      "114393/114393 [==============================] - 0s 2us/step\n",
      "114393/114393 [==============================] - 0s 2us/step\n",
      "114393/114393 [==============================] - 0s 2us/step\n",
      "114393/114393 [==============================] - 0s 2us/step\n",
      "114393/114393 [==============================] - 0s 2us/step\n",
      "114393/114393 [==============================] - 0s 2us/step\n",
      "114393/114393 [==============================] - 0s 2us/step\n",
      "114393/114393 [==============================] - 0s 2us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(114393, 2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = predict(test_filtered, models_nn)\n",
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(model, x, batch_size=32, verbose=0):\n",
    "    preds = model.predict(x, batch_size, verbose)\n",
    "    if preds.min() < 0. or preds.max() > 1.:\n",
    "        warnings.warn('Network returning invalid probability values. '\n",
    "                      'The last layer might not normalize predictions '\n",
    "                      'into probabilities '\n",
    "                      '(like softmax or sigmoid would).')\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7225215 , 0.8121257 , 0.66001093, ..., 0.7577859 , 0.6339866 ,\n",
       "       0.55162346], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114393, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PredictedProb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.722521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.660011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.748874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.791231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  PredictedProb\n",
       "0   0       0.722521\n",
       "1   1       0.812126\n",
       "2   2       0.660011\n",
       "3   7       0.748874\n",
       "4  10       0.791231"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../dataset/sample_submission.csv')\n",
    "submission['PredictedProb'] = test_pred[:,1]\n",
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../submission/submission_nn_v1.0.1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.617608    0.000026\n",
       "0.869949    0.000026\n",
       "0.867662    0.000026\n",
       "0.823046    0.000026\n",
       "0.779732    0.000026\n",
       "              ...   \n",
       "0.916275    0.000009\n",
       "0.666276    0.000009\n",
       "0.916277    0.000009\n",
       "0.916277    0.000009\n",
       "0.750001    0.000009\n",
       "Name: PredictedProb, Length: 113081, dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['PredictedProb'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(submission.PredictedProb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03f293c5aaf34fd7a50a3b2bf14dab7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0af695c1d5e24a17b3a2aebc133ddd39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6782fd7c22664c55904d66122a64843b",
        "IPY_MODEL_9163f58ecd4f4a84922abc60cc69d789"
       ],
       "layout": "IPY_MODEL_d8264eeec6af442097559d8923081902"
      }
     },
     "1c88767c318e4d86a9ab43245f5daa77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "219092655bed428f83aeb751ccaa2a4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "44521ec2de994d5186cdf7d1e445d158": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6782fd7c22664c55904d66122a64843b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c5e6acfe3f654e7ca86e4cddf862e984",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_219092655bed428f83aeb751ccaa2a4d",
       "value": 1000
      }
     },
     "9163f58ecd4f4a84922abc60cc69d789": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9a5c32201e5c447b829c604d757794df",
       "placeholder": "​",
       "style": "IPY_MODEL_b90e11ceb23b4a84a2f4908689dff66f",
       "value": " 1000/1000 [01:50&lt;00:00,  9.06it/s]"
      }
     },
     "9a5c32201e5c447b829c604d757794df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a8bfc018378d49efa1570fcb43b496f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b90e11ceb23b4a84a2f4908689dff66f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c5e6acfe3f654e7ca86e4cddf862e984": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c95734415f404758ba787c86b58e7cc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d0cb9d8667c340c4b49fb7cf32acaf7c",
        "IPY_MODEL_e1aabb08150346cc9871c9f6b3bb5d36"
       ],
       "layout": "IPY_MODEL_1c88767c318e4d86a9ab43245f5daa77"
      }
     },
     "d0cb9d8667c340c4b49fb7cf32acaf7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_44521ec2de994d5186cdf7d1e445d158",
       "max": 17000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_eec34a1f17e2477fb10e7c11419a1382",
       "value": 17000
      }
     },
     "d8264eeec6af442097559d8923081902": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1aabb08150346cc9871c9f6b3bb5d36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a8bfc018378d49efa1570fcb43b496f3",
       "placeholder": "​",
       "style": "IPY_MODEL_03f293c5aaf34fd7a50a3b2bf14dab7d",
       "value": " 17000/17000 [11:01&lt;00:00, 25.68it/s]"
      }
     },
     "eec34a1f17e2477fb10e7c11419a1382": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
