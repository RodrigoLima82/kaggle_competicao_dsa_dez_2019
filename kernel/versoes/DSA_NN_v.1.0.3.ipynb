{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle\n",
    "## Competição DSA de Machine Learning - Dezembro 2019\n",
    "\n",
    "- Teste com redes neurais multicamadas (MLP)\n",
    "- Se gostou ou achou útil, up-vote!! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Versão 1.0.0: LB = 0.50744 / CV = ???**\n",
    "- modelo: NN com 3 camadas\n",
    "- features categoricas: removido\n",
    "- dados missing: atribuído o valor medio\n",
    "- feature selection: 25\n",
    "\n",
    "**Versão 1.0.1: LB = 0.52913 / CV = 0.471703**\n",
    "- modelo: NN com 3 camadas ocultas\n",
    "- dados missing: removido colunas com mais de 50% de NA e as demais usei a média\n",
    "- features categoricas: label encoder\n",
    "- feature engineering: usando pacote Boruta (dica do Allyson)\n",
    "\n",
    "**Versão 1.0.2: LB = 0.50098 / CV = 0.472390**\n",
    "- modelo: NN com 3 camadas ocultas\n",
    "- features engineering: Kernel_Feature_Engineering_v.1.0.0\n",
    "\n",
    "**Versão 1.0.3: LB = ??? / CV = 0.478041 - 0.478498**\n",
    "- modelo: NN com 3 camadas ocultas\n",
    "- features engineering: Kernel_Feature_Engineering_v.1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar os principais pacotes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "import random as rd\n",
    "import os\n",
    "import codecs\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "from numba import jit\n",
    "from collections import Counter\n",
    "import copy\n",
    "from typing import Any\n",
    "\n",
    "seed = 12345\n",
    "np.random.seed(seed)\n",
    "rd.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Evitar que aparece os warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Seta algumas opções no Jupyter para exibição dos datasets\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# Variavel para controlar o treinamento no Kaggle\n",
    "TRAIN_OFFLINE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importa os pacotes de algoritmos\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb \n",
    "\n",
    "# Importa os pacotes de algoritmos de redes neurais (Keras)\n",
    "import keras\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda,BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import Callback,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Importa pacotes do sklearn\n",
    "from sklearn import preprocessing\n",
    "import sklearn.metrics as mtr\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, log_loss, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, StandardScaler\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler as SS\n",
    "from sklearn.model_selection import train_test_split as TTS\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau as RLRP\n",
    "from keras.callbacks import EarlyStopping as ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando os dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \n",
    "    if TRAIN_OFFLINE:\n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        train = pd.read_csv('../dataset/dataset_treino.csv')\n",
    "        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n",
    "\n",
    "        print('Carregando arquivo dataset_teste.csv....')\n",
    "        test = pd.read_csv('../dataset/dataset_teste.csv')\n",
    "        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n",
    "\n",
    "        \n",
    "    else:\n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        train = pd.read_csv('/kaggle/input/competicao-dsa-machine-learning-dec-2019/dataset_treino.csv')\n",
    "        print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n",
    "        \n",
    "        print('Carregando arquivo dataset_treino.csv....')\n",
    "        test = pd.read_csv('/kaggle/input/competicao-dsa-machine-learning-dec-2019/dataset_teste.csv')\n",
    "        print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando arquivo dataset_treino.csv....\n",
      "dataset_treino.csv tem 114321 linhas and 133 colunas\n",
      "Carregando arquivo dataset_teste.csv....\n",
      "dataset_teste.csv tem 114393 linhas and 132 colunas\n"
     ]
    }
   ],
   "source": [
    "# Leitura dos dados\n",
    "train, test = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando arquivo dataset_treino.csv....\n",
      "dataset_treino.csv tem 114321 linhas and 133 colunas\n",
      "Carregando arquivo dataset_teste.csv....\n",
      "dataset_teste.csv tem 114393 linhas and 132 colunas\n"
     ]
    }
   ],
   "source": [
    "# Leitura dos dados\n",
    "train, test = read_data()\n",
    "df = train.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Numerical features:  114\n",
      "Number of Categorical features:  19\n"
     ]
    }
   ],
   "source": [
    "# Verificar a quantidade de features numericas e categoricas\n",
    "\n",
    "numerical_feats = df.dtypes[df.dtypes != \"object\"]\n",
    "print(\"Number of Numerical features: \", len(numerical_feats))\n",
    "\n",
    "categorical_feats = df.dtypes[df.dtypes == \"object\"].index\n",
    "print(\"Number of Categorical features: \", len(categorical_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo alguns outliers\n",
    "df = df[df['v129'] <= 2]\n",
    "df = df[df['v3'] == 'C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ajustando o skewness de algumas features com logaritmo\n",
    "df[\"v19\"]  = np.log1p(df[\"v19\"])\n",
    "df[\"v105\"] = np.log1p(df[\"v105\"])\n",
    "df[\"v119\"] = np.log1p(df[\"v119\"])\n",
    "#df[\"v124\"] = np.log1p(df[\"v124\"])\n",
    "#df[\"v23\"]  = np.log1p(df[\"v23\"])\n",
    "#df[\"v39\"]  = np.log1p(df[\"v39\"])\n",
    "#df[\"v68\"]  = np.log1p(df[\"v68\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove colunas com alta correlacao entre si ou com mais de 95% com zeros\n",
    "#df = df.drop(columns = ['v12','v128','v13','v25','v32','v33','v34','v38','v41','v43','v46',\n",
    "#                           'v49','v53','v54','v55','v60','v63','v64','v65','v67','v73','v76','v77',\n",
    "#                           'v8','v83','v87','v89','v95','v96','v97'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando as features categorias com LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "for i, col in enumerate(df):\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = le.fit_transform(np.array(df[col].astype(str)).reshape((-1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "for c in df.columns:\n",
    "    col_type = df[c].dtype\n",
    "    #if col_type == 'float64' and c != 'target':\n",
    "    if c != 'ID' and c != 'target':\n",
    "        df[c] = scaler.fit_transform(df[c].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando tratamento de missing value\n",
    "for c in df.columns:\n",
    "    if c != 'ID' and c != 'target':\n",
    "        df[c].fillna(df[c].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add decomposed components: PCA / ICA etc.\n",
    "n_comp = 12\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "tsvd_results_df = tsvd.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=42)\n",
    "pca2_results_df = pca.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica2_results_df = ica.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=42)\n",
    "grp_results_df = grp.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=42)\n",
    "srp_results_df = srp.fit_transform(df.drop(columns = ['ID','target'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp+1):\n",
    "    df['pca_' + str(i)]  = pca2_results_df[:,i-1]\n",
    "    df['ica_' + str(i)]  = ica2_results_df[:,i-1]\n",
    "    df['tsvd_' + str(i)] = tsvd_results_df[:,i-1]\n",
    "    df['grp_' + str(i)]  = grp_results_df[:,i-1]\n",
    "    df['srp_' + str(i)]  = srp_results_df[:,i-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217600, 193)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>v1</th>\n",
       "      <th>v10</th>\n",
       "      <th>v100</th>\n",
       "      <th>v101</th>\n",
       "      <th>v102</th>\n",
       "      <th>v103</th>\n",
       "      <th>v104</th>\n",
       "      <th>v105</th>\n",
       "      <th>v106</th>\n",
       "      <th>v107</th>\n",
       "      <th>v108</th>\n",
       "      <th>v109</th>\n",
       "      <th>v11</th>\n",
       "      <th>v110</th>\n",
       "      <th>v111</th>\n",
       "      <th>v112</th>\n",
       "      <th>v113</th>\n",
       "      <th>v114</th>\n",
       "      <th>v115</th>\n",
       "      <th>v116</th>\n",
       "      <th>v117</th>\n",
       "      <th>v118</th>\n",
       "      <th>v119</th>\n",
       "      <th>v12</th>\n",
       "      <th>v120</th>\n",
       "      <th>v121</th>\n",
       "      <th>v122</th>\n",
       "      <th>v123</th>\n",
       "      <th>v124</th>\n",
       "      <th>v125</th>\n",
       "      <th>v126</th>\n",
       "      <th>v127</th>\n",
       "      <th>v128</th>\n",
       "      <th>v129</th>\n",
       "      <th>v13</th>\n",
       "      <th>v130</th>\n",
       "      <th>v131</th>\n",
       "      <th>v14</th>\n",
       "      <th>v15</th>\n",
       "      <th>v16</th>\n",
       "      <th>v17</th>\n",
       "      <th>v18</th>\n",
       "      <th>v19</th>\n",
       "      <th>v2</th>\n",
       "      <th>v20</th>\n",
       "      <th>v21</th>\n",
       "      <th>v22</th>\n",
       "      <th>v23</th>\n",
       "      <th>v24</th>\n",
       "      <th>v25</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>v29</th>\n",
       "      <th>v3</th>\n",
       "      <th>v30</th>\n",
       "      <th>v31</th>\n",
       "      <th>v32</th>\n",
       "      <th>v33</th>\n",
       "      <th>v34</th>\n",
       "      <th>v35</th>\n",
       "      <th>v36</th>\n",
       "      <th>v37</th>\n",
       "      <th>v38</th>\n",
       "      <th>v39</th>\n",
       "      <th>v4</th>\n",
       "      <th>v40</th>\n",
       "      <th>v41</th>\n",
       "      <th>v42</th>\n",
       "      <th>v43</th>\n",
       "      <th>v44</th>\n",
       "      <th>v45</th>\n",
       "      <th>v46</th>\n",
       "      <th>v47</th>\n",
       "      <th>v48</th>\n",
       "      <th>v49</th>\n",
       "      <th>v5</th>\n",
       "      <th>v50</th>\n",
       "      <th>v51</th>\n",
       "      <th>v52</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>v55</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57</th>\n",
       "      <th>v58</th>\n",
       "      <th>v59</th>\n",
       "      <th>v6</th>\n",
       "      <th>v60</th>\n",
       "      <th>v61</th>\n",
       "      <th>v62</th>\n",
       "      <th>v63</th>\n",
       "      <th>v64</th>\n",
       "      <th>v65</th>\n",
       "      <th>v66</th>\n",
       "      <th>v67</th>\n",
       "      <th>v68</th>\n",
       "      <th>v69</th>\n",
       "      <th>v7</th>\n",
       "      <th>v70</th>\n",
       "      <th>v71</th>\n",
       "      <th>v72</th>\n",
       "      <th>v73</th>\n",
       "      <th>v74</th>\n",
       "      <th>v75</th>\n",
       "      <th>v76</th>\n",
       "      <th>v77</th>\n",
       "      <th>v78</th>\n",
       "      <th>v79</th>\n",
       "      <th>v8</th>\n",
       "      <th>v80</th>\n",
       "      <th>v81</th>\n",
       "      <th>v82</th>\n",
       "      <th>v83</th>\n",
       "      <th>v84</th>\n",
       "      <th>v85</th>\n",
       "      <th>v86</th>\n",
       "      <th>v87</th>\n",
       "      <th>v88</th>\n",
       "      <th>v89</th>\n",
       "      <th>v9</th>\n",
       "      <th>v90</th>\n",
       "      <th>v91</th>\n",
       "      <th>v92</th>\n",
       "      <th>v93</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "      <th>v96</th>\n",
       "      <th>v97</th>\n",
       "      <th>v98</th>\n",
       "      <th>v99</th>\n",
       "      <th>pca_1</th>\n",
       "      <th>ica_1</th>\n",
       "      <th>tsvd_1</th>\n",
       "      <th>grp_1</th>\n",
       "      <th>srp_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>ica_2</th>\n",
       "      <th>tsvd_2</th>\n",
       "      <th>grp_2</th>\n",
       "      <th>srp_2</th>\n",
       "      <th>pca_3</th>\n",
       "      <th>ica_3</th>\n",
       "      <th>tsvd_3</th>\n",
       "      <th>grp_3</th>\n",
       "      <th>srp_3</th>\n",
       "      <th>pca_4</th>\n",
       "      <th>ica_4</th>\n",
       "      <th>tsvd_4</th>\n",
       "      <th>grp_4</th>\n",
       "      <th>srp_4</th>\n",
       "      <th>pca_5</th>\n",
       "      <th>ica_5</th>\n",
       "      <th>tsvd_5</th>\n",
       "      <th>grp_5</th>\n",
       "      <th>srp_5</th>\n",
       "      <th>pca_6</th>\n",
       "      <th>ica_6</th>\n",
       "      <th>tsvd_6</th>\n",
       "      <th>grp_6</th>\n",
       "      <th>srp_6</th>\n",
       "      <th>pca_7</th>\n",
       "      <th>ica_7</th>\n",
       "      <th>tsvd_7</th>\n",
       "      <th>grp_7</th>\n",
       "      <th>srp_7</th>\n",
       "      <th>pca_8</th>\n",
       "      <th>ica_8</th>\n",
       "      <th>tsvd_8</th>\n",
       "      <th>grp_8</th>\n",
       "      <th>srp_8</th>\n",
       "      <th>pca_9</th>\n",
       "      <th>ica_9</th>\n",
       "      <th>tsvd_9</th>\n",
       "      <th>grp_9</th>\n",
       "      <th>srp_9</th>\n",
       "      <th>pca_10</th>\n",
       "      <th>ica_10</th>\n",
       "      <th>tsvd_10</th>\n",
       "      <th>grp_10</th>\n",
       "      <th>srp_10</th>\n",
       "      <th>pca_11</th>\n",
       "      <th>ica_11</th>\n",
       "      <th>tsvd_11</th>\n",
       "      <th>grp_11</th>\n",
       "      <th>srp_11</th>\n",
       "      <th>pca_12</th>\n",
       "      <th>ica_12</th>\n",
       "      <th>tsvd_12</th>\n",
       "      <th>grp_12</th>\n",
       "      <th>srp_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.275654</td>\n",
       "      <td>-0.987378</td>\n",
       "      <td>1.067835</td>\n",
       "      <td>0.674750</td>\n",
       "      <td>-0.058035</td>\n",
       "      <td>-0.759300</td>\n",
       "      <td>-1.209903</td>\n",
       "      <td>-0.786148</td>\n",
       "      <td>0.288279</td>\n",
       "      <td>1.181801</td>\n",
       "      <td>0.278535</td>\n",
       "      <td>-0.046171</td>\n",
       "      <td>1.258236</td>\n",
       "      <td>1.024331</td>\n",
       "      <td>-1.974168</td>\n",
       "      <td>0.889489</td>\n",
       "      <td>0.864307</td>\n",
       "      <td>0.769349</td>\n",
       "      <td>-4.055820</td>\n",
       "      <td>-0.503088</td>\n",
       "      <td>-0.453998</td>\n",
       "      <td>-1.214172</td>\n",
       "      <td>-1.221152</td>\n",
       "      <td>-0.858905</td>\n",
       "      <td>-0.316689</td>\n",
       "      <td>-1.430060</td>\n",
       "      <td>0.655249</td>\n",
       "      <td>-0.605648</td>\n",
       "      <td>-0.418600</td>\n",
       "      <td>-0.972244</td>\n",
       "      <td>0.254044</td>\n",
       "      <td>-0.078336</td>\n",
       "      <td>0.016498</td>\n",
       "      <td>-0.466762</td>\n",
       "      <td>-0.793909</td>\n",
       "      <td>-1.023106</td>\n",
       "      <td>0.995685</td>\n",
       "      <td>-0.268430</td>\n",
       "      <td>-0.998675</td>\n",
       "      <td>2.044445</td>\n",
       "      <td>-0.060052</td>\n",
       "      <td>-1.130556</td>\n",
       "      <td>-0.850814</td>\n",
       "      <td>0.442069</td>\n",
       "      <td>0.955301</td>\n",
       "      <td>0.705230</td>\n",
       "      <td>1.405509</td>\n",
       "      <td>-0.277626</td>\n",
       "      <td>-0.963439</td>\n",
       "      <td>-0.527301</td>\n",
       "      <td>-0.268266</td>\n",
       "      <td>0.783797</td>\n",
       "      <td>-1.671470</td>\n",
       "      <td>0.539102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.294630</td>\n",
       "      <td>-0.453732</td>\n",
       "      <td>-0.960028</td>\n",
       "      <td>-1.175528</td>\n",
       "      <td>0.428994</td>\n",
       "      <td>0.197948</td>\n",
       "      <td>-0.885709</td>\n",
       "      <td>-0.544293</td>\n",
       "      <td>-0.155959</td>\n",
       "      <td>1.156430</td>\n",
       "      <td>-0.177018</td>\n",
       "      <td>-0.844050</td>\n",
       "      <td>0.482030</td>\n",
       "      <td>-0.215068</td>\n",
       "      <td>-0.303230</td>\n",
       "      <td>-0.121756</td>\n",
       "      <td>0.338058</td>\n",
       "      <td>-0.523561</td>\n",
       "      <td>-0.943312</td>\n",
       "      <td>-0.191223</td>\n",
       "      <td>0.083027</td>\n",
       "      <td>-0.390014</td>\n",
       "      <td>-0.493190</td>\n",
       "      <td>0.012067</td>\n",
       "      <td>0.126864</td>\n",
       "      <td>1.301914</td>\n",
       "      <td>-0.527220</td>\n",
       "      <td>-0.322817</td>\n",
       "      <td>0.500024</td>\n",
       "      <td>-0.159813</td>\n",
       "      <td>-1.044822</td>\n",
       "      <td>0.162719</td>\n",
       "      <td>0.280774</td>\n",
       "      <td>-0.220197</td>\n",
       "      <td>0.606402</td>\n",
       "      <td>-0.112236</td>\n",
       "      <td>-0.516334</td>\n",
       "      <td>0.033841</td>\n",
       "      <td>1.313802</td>\n",
       "      <td>1.661200</td>\n",
       "      <td>0.034902</td>\n",
       "      <td>-0.999835</td>\n",
       "      <td>4.054921</td>\n",
       "      <td>1.172564</td>\n",
       "      <td>-0.199172</td>\n",
       "      <td>0.701259</td>\n",
       "      <td>-0.517205</td>\n",
       "      <td>-1.036249</td>\n",
       "      <td>-0.073551</td>\n",
       "      <td>0.714585</td>\n",
       "      <td>-0.103087</td>\n",
       "      <td>-0.000794</td>\n",
       "      <td>-2.592016</td>\n",
       "      <td>0.224784</td>\n",
       "      <td>-0.532054</td>\n",
       "      <td>0.720065</td>\n",
       "      <td>0.092164</td>\n",
       "      <td>0.710813</td>\n",
       "      <td>-1.438321</td>\n",
       "      <td>-0.342150</td>\n",
       "      <td>-0.786954</td>\n",
       "      <td>-0.755194</td>\n",
       "      <td>-0.189619</td>\n",
       "      <td>1.346015</td>\n",
       "      <td>-0.507331</td>\n",
       "      <td>0.493548</td>\n",
       "      <td>-0.316868</td>\n",
       "      <td>-1.117668</td>\n",
       "      <td>-0.523471</td>\n",
       "      <td>0.238570</td>\n",
       "      <td>-0.401760</td>\n",
       "      <td>-0.707347</td>\n",
       "      <td>0.888226</td>\n",
       "      <td>-1.112682</td>\n",
       "      <td>0.513062</td>\n",
       "      <td>-0.110201</td>\n",
       "      <td>-3.585664</td>\n",
       "      <td>-0.002524</td>\n",
       "      <td>-3.917597</td>\n",
       "      <td>0.554970</td>\n",
       "      <td>-1.166822</td>\n",
       "      <td>-2.922923</td>\n",
       "      <td>-0.001239</td>\n",
       "      <td>-3.322619</td>\n",
       "      <td>2.089918</td>\n",
       "      <td>-7.100143</td>\n",
       "      <td>-0.959253</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>-1.083140</td>\n",
       "      <td>-0.881790</td>\n",
       "      <td>-2.376196</td>\n",
       "      <td>0.120889</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>-4.786253</td>\n",
       "      <td>1.850191</td>\n",
       "      <td>0.858335</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.862062</td>\n",
       "      <td>-1.035636</td>\n",
       "      <td>-2.390488</td>\n",
       "      <td>-0.859693</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>-0.840062</td>\n",
       "      <td>-1.687347</td>\n",
       "      <td>0.625513</td>\n",
       "      <td>-0.584567</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>-0.588055</td>\n",
       "      <td>2.151814</td>\n",
       "      <td>0.566630</td>\n",
       "      <td>-0.698610</td>\n",
       "      <td>-0.002375</td>\n",
       "      <td>-0.654680</td>\n",
       "      <td>2.527350</td>\n",
       "      <td>-2.271907</td>\n",
       "      <td>1.361426</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>1.302701</td>\n",
       "      <td>-2.263188</td>\n",
       "      <td>-0.052533</td>\n",
       "      <td>0.868118</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.698083</td>\n",
       "      <td>4.037740</td>\n",
       "      <td>-5.547656</td>\n",
       "      <td>0.734112</td>\n",
       "      <td>-0.002135</td>\n",
       "      <td>0.584343</td>\n",
       "      <td>3.229630</td>\n",
       "      <td>1.334592</td>\n",
       "      <td>-1.669155</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>-1.209420</td>\n",
       "      <td>-5.045614</td>\n",
       "      <td>3.860268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.152544</td>\n",
       "      <td>-0.361247</td>\n",
       "      <td>0.347878</td>\n",
       "      <td>-0.096819</td>\n",
       "      <td>-0.234845</td>\n",
       "      <td>-0.143543</td>\n",
       "      <td>-0.149729</td>\n",
       "      <td>0.754226</td>\n",
       "      <td>0.105368</td>\n",
       "      <td>-0.949382</td>\n",
       "      <td>-0.345458</td>\n",
       "      <td>0.040973</td>\n",
       "      <td>0.059764</td>\n",
       "      <td>-0.976247</td>\n",
       "      <td>-0.173830</td>\n",
       "      <td>1.905236</td>\n",
       "      <td>-0.802408</td>\n",
       "      <td>-1.267794</td>\n",
       "      <td>-0.036461</td>\n",
       "      <td>-0.096978</td>\n",
       "      <td>0.657217</td>\n",
       "      <td>-0.112657</td>\n",
       "      <td>-0.157795</td>\n",
       "      <td>-0.366415</td>\n",
       "      <td>-0.200869</td>\n",
       "      <td>-0.222199</td>\n",
       "      <td>-0.041031</td>\n",
       "      <td>-0.316561</td>\n",
       "      <td>-0.148388</td>\n",
       "      <td>-1.562948</td>\n",
       "      <td>-0.115163</td>\n",
       "      <td>-0.167718</td>\n",
       "      <td>-0.047083</td>\n",
       "      <td>-0.466762</td>\n",
       "      <td>-0.172713</td>\n",
       "      <td>-0.292016</td>\n",
       "      <td>-0.134309</td>\n",
       "      <td>-0.268431</td>\n",
       "      <td>-0.124982</td>\n",
       "      <td>0.009864</td>\n",
       "      <td>-0.136239</td>\n",
       "      <td>-0.106147</td>\n",
       "      <td>-0.237748</td>\n",
       "      <td>-0.153884</td>\n",
       "      <td>0.233301</td>\n",
       "      <td>-0.230905</td>\n",
       "      <td>-0.285847</td>\n",
       "      <td>-0.277625</td>\n",
       "      <td>-0.963439</td>\n",
       "      <td>0.479715</td>\n",
       "      <td>-0.093490</td>\n",
       "      <td>-0.082764</td>\n",
       "      <td>-0.019236</td>\n",
       "      <td>0.071174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.294630</td>\n",
       "      <td>-0.453732</td>\n",
       "      <td>-0.117536</td>\n",
       "      <td>-0.220666</td>\n",
       "      <td>-1.360801</td>\n",
       "      <td>-0.036843</td>\n",
       "      <td>0.504732</td>\n",
       "      <td>-0.186837</td>\n",
       "      <td>-0.155959</td>\n",
       "      <td>-0.366315</td>\n",
       "      <td>0.058662</td>\n",
       "      <td>1.240442</td>\n",
       "      <td>0.022154</td>\n",
       "      <td>0.012366</td>\n",
       "      <td>-0.097738</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>0.005302</td>\n",
       "      <td>0.296723</td>\n",
       "      <td>-0.423033</td>\n",
       "      <td>-0.065618</td>\n",
       "      <td>0.007337</td>\n",
       "      <td>0.234703</td>\n",
       "      <td>-0.071136</td>\n",
       "      <td>-0.029223</td>\n",
       "      <td>0.126864</td>\n",
       "      <td>0.075014</td>\n",
       "      <td>-0.042617</td>\n",
       "      <td>-0.225531</td>\n",
       "      <td>1.000102</td>\n",
       "      <td>-0.014516</td>\n",
       "      <td>-0.347035</td>\n",
       "      <td>-0.030960</td>\n",
       "      <td>-0.040551</td>\n",
       "      <td>-0.087151</td>\n",
       "      <td>0.236496</td>\n",
       "      <td>1.368007</td>\n",
       "      <td>0.303106</td>\n",
       "      <td>-0.081848</td>\n",
       "      <td>0.222002</td>\n",
       "      <td>-0.748262</td>\n",
       "      <td>0.025196</td>\n",
       "      <td>0.312371</td>\n",
       "      <td>0.033579</td>\n",
       "      <td>-0.053059</td>\n",
       "      <td>-0.083837</td>\n",
       "      <td>0.701259</td>\n",
       "      <td>0.684646</td>\n",
       "      <td>-0.128398</td>\n",
       "      <td>-0.073551</td>\n",
       "      <td>0.714585</td>\n",
       "      <td>-0.158709</td>\n",
       "      <td>0.051393</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.153407</td>\n",
       "      <td>0.298332</td>\n",
       "      <td>-0.112075</td>\n",
       "      <td>-0.020537</td>\n",
       "      <td>-0.759959</td>\n",
       "      <td>-0.221131</td>\n",
       "      <td>-0.197192</td>\n",
       "      <td>-0.164700</td>\n",
       "      <td>-0.135502</td>\n",
       "      <td>-0.090769</td>\n",
       "      <td>-0.155783</td>\n",
       "      <td>0.436395</td>\n",
       "      <td>0.016196</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>-0.687424</td>\n",
       "      <td>-0.150764</td>\n",
       "      <td>-0.099318</td>\n",
       "      <td>-0.129742</td>\n",
       "      <td>-0.158117</td>\n",
       "      <td>0.052106</td>\n",
       "      <td>-0.095356</td>\n",
       "      <td>0.267483</td>\n",
       "      <td>-0.029997</td>\n",
       "      <td>0.105673</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>-0.169314</td>\n",
       "      <td>-0.472667</td>\n",
       "      <td>0.717916</td>\n",
       "      <td>0.416596</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>-0.033829</td>\n",
       "      <td>-1.872579</td>\n",
       "      <td>3.051130</td>\n",
       "      <td>0.031706</td>\n",
       "      <td>-0.001584</td>\n",
       "      <td>-0.385141</td>\n",
       "      <td>1.793758</td>\n",
       "      <td>2.345591</td>\n",
       "      <td>1.332538</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>1.221032</td>\n",
       "      <td>-0.993199</td>\n",
       "      <td>0.878720</td>\n",
       "      <td>-2.145494</td>\n",
       "      <td>-0.000565</td>\n",
       "      <td>-2.093687</td>\n",
       "      <td>-1.506571</td>\n",
       "      <td>-2.583064</td>\n",
       "      <td>0.863256</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.859790</td>\n",
       "      <td>3.106351</td>\n",
       "      <td>1.196657</td>\n",
       "      <td>-0.082007</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.105353</td>\n",
       "      <td>-0.732773</td>\n",
       "      <td>-0.835868</td>\n",
       "      <td>0.948514</td>\n",
       "      <td>-0.001302</td>\n",
       "      <td>0.923055</td>\n",
       "      <td>0.378069</td>\n",
       "      <td>-2.466910</td>\n",
       "      <td>0.380642</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.265281</td>\n",
       "      <td>0.781056</td>\n",
       "      <td>-0.548029</td>\n",
       "      <td>1.153526</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.672743</td>\n",
       "      <td>0.057144</td>\n",
       "      <td>0.069862</td>\n",
       "      <td>0.768393</td>\n",
       "      <td>-0.000652</td>\n",
       "      <td>1.277272</td>\n",
       "      <td>-0.093204</td>\n",
       "      <td>0.436079</td>\n",
       "      <td>-0.084614</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.099376</td>\n",
       "      <td>-1.721996</td>\n",
       "      <td>-0.314671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.639707</td>\n",
       "      <td>-0.784308</td>\n",
       "      <td>0.489417</td>\n",
       "      <td>-0.402473</td>\n",
       "      <td>0.242918</td>\n",
       "      <td>0.496746</td>\n",
       "      <td>-1.103147</td>\n",
       "      <td>-0.784502</td>\n",
       "      <td>-0.019570</td>\n",
       "      <td>-0.238988</td>\n",
       "      <td>-0.848841</td>\n",
       "      <td>-0.801395</td>\n",
       "      <td>-0.873737</td>\n",
       "      <td>1.024331</td>\n",
       "      <td>-0.013225</td>\n",
       "      <td>1.566654</td>\n",
       "      <td>0.864307</td>\n",
       "      <td>-0.924558</td>\n",
       "      <td>1.263920</td>\n",
       "      <td>1.262251</td>\n",
       "      <td>-1.318107</td>\n",
       "      <td>-1.051538</td>\n",
       "      <td>-1.221151</td>\n",
       "      <td>-0.509955</td>\n",
       "      <td>1.138497</td>\n",
       "      <td>-0.379680</td>\n",
       "      <td>1.399137</td>\n",
       "      <td>-0.420472</td>\n",
       "      <td>-0.429301</td>\n",
       "      <td>-1.602328</td>\n",
       "      <td>0.193972</td>\n",
       "      <td>0.418783</td>\n",
       "      <td>-0.848155</td>\n",
       "      <td>3.074822</td>\n",
       "      <td>-1.101458</td>\n",
       "      <td>-0.829379</td>\n",
       "      <td>-0.490119</td>\n",
       "      <td>-1.735190</td>\n",
       "      <td>-0.140166</td>\n",
       "      <td>0.533469</td>\n",
       "      <td>-0.327050</td>\n",
       "      <td>-0.918720</td>\n",
       "      <td>-0.908679</td>\n",
       "      <td>-0.716882</td>\n",
       "      <td>0.169528</td>\n",
       "      <td>-1.699289</td>\n",
       "      <td>-0.408407</td>\n",
       "      <td>-0.277626</td>\n",
       "      <td>0.833800</td>\n",
       "      <td>-0.536076</td>\n",
       "      <td>0.678859</td>\n",
       "      <td>3.085167</td>\n",
       "      <td>-1.580164</td>\n",
       "      <td>-0.533378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.842251</td>\n",
       "      <td>-0.453732</td>\n",
       "      <td>-0.304452</td>\n",
       "      <td>-0.445402</td>\n",
       "      <td>-1.150835</td>\n",
       "      <td>-0.114711</td>\n",
       "      <td>-0.285604</td>\n",
       "      <td>-0.913259</td>\n",
       "      <td>-0.155959</td>\n",
       "      <td>2.565146</td>\n",
       "      <td>0.251052</td>\n",
       "      <td>0.852087</td>\n",
       "      <td>-0.996988</td>\n",
       "      <td>-0.578135</td>\n",
       "      <td>1.110849</td>\n",
       "      <td>-0.884601</td>\n",
       "      <td>-1.847973</td>\n",
       "      <td>-0.519182</td>\n",
       "      <td>-0.943312</td>\n",
       "      <td>0.055501</td>\n",
       "      <td>-1.309378</td>\n",
       "      <td>-1.657615</td>\n",
       "      <td>-0.752617</td>\n",
       "      <td>0.951305</td>\n",
       "      <td>-0.163807</td>\n",
       "      <td>-0.762074</td>\n",
       "      <td>-0.505715</td>\n",
       "      <td>-0.959264</td>\n",
       "      <td>-1.400272</td>\n",
       "      <td>-0.071737</td>\n",
       "      <td>-0.496303</td>\n",
       "      <td>-0.701081</td>\n",
       "      <td>2.596719</td>\n",
       "      <td>0.822298</td>\n",
       "      <td>0.708167</td>\n",
       "      <td>-0.112236</td>\n",
       "      <td>-0.526552</td>\n",
       "      <td>-0.305563</td>\n",
       "      <td>0.297037</td>\n",
       "      <td>-0.748262</td>\n",
       "      <td>-0.809864</td>\n",
       "      <td>-2.807061</td>\n",
       "      <td>-1.882402</td>\n",
       "      <td>2.444554</td>\n",
       "      <td>-1.625347</td>\n",
       "      <td>-1.556360</td>\n",
       "      <td>1.886497</td>\n",
       "      <td>-0.036731</td>\n",
       "      <td>-0.073551</td>\n",
       "      <td>-1.398976</td>\n",
       "      <td>-0.301223</td>\n",
       "      <td>-1.092159</td>\n",
       "      <td>-0.853925</td>\n",
       "      <td>0.224784</td>\n",
       "      <td>-0.529622</td>\n",
       "      <td>1.030805</td>\n",
       "      <td>1.290849</td>\n",
       "      <td>0.549559</td>\n",
       "      <td>-0.614895</td>\n",
       "      <td>-0.097230</td>\n",
       "      <td>-0.280197</td>\n",
       "      <td>-0.318271</td>\n",
       "      <td>-0.558218</td>\n",
       "      <td>1.389974</td>\n",
       "      <td>-0.501591</td>\n",
       "      <td>1.883799</td>\n",
       "      <td>-0.789134</td>\n",
       "      <td>1.463797</td>\n",
       "      <td>-1.165945</td>\n",
       "      <td>-0.169645</td>\n",
       "      <td>0.411943</td>\n",
       "      <td>-1.237118</td>\n",
       "      <td>-0.694685</td>\n",
       "      <td>-0.842063</td>\n",
       "      <td>1.674241</td>\n",
       "      <td>-0.870342</td>\n",
       "      <td>-0.398987</td>\n",
       "      <td>-0.003507</td>\n",
       "      <td>-0.725067</td>\n",
       "      <td>-1.890098</td>\n",
       "      <td>-6.138869</td>\n",
       "      <td>-6.240810</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>-6.380239</td>\n",
       "      <td>4.007962</td>\n",
       "      <td>-4.627935</td>\n",
       "      <td>3.552112</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>3.611366</td>\n",
       "      <td>1.049829</td>\n",
       "      <td>-1.755852</td>\n",
       "      <td>-0.399683</td>\n",
       "      <td>-0.005431</td>\n",
       "      <td>-0.560023</td>\n",
       "      <td>-6.722105</td>\n",
       "      <td>-0.379918</td>\n",
       "      <td>-1.786987</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>-1.812942</td>\n",
       "      <td>-3.636685</td>\n",
       "      <td>3.400459</td>\n",
       "      <td>4.168886</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>4.172818</td>\n",
       "      <td>-1.435500</td>\n",
       "      <td>-4.177917</td>\n",
       "      <td>-2.419532</td>\n",
       "      <td>0.003315</td>\n",
       "      <td>-2.295020</td>\n",
       "      <td>-5.847409</td>\n",
       "      <td>1.920368</td>\n",
       "      <td>-2.235889</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>-2.219783</td>\n",
       "      <td>0.239170</td>\n",
       "      <td>-8.424546</td>\n",
       "      <td>2.291440</td>\n",
       "      <td>0.002029</td>\n",
       "      <td>2.284546</td>\n",
       "      <td>-2.369013</td>\n",
       "      <td>0.089171</td>\n",
       "      <td>-1.026550</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-1.212573</td>\n",
       "      <td>-0.834143</td>\n",
       "      <td>5.704386</td>\n",
       "      <td>1.073497</td>\n",
       "      <td>-0.002824</td>\n",
       "      <td>0.412738</td>\n",
       "      <td>1.396783</td>\n",
       "      <td>-0.653641</td>\n",
       "      <td>0.983344</td>\n",
       "      <td>-0.003095</td>\n",
       "      <td>1.404729</td>\n",
       "      <td>-2.307927</td>\n",
       "      <td>-3.375368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.775775</td>\n",
       "      <td>3.683220</td>\n",
       "      <td>0.891375</td>\n",
       "      <td>0.725414</td>\n",
       "      <td>-0.200988</td>\n",
       "      <td>-0.355898</td>\n",
       "      <td>-0.078981</td>\n",
       "      <td>-0.615974</td>\n",
       "      <td>0.279839</td>\n",
       "      <td>-0.949382</td>\n",
       "      <td>0.108424</td>\n",
       "      <td>-0.579803</td>\n",
       "      <td>1.148175</td>\n",
       "      <td>1.024331</td>\n",
       "      <td>-0.496868</td>\n",
       "      <td>0.043034</td>\n",
       "      <td>0.864307</td>\n",
       "      <td>0.059088</td>\n",
       "      <td>0.015514</td>\n",
       "      <td>-1.162684</td>\n",
       "      <td>-0.909883</td>\n",
       "      <td>-0.577196</td>\n",
       "      <td>-0.696911</td>\n",
       "      <td>3.297478</td>\n",
       "      <td>-0.172835</td>\n",
       "      <td>-0.586272</td>\n",
       "      <td>0.107518</td>\n",
       "      <td>-0.672832</td>\n",
       "      <td>-0.434667</td>\n",
       "      <td>0.721108</td>\n",
       "      <td>-0.514019</td>\n",
       "      <td>-0.176310</td>\n",
       "      <td>-0.015492</td>\n",
       "      <td>1.304030</td>\n",
       "      <td>0.088515</td>\n",
       "      <td>-0.206014</td>\n",
       "      <td>-0.615645</td>\n",
       "      <td>1.505334</td>\n",
       "      <td>-0.193500</td>\n",
       "      <td>0.328316</td>\n",
       "      <td>-0.091889</td>\n",
       "      <td>0.586927</td>\n",
       "      <td>0.146917</td>\n",
       "      <td>0.298712</td>\n",
       "      <td>0.532935</td>\n",
       "      <td>0.498429</td>\n",
       "      <td>-1.460392</td>\n",
       "      <td>-0.277626</td>\n",
       "      <td>-0.064820</td>\n",
       "      <td>-0.524045</td>\n",
       "      <td>-1.013832</td>\n",
       "      <td>-0.527586</td>\n",
       "      <td>1.424186</td>\n",
       "      <td>0.534173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.294630</td>\n",
       "      <td>1.611834</td>\n",
       "      <td>-0.068473</td>\n",
       "      <td>-0.514018</td>\n",
       "      <td>1.130896</td>\n",
       "      <td>0.590786</td>\n",
       "      <td>-0.895881</td>\n",
       "      <td>-0.583710</td>\n",
       "      <td>-0.155959</td>\n",
       "      <td>-0.402028</td>\n",
       "      <td>0.089381</td>\n",
       "      <td>0.360811</td>\n",
       "      <td>0.759460</td>\n",
       "      <td>0.014709</td>\n",
       "      <td>-1.148313</td>\n",
       "      <td>0.931526</td>\n",
       "      <td>0.251755</td>\n",
       "      <td>-0.525378</td>\n",
       "      <td>-0.943312</td>\n",
       "      <td>-0.180652</td>\n",
       "      <td>0.645947</td>\n",
       "      <td>1.427429</td>\n",
       "      <td>1.644182</td>\n",
       "      <td>-0.981938</td>\n",
       "      <td>0.417536</td>\n",
       "      <td>1.143568</td>\n",
       "      <td>-0.481693</td>\n",
       "      <td>-0.467509</td>\n",
       "      <td>-0.533470</td>\n",
       "      <td>-0.168105</td>\n",
       "      <td>-0.868849</td>\n",
       "      <td>0.591802</td>\n",
       "      <td>-0.560792</td>\n",
       "      <td>-0.885428</td>\n",
       "      <td>0.638805</td>\n",
       "      <td>-0.112236</td>\n",
       "      <td>-0.520858</td>\n",
       "      <td>0.005648</td>\n",
       "      <td>0.631843</td>\n",
       "      <td>-0.748262</td>\n",
       "      <td>0.381993</td>\n",
       "      <td>0.438738</td>\n",
       "      <td>-0.014320</td>\n",
       "      <td>-0.840054</td>\n",
       "      <td>0.577517</td>\n",
       "      <td>0.701259</td>\n",
       "      <td>0.684646</td>\n",
       "      <td>-0.213294</td>\n",
       "      <td>-0.073551</td>\n",
       "      <td>0.714585</td>\n",
       "      <td>-0.134808</td>\n",
       "      <td>0.730873</td>\n",
       "      <td>0.062438</td>\n",
       "      <td>-0.909788</td>\n",
       "      <td>-0.474364</td>\n",
       "      <td>-0.261319</td>\n",
       "      <td>-1.135450</td>\n",
       "      <td>1.917060</td>\n",
       "      <td>-0.474269</td>\n",
       "      <td>-0.257668</td>\n",
       "      <td>-0.871302</td>\n",
       "      <td>0.047008</td>\n",
       "      <td>0.209332</td>\n",
       "      <td>-0.480495</td>\n",
       "      <td>-0.528021</td>\n",
       "      <td>-0.045774</td>\n",
       "      <td>0.372616</td>\n",
       "      <td>-0.687424</td>\n",
       "      <td>0.686721</td>\n",
       "      <td>0.042915</td>\n",
       "      <td>-0.511202</td>\n",
       "      <td>0.580900</td>\n",
       "      <td>0.833476</td>\n",
       "      <td>-0.439396</td>\n",
       "      <td>-0.315780</td>\n",
       "      <td>0.201413</td>\n",
       "      <td>-2.730231</td>\n",
       "      <td>-0.001646</td>\n",
       "      <td>-3.026350</td>\n",
       "      <td>-5.312256</td>\n",
       "      <td>-0.080955</td>\n",
       "      <td>0.673835</td>\n",
       "      <td>-0.000273</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>-1.404225</td>\n",
       "      <td>0.089414</td>\n",
       "      <td>-2.277760</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>-2.873603</td>\n",
       "      <td>-1.266381</td>\n",
       "      <td>-0.543541</td>\n",
       "      <td>-5.872297</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-5.831198</td>\n",
       "      <td>0.646278</td>\n",
       "      <td>-1.303451</td>\n",
       "      <td>0.642633</td>\n",
       "      <td>-0.001048</td>\n",
       "      <td>0.618376</td>\n",
       "      <td>1.404252</td>\n",
       "      <td>-2.224572</td>\n",
       "      <td>-0.805719</td>\n",
       "      <td>-0.005124</td>\n",
       "      <td>-0.804767</td>\n",
       "      <td>0.883393</td>\n",
       "      <td>3.442932</td>\n",
       "      <td>-0.140039</td>\n",
       "      <td>-0.001056</td>\n",
       "      <td>-0.210200</td>\n",
       "      <td>-0.251755</td>\n",
       "      <td>0.115090</td>\n",
       "      <td>1.940719</td>\n",
       "      <td>-0.001762</td>\n",
       "      <td>1.977572</td>\n",
       "      <td>-7.177529</td>\n",
       "      <td>4.728945</td>\n",
       "      <td>-0.056127</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.030881</td>\n",
       "      <td>-2.964102</td>\n",
       "      <td>-1.295518</td>\n",
       "      <td>0.726703</td>\n",
       "      <td>-0.000460</td>\n",
       "      <td>1.277813</td>\n",
       "      <td>-0.177158</td>\n",
       "      <td>-1.383419</td>\n",
       "      <td>-2.022595</td>\n",
       "      <td>-0.001116</td>\n",
       "      <td>-1.752207</td>\n",
       "      <td>2.073431</td>\n",
       "      <td>-1.517254</td>\n",
       "      <td>-0.941347</td>\n",
       "      <td>0.004699</td>\n",
       "      <td>-1.012961</td>\n",
       "      <td>3.032766</td>\n",
       "      <td>3.243011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.152544</td>\n",
       "      <td>-0.564316</td>\n",
       "      <td>0.347878</td>\n",
       "      <td>-0.096819</td>\n",
       "      <td>-0.234845</td>\n",
       "      <td>-0.143543</td>\n",
       "      <td>-0.149729</td>\n",
       "      <td>-0.437881</td>\n",
       "      <td>0.105368</td>\n",
       "      <td>-0.238988</td>\n",
       "      <td>-0.190475</td>\n",
       "      <td>-0.299420</td>\n",
       "      <td>0.059764</td>\n",
       "      <td>-0.976247</td>\n",
       "      <td>-0.173830</td>\n",
       "      <td>1.735945</td>\n",
       "      <td>-0.802408</td>\n",
       "      <td>0.181248</td>\n",
       "      <td>-0.036461</td>\n",
       "      <td>-0.096978</td>\n",
       "      <td>-0.061533</td>\n",
       "      <td>-0.112657</td>\n",
       "      <td>-0.157795</td>\n",
       "      <td>-0.585337</td>\n",
       "      <td>-0.200869</td>\n",
       "      <td>-0.222199</td>\n",
       "      <td>-0.041031</td>\n",
       "      <td>-0.316561</td>\n",
       "      <td>-0.369689</td>\n",
       "      <td>1.705615</td>\n",
       "      <td>-0.115163</td>\n",
       "      <td>-0.167718</td>\n",
       "      <td>-0.207963</td>\n",
       "      <td>-0.466762</td>\n",
       "      <td>-0.172713</td>\n",
       "      <td>-0.292016</td>\n",
       "      <td>-0.134309</td>\n",
       "      <td>-0.734026</td>\n",
       "      <td>-0.124982</td>\n",
       "      <td>0.009864</td>\n",
       "      <td>-0.136239</td>\n",
       "      <td>-0.106147</td>\n",
       "      <td>-0.237748</td>\n",
       "      <td>-0.153884</td>\n",
       "      <td>0.233301</td>\n",
       "      <td>-0.568039</td>\n",
       "      <td>-0.230613</td>\n",
       "      <td>-0.277625</td>\n",
       "      <td>0.833800</td>\n",
       "      <td>-0.419336</td>\n",
       "      <td>-0.093490</td>\n",
       "      <td>-0.082764</td>\n",
       "      <td>-0.019236</td>\n",
       "      <td>0.071174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.842251</td>\n",
       "      <td>-0.453732</td>\n",
       "      <td>-0.117536</td>\n",
       "      <td>-0.220666</td>\n",
       "      <td>-0.152248</td>\n",
       "      <td>-0.036843</td>\n",
       "      <td>0.158687</td>\n",
       "      <td>-0.186837</td>\n",
       "      <td>-0.155959</td>\n",
       "      <td>-0.366315</td>\n",
       "      <td>0.058662</td>\n",
       "      <td>-0.076717</td>\n",
       "      <td>0.022154</td>\n",
       "      <td>0.012366</td>\n",
       "      <td>-0.097738</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>0.005302</td>\n",
       "      <td>-0.414136</td>\n",
       "      <td>1.137805</td>\n",
       "      <td>-0.065618</td>\n",
       "      <td>0.007337</td>\n",
       "      <td>-0.034293</td>\n",
       "      <td>-0.084044</td>\n",
       "      <td>-0.029223</td>\n",
       "      <td>0.417536</td>\n",
       "      <td>0.075014</td>\n",
       "      <td>-0.408059</td>\n",
       "      <td>-0.225531</td>\n",
       "      <td>1.600195</td>\n",
       "      <td>-0.014516</td>\n",
       "      <td>-0.347035</td>\n",
       "      <td>-0.030960</td>\n",
       "      <td>-0.040551</td>\n",
       "      <td>-0.087151</td>\n",
       "      <td>0.236496</td>\n",
       "      <td>-0.112236</td>\n",
       "      <td>-0.413415</td>\n",
       "      <td>-0.081848</td>\n",
       "      <td>0.222002</td>\n",
       "      <td>1.661200</td>\n",
       "      <td>0.025196</td>\n",
       "      <td>0.312371</td>\n",
       "      <td>0.033579</td>\n",
       "      <td>-0.053059</td>\n",
       "      <td>0.096461</td>\n",
       "      <td>0.701259</td>\n",
       "      <td>-0.517205</td>\n",
       "      <td>-0.128398</td>\n",
       "      <td>-0.073551</td>\n",
       "      <td>0.714585</td>\n",
       "      <td>-0.158709</td>\n",
       "      <td>0.051393</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.531598</td>\n",
       "      <td>-0.398618</td>\n",
       "      <td>-0.112075</td>\n",
       "      <td>0.107214</td>\n",
       "      <td>-0.689027</td>\n",
       "      <td>-0.221131</td>\n",
       "      <td>-0.197192</td>\n",
       "      <td>-0.164700</td>\n",
       "      <td>-0.135502</td>\n",
       "      <td>-0.050489</td>\n",
       "      <td>-0.155783</td>\n",
       "      <td>-0.426073</td>\n",
       "      <td>0.016196</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>1.463797</td>\n",
       "      <td>-0.150764</td>\n",
       "      <td>-0.099318</td>\n",
       "      <td>-0.129742</td>\n",
       "      <td>-0.158117</td>\n",
       "      <td>0.052106</td>\n",
       "      <td>-0.095356</td>\n",
       "      <td>-0.006467</td>\n",
       "      <td>-0.029997</td>\n",
       "      <td>-0.176526</td>\n",
       "      <td>0.003172</td>\n",
       "      <td>-0.490020</td>\n",
       "      <td>1.767868</td>\n",
       "      <td>3.167540</td>\n",
       "      <td>-0.887927</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>-1.410602</td>\n",
       "      <td>0.322841</td>\n",
       "      <td>-1.355954</td>\n",
       "      <td>-1.372566</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>-1.662917</td>\n",
       "      <td>-1.069109</td>\n",
       "      <td>2.279020</td>\n",
       "      <td>2.240917</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>2.202936</td>\n",
       "      <td>-0.674175</td>\n",
       "      <td>0.165503</td>\n",
       "      <td>0.403314</td>\n",
       "      <td>-0.000160</td>\n",
       "      <td>0.452442</td>\n",
       "      <td>-1.779336</td>\n",
       "      <td>-0.561282</td>\n",
       "      <td>-0.024966</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>-0.032027</td>\n",
       "      <td>1.022586</td>\n",
       "      <td>-0.394315</td>\n",
       "      <td>0.192698</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.291228</td>\n",
       "      <td>1.662435</td>\n",
       "      <td>-0.295593</td>\n",
       "      <td>0.918557</td>\n",
       "      <td>-0.001415</td>\n",
       "      <td>0.882039</td>\n",
       "      <td>1.180253</td>\n",
       "      <td>-1.532531</td>\n",
       "      <td>-1.238934</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>-1.302302</td>\n",
       "      <td>0.234552</td>\n",
       "      <td>0.821530</td>\n",
       "      <td>0.846140</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.493636</td>\n",
       "      <td>-2.350684</td>\n",
       "      <td>0.414904</td>\n",
       "      <td>0.365121</td>\n",
       "      <td>-0.000440</td>\n",
       "      <td>1.044103</td>\n",
       "      <td>-1.026373</td>\n",
       "      <td>1.694602</td>\n",
       "      <td>0.437974</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>0.257640</td>\n",
       "      <td>1.849796</td>\n",
       "      <td>-0.730044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  target        v1       v10      v100      v101      v102      v103  \\\n",
       "0   3     1.0 -0.275654 -0.987378  1.067835  0.674750 -0.058035 -0.759300   \n",
       "1   4     1.0 -0.152544 -0.361247  0.347878 -0.096819 -0.234845 -0.143543   \n",
       "2   5     1.0 -0.639707 -0.784308  0.489417 -0.402473  0.242918  0.496746   \n",
       "3   6     1.0 -0.775775  3.683220  0.891375  0.725414 -0.200988 -0.355898   \n",
       "4   8     1.0 -0.152544 -0.564316  0.347878 -0.096819 -0.234845 -0.143543   \n",
       "\n",
       "       v104      v105      v106      v107      v108      v109       v11  \\\n",
       "0 -1.209903 -0.786148  0.288279  1.181801  0.278535 -0.046171  1.258236   \n",
       "1 -0.149729  0.754226  0.105368 -0.949382 -0.345458  0.040973  0.059764   \n",
       "2 -1.103147 -0.784502 -0.019570 -0.238988 -0.848841 -0.801395 -0.873737   \n",
       "3 -0.078981 -0.615974  0.279839 -0.949382  0.108424 -0.579803  1.148175   \n",
       "4 -0.149729 -0.437881  0.105368 -0.238988 -0.190475 -0.299420  0.059764   \n",
       "\n",
       "       v110      v111      v112      v113      v114      v115      v116  \\\n",
       "0  1.024331 -1.974168  0.889489  0.864307  0.769349 -4.055820 -0.503088   \n",
       "1 -0.976247 -0.173830  1.905236 -0.802408 -1.267794 -0.036461 -0.096978   \n",
       "2  1.024331 -0.013225  1.566654  0.864307 -0.924558  1.263920  1.262251   \n",
       "3  1.024331 -0.496868  0.043034  0.864307  0.059088  0.015514 -1.162684   \n",
       "4 -0.976247 -0.173830  1.735945 -0.802408  0.181248 -0.036461 -0.096978   \n",
       "\n",
       "       v117      v118      v119       v12      v120      v121      v122  \\\n",
       "0 -0.453998 -1.214172 -1.221152 -0.858905 -0.316689 -1.430060  0.655249   \n",
       "1  0.657217 -0.112657 -0.157795 -0.366415 -0.200869 -0.222199 -0.041031   \n",
       "2 -1.318107 -1.051538 -1.221151 -0.509955  1.138497 -0.379680  1.399137   \n",
       "3 -0.909883 -0.577196 -0.696911  3.297478 -0.172835 -0.586272  0.107518   \n",
       "4 -0.061533 -0.112657 -0.157795 -0.585337 -0.200869 -0.222199 -0.041031   \n",
       "\n",
       "       v123      v124      v125      v126      v127      v128      v129  \\\n",
       "0 -0.605648 -0.418600 -0.972244  0.254044 -0.078336  0.016498 -0.466762   \n",
       "1 -0.316561 -0.148388 -1.562948 -0.115163 -0.167718 -0.047083 -0.466762   \n",
       "2 -0.420472 -0.429301 -1.602328  0.193972  0.418783 -0.848155  3.074822   \n",
       "3 -0.672832 -0.434667  0.721108 -0.514019 -0.176310 -0.015492  1.304030   \n",
       "4 -0.316561 -0.369689  1.705615 -0.115163 -0.167718 -0.207963 -0.466762   \n",
       "\n",
       "        v13      v130      v131       v14       v15       v16       v17  \\\n",
       "0 -0.793909 -1.023106  0.995685 -0.268430 -0.998675  2.044445 -0.060052   \n",
       "1 -0.172713 -0.292016 -0.134309 -0.268431 -0.124982  0.009864 -0.136239   \n",
       "2 -1.101458 -0.829379 -0.490119 -1.735190 -0.140166  0.533469 -0.327050   \n",
       "3  0.088515 -0.206014 -0.615645  1.505334 -0.193500  0.328316 -0.091889   \n",
       "4 -0.172713 -0.292016 -0.134309 -0.734026 -0.124982  0.009864 -0.136239   \n",
       "\n",
       "        v18       v19        v2       v20       v21       v22       v23  \\\n",
       "0 -1.130556 -0.850814  0.442069  0.955301  0.705230  1.405509 -0.277626   \n",
       "1 -0.106147 -0.237748 -0.153884  0.233301 -0.230905 -0.285847 -0.277625   \n",
       "2 -0.918720 -0.908679 -0.716882  0.169528 -1.699289 -0.408407 -0.277626   \n",
       "3  0.586927  0.146917  0.298712  0.532935  0.498429 -1.460392 -0.277626   \n",
       "4 -0.106147 -0.237748 -0.153884  0.233301 -0.568039 -0.230613 -0.277625   \n",
       "\n",
       "        v24       v25       v26       v27       v28       v29   v3       v30  \\\n",
       "0 -0.963439 -0.527301 -0.268266  0.783797 -1.671470  0.539102  0.0 -1.294630   \n",
       "1 -0.963439  0.479715 -0.093490 -0.082764 -0.019236  0.071174  0.0 -1.294630   \n",
       "2  0.833800 -0.536076  0.678859  3.085167 -1.580164 -0.533378  0.0  0.842251   \n",
       "3 -0.064820 -0.524045 -1.013832 -0.527586  1.424186  0.534173  0.0 -1.294630   \n",
       "4  0.833800 -0.419336 -0.093490 -0.082764 -0.019236  0.071174  0.0  0.842251   \n",
       "\n",
       "        v31       v32       v33       v34       v35       v36       v37  \\\n",
       "0 -0.453732 -0.960028 -1.175528  0.428994  0.197948 -0.885709 -0.544293   \n",
       "1 -0.453732 -0.117536 -0.220666 -1.360801 -0.036843  0.504732 -0.186837   \n",
       "2 -0.453732 -0.304452 -0.445402 -1.150835 -0.114711 -0.285604 -0.913259   \n",
       "3  1.611834 -0.068473 -0.514018  1.130896  0.590786 -0.895881 -0.583710   \n",
       "4 -0.453732 -0.117536 -0.220666 -0.152248 -0.036843  0.158687 -0.186837   \n",
       "\n",
       "        v38       v39        v4       v40       v41       v42       v43  \\\n",
       "0 -0.155959  1.156430 -0.177018 -0.844050  0.482030 -0.215068 -0.303230   \n",
       "1 -0.155959 -0.366315  0.058662  1.240442  0.022154  0.012366 -0.097738   \n",
       "2 -0.155959  2.565146  0.251052  0.852087 -0.996988 -0.578135  1.110849   \n",
       "3 -0.155959 -0.402028  0.089381  0.360811  0.759460  0.014709 -1.148313   \n",
       "4 -0.155959 -0.366315  0.058662 -0.076717  0.022154  0.012366 -0.097738   \n",
       "\n",
       "        v44       v45       v46       v47       v48       v49        v5  \\\n",
       "0 -0.121756  0.338058 -0.523561 -0.943312 -0.191223  0.083027 -0.390014   \n",
       "1 -0.001877  0.005302  0.296723 -0.423033 -0.065618  0.007337  0.234703   \n",
       "2 -0.884601 -1.847973 -0.519182 -0.943312  0.055501 -1.309378 -1.657615   \n",
       "3  0.931526  0.251755 -0.525378 -0.943312 -0.180652  0.645947  1.427429   \n",
       "4 -0.001877  0.005302 -0.414136  1.137805 -0.065618  0.007337 -0.034293   \n",
       "\n",
       "        v50       v51       v52       v53       v54       v55       v56  \\\n",
       "0 -0.493190  0.012067  0.126864  1.301914 -0.527220 -0.322817  0.500024   \n",
       "1 -0.071136 -0.029223  0.126864  0.075014 -0.042617 -0.225531  1.000102   \n",
       "2 -0.752617  0.951305 -0.163807 -0.762074 -0.505715 -0.959264 -1.400272   \n",
       "3  1.644182 -0.981938  0.417536  1.143568 -0.481693 -0.467509 -0.533470   \n",
       "4 -0.084044 -0.029223  0.417536  0.075014 -0.408059 -0.225531  1.600195   \n",
       "\n",
       "        v57       v58       v59        v6       v60       v61       v62  \\\n",
       "0 -0.159813 -1.044822  0.162719  0.280774 -0.220197  0.606402 -0.112236   \n",
       "1 -0.014516 -0.347035 -0.030960 -0.040551 -0.087151  0.236496  1.368007   \n",
       "2 -0.071737 -0.496303 -0.701081  2.596719  0.822298  0.708167 -0.112236   \n",
       "3 -0.168105 -0.868849  0.591802 -0.560792 -0.885428  0.638805 -0.112236   \n",
       "4 -0.014516 -0.347035 -0.030960 -0.040551 -0.087151  0.236496 -0.112236   \n",
       "\n",
       "        v63       v64       v65       v66       v67       v68       v69  \\\n",
       "0 -0.516334  0.033841  1.313802  1.661200  0.034902 -0.999835  4.054921   \n",
       "1  0.303106 -0.081848  0.222002 -0.748262  0.025196  0.312371  0.033579   \n",
       "2 -0.526552 -0.305563  0.297037 -0.748262 -0.809864 -2.807061 -1.882402   \n",
       "3 -0.520858  0.005648  0.631843 -0.748262  0.381993  0.438738 -0.014320   \n",
       "4 -0.413415 -0.081848  0.222002  1.661200  0.025196  0.312371  0.033579   \n",
       "\n",
       "         v7       v70       v71       v72       v73       v74       v75  \\\n",
       "0  1.172564 -0.199172  0.701259 -0.517205 -1.036249 -0.073551  0.714585   \n",
       "1 -0.053059 -0.083837  0.701259  0.684646 -0.128398 -0.073551  0.714585   \n",
       "2  2.444554 -1.625347 -1.556360  1.886497 -0.036731 -0.073551 -1.398976   \n",
       "3 -0.840054  0.577517  0.701259  0.684646 -0.213294 -0.073551  0.714585   \n",
       "4 -0.053059  0.096461  0.701259 -0.517205 -0.128398 -0.073551  0.714585   \n",
       "\n",
       "        v76       v77       v78       v79        v8       v80       v81  \\\n",
       "0 -0.103087 -0.000794 -2.592016  0.224784 -0.532054  0.720065  0.092164   \n",
       "1 -0.158709  0.051393 -0.000125 -0.153407  0.298332 -0.112075 -0.020537   \n",
       "2 -0.301223 -1.092159 -0.853925  0.224784 -0.529622  1.030805  1.290849   \n",
       "3 -0.134808  0.730873  0.062438 -0.909788 -0.474364 -0.261319 -1.135450   \n",
       "4 -0.158709  0.051393 -0.000125 -0.531598 -0.398618 -0.112075  0.107214   \n",
       "\n",
       "        v82       v83       v84       v85       v86       v87       v88  \\\n",
       "0  0.710813 -1.438321 -0.342150 -0.786954 -0.755194 -0.189619  1.346015   \n",
       "1 -0.759959 -0.221131 -0.197192 -0.164700 -0.135502 -0.090769 -0.155783   \n",
       "2  0.549559 -0.614895 -0.097230 -0.280197 -0.318271 -0.558218  1.389974   \n",
       "3  1.917060 -0.474269 -0.257668 -0.871302  0.047008  0.209332 -0.480495   \n",
       "4 -0.689027 -0.221131 -0.197192 -0.164700 -0.135502 -0.050489 -0.155783   \n",
       "\n",
       "        v89        v9       v90       v91       v92       v93       v94  \\\n",
       "0 -0.507331  0.493548 -0.316868 -1.117668 -0.523471  0.238570 -0.401760   \n",
       "1  0.436395  0.016196  0.006097 -0.687424 -0.150764 -0.099318 -0.129742   \n",
       "2 -0.501591  1.883799 -0.789134  1.463797 -1.165945 -0.169645  0.411943   \n",
       "3 -0.528021 -0.045774  0.372616 -0.687424  0.686721  0.042915 -0.511202   \n",
       "4 -0.426073  0.016196  0.006097  1.463797 -0.150764 -0.099318 -0.129742   \n",
       "\n",
       "        v95       v96       v97       v98       v99     pca_1     ica_1  \\\n",
       "0 -0.707347  0.888226 -1.112682  0.513062 -0.110201 -3.585664 -0.002524   \n",
       "1 -0.158117  0.052106 -0.095356  0.267483 -0.029997  0.105673  0.000856   \n",
       "2 -1.237118 -0.694685 -0.842063  1.674241 -0.870342 -0.398987 -0.003507   \n",
       "3  0.580900  0.833476 -0.439396 -0.315780  0.201413 -2.730231 -0.001646   \n",
       "4 -0.158117  0.052106 -0.095356 -0.006467 -0.029997 -0.176526  0.003172   \n",
       "\n",
       "     tsvd_1     grp_1     srp_1     pca_2     ica_2    tsvd_2     grp_2  \\\n",
       "0 -3.917597  0.554970 -1.166822 -2.922923 -0.001239 -3.322619  2.089918   \n",
       "1 -0.169314 -0.472667  0.717916  0.416596  0.002810 -0.033829 -1.872579   \n",
       "2 -0.725067 -1.890098 -6.138869 -6.240810  0.002527 -6.380239  4.007962   \n",
       "3 -3.026350 -5.312256 -0.080955  0.673835 -0.000273  0.107900 -1.404225   \n",
       "4 -0.490020  1.767868  3.167540 -0.887927 -0.000470 -1.410602  0.322841   \n",
       "\n",
       "      srp_2     pca_3     ica_3    tsvd_3     grp_3     srp_3     pca_4  \\\n",
       "0 -7.100143 -0.959253  0.001249 -1.083140 -0.881790 -2.376196  0.120889   \n",
       "1  3.051130  0.031706 -0.001584 -0.385141  1.793758  2.345591  1.332538   \n",
       "2 -4.627935  3.552112 -0.000028  3.611366  1.049829 -1.755852 -0.399683   \n",
       "3  0.089414 -2.277760  0.001416 -2.873603 -1.266381 -0.543541 -5.872297   \n",
       "4 -1.355954 -1.372566  0.000413 -1.662917 -1.069109  2.279020  2.240917   \n",
       "\n",
       "      ica_4    tsvd_4     grp_4     srp_4     pca_5     ica_5    tsvd_5  \\\n",
       "0  0.000223  0.123400 -4.786253  1.850191  0.858335  0.000004  0.862062   \n",
       "1  0.000201  1.221032 -0.993199  0.878720 -2.145494 -0.000565 -2.093687   \n",
       "2 -0.005431 -0.560023 -6.722105 -0.379918 -1.786987  0.000863 -1.812942   \n",
       "3  0.000200 -5.831198  0.646278 -1.303451  0.642633 -0.001048  0.618376   \n",
       "4  0.000438  2.202936 -0.674175  0.165503  0.403314 -0.000160  0.452442   \n",
       "\n",
       "      grp_5     srp_5     pca_6     ica_6    tsvd_6     grp_6     srp_6  \\\n",
       "0 -1.035636 -2.390488 -0.859693  0.001271 -0.840062 -1.687347  0.625513   \n",
       "1 -1.506571 -2.583064  0.863256  0.001332  0.859790  3.106351  1.196657   \n",
       "2 -3.636685  3.400459  4.168886  0.002591  4.172818 -1.435500 -4.177917   \n",
       "3  1.404252 -2.224572 -0.805719 -0.005124 -0.804767  0.883393  3.442932   \n",
       "4 -1.779336 -0.561282 -0.024966  0.000836 -0.032027  1.022586 -0.394315   \n",
       "\n",
       "      pca_7     ica_7    tsvd_7     grp_7     srp_7     pca_8     ica_8  \\\n",
       "0 -0.584567  0.000831 -0.588055  2.151814  0.566630 -0.698610 -0.002375   \n",
       "1 -0.082007  0.000640  0.105353 -0.732773 -0.835868  0.948514 -0.001302   \n",
       "2 -2.419532  0.003315 -2.295020 -5.847409  1.920368 -2.235889  0.003322   \n",
       "3 -0.140039 -0.001056 -0.210200 -0.251755  0.115090  1.940719 -0.001762   \n",
       "4  0.192698  0.000419  0.291228  1.662435 -0.295593  0.918557 -0.001415   \n",
       "\n",
       "     tsvd_8     grp_8     srp_8     pca_9     ica_9    tsvd_9     grp_9  \\\n",
       "0 -0.654680  2.527350 -2.271907  1.361426 -0.001013  1.302701 -2.263188   \n",
       "1  0.923055  0.378069 -2.466910  0.380642  0.000514  0.265281  0.781056   \n",
       "2 -2.219783  0.239170 -8.424546  2.291440  0.002029  2.284546 -2.369013   \n",
       "3  1.977572 -7.177529  4.728945 -0.056127  0.000069  0.030881 -2.964102   \n",
       "4  0.882039  1.180253 -1.532531 -1.238934  0.000337 -1.302302  0.234552   \n",
       "\n",
       "      srp_9    pca_10    ica_10   tsvd_10    grp_10    srp_10    pca_11  \\\n",
       "0 -0.052533  0.868118  0.002003  0.698083  4.037740 -5.547656  0.734112   \n",
       "1 -0.548029  1.153526  0.000447  0.672743  0.057144  0.069862  0.768393   \n",
       "2  0.089171 -1.026550 -0.000067 -1.212573 -0.834143  5.704386  1.073497   \n",
       "3 -1.295518  0.726703 -0.000460  1.277813 -0.177158 -1.383419 -2.022595   \n",
       "4  0.821530  0.846140  0.000065  0.493636 -2.350684  0.414904  0.365121   \n",
       "\n",
       "     ica_11   tsvd_11    grp_11    srp_11    pca_12    ica_12   tsvd_12  \\\n",
       "0 -0.002135  0.584343  3.229630  1.334592 -1.669155 -0.000162 -1.209420   \n",
       "1 -0.000652  1.277272 -0.093204  0.436079 -0.084614  0.000083  0.099376   \n",
       "2 -0.002824  0.412738  1.396783 -0.653641  0.983344 -0.003095  1.404729   \n",
       "3 -0.001116 -1.752207  2.073431 -1.517254 -0.941347  0.004699 -1.012961   \n",
       "4 -0.000440  1.044103 -1.026373  1.694602  0.437974 -0.000167  0.257640   \n",
       "\n",
       "     grp_12    srp_12  \n",
       "0 -5.045614  3.860268  \n",
       "1 -1.721996 -0.314671  \n",
       "2 -2.307927 -3.375368  \n",
       "3  3.032766  3.243011  \n",
       "4  1.849796 -0.730044  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "treino = df[df['target'].notnull()]\n",
    "teste = df[df['target'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Feature  importance\n",
      "77       v50    0.148536\n",
      "94       v66    0.043550\n",
      "180   srp_10    0.020940\n",
      "160    srp_6    0.020674\n",
      "56       v31    0.016957\n",
      "108      v79    0.015296\n",
      "146    pca_4    0.014349\n",
      "148   tsvd_4    0.013262\n",
      "23       v12    0.013094\n",
      "13      v110    0.012757\n",
      "73       v47    0.012399\n",
      "132    ica_1    0.011449\n",
      "137    ica_2    0.010966\n",
      "1        v10    0.009958\n",
      "17      v114    0.009801\n",
      "83       v56    0.009724\n",
      "174    grp_9    0.009468\n",
      "59       v34    0.009433\n",
      "157    ica_6    0.009184\n",
      "46       v22    0.008927\n",
      "66       v40    0.008920\n",
      "172    ica_9    0.008823\n",
      "184   grp_11    0.008469\n",
      "139    grp_2    0.008451\n",
      "45       v21    0.008432\n",
      "164    grp_7    0.008307\n",
      "167    ica_8    0.008080\n",
      "179   grp_10    0.008023\n",
      "134    grp_1    0.007994\n",
      "154    grp_5    0.007894\n",
      "176   pca_10    0.007574\n",
      "169    grp_8    0.007536\n",
      "155    srp_5    0.007502\n",
      "149    grp_4    0.007485\n",
      "189   grp_12    0.007367\n",
      "153   tsvd_5    0.007297\n",
      "159    grp_6    0.007087\n",
      "188  tsvd_12    0.006990\n",
      "185   srp_11    0.006962\n",
      "48       v24    0.006948\n",
      "151    pca_5    0.006927\n",
      "140    srp_2    0.006863\n",
      "178  tsvd_10    0.006839\n",
      "186   pca_12    0.006725\n",
      "37       v14    0.006680\n",
      "144    grp_3    0.006659\n",
      "152    ica_5    0.006574\n",
      "183  tsvd_11    0.006490\n",
      "135    srp_1    0.006436\n",
      "181   pca_11    0.006229\n",
      "173   tsvd_9    0.006142\n",
      "187   ica_12    0.006141\n",
      "142    ica_3    0.006036\n",
      "29      v125    0.006015\n",
      "171    pca_9    0.005962\n",
      "150    srp_4    0.005925\n",
      "177   ica_10    0.005868\n",
      "162    ica_7    0.005811\n",
      "182   ica_11    0.005783\n",
      "33      v129    0.005736\n",
      "147    ica_4    0.005711\n",
      "166    pca_8    0.005524\n",
      "168   tsvd_8    0.005469\n",
      "190   srp_12    0.005467\n",
      "143   tsvd_3    0.005334\n",
      "161    pca_7    0.005224\n",
      "141    pca_3    0.005209\n",
      "136    pca_2    0.005187\n",
      "163   tsvd_7    0.004965\n",
      "170    srp_8    0.004951\n",
      "165    srp_7    0.004949\n",
      "158   tsvd_6    0.004946\n",
      "138   tsvd_2    0.004914\n",
      "156    pca_6    0.004846\n",
      "16      v113    0.004842\n",
      "145    srp_3    0.004826\n",
      "131    pca_1    0.004227\n",
      "79       v52    0.003990\n",
      "133   tsvd_1    0.003883\n",
      "15      v112    0.003096\n",
      "175    srp_9    0.002926\n",
      "97       v69    0.002797\n",
      "55       v30    0.002771\n",
      "24      v120    0.002748\n",
      "90       v62    0.002651\n",
      "87        v6    0.002618\n",
      "130      v99    0.002590\n",
      "118      v88    0.002540\n",
      "4       v102    0.002510\n",
      "64       v39    0.002486\n",
      "107      v78    0.002475\n",
      "39       v16    0.002464\n",
      "18      v115    0.002402\n",
      "84       v57    0.002348\n",
      "68       v42    0.002327\n",
      "129      v98    0.002313\n",
      "0         v1    0.002309\n",
      "96       v68    0.002309\n",
      "52       v28    0.002302\n",
      "71       v45    0.002288\n",
      "28      v124    0.002276\n",
      "22      v119    0.002231\n",
      "2       v100    0.002215\n",
      "85       v58    0.002215\n",
      "41       v18    0.002198\n",
      "128      v97    0.002186\n",
      "36      v131    0.002186\n",
      "26      v122    0.002175\n",
      "120       v9    0.002168\n",
      "42       v19    0.002163\n",
      "30      v126    0.002161\n",
      "62       v37    0.002150\n",
      "31      v127    0.002144\n",
      "43        v2    0.002143\n",
      "122      v91    0.002121\n",
      "115      v85    0.002115\n",
      "60       v35    0.002110\n",
      "99       v70    0.002103\n",
      "110      v80    0.002079\n",
      "98        v7    0.002079\n",
      "112      v82    0.002074\n",
      "121      v90    0.002069\n",
      "44       v20    0.002065\n",
      "51       v27    0.002038\n",
      "70       v44    0.002027\n",
      "61       v36    0.002015\n",
      "80       v53    0.002003\n",
      "12       v11    0.001969\n",
      "116      v86    0.001939\n",
      "5       v103    0.001926\n",
      "20      v117    0.001923\n",
      "125      v94    0.001889\n",
      "14      v111    0.001830\n",
      "10      v108    0.001817\n",
      "123      v92    0.001815\n",
      "117      v87    0.001807\n",
      "126      v95    0.001802\n",
      "65        v4    0.001801\n",
      "75       v49    0.001799\n",
      "34       v13    0.001785\n",
      "81       v54    0.001771\n",
      "21      v118    0.001742\n",
      "78       v51    0.001739\n",
      "11      v109    0.001738\n",
      "114      v84    0.001705\n",
      "69       v43    0.001694\n",
      "6       v104    0.001689\n",
      "119      v89    0.001678\n",
      "9       v107    0.001669\n",
      "27      v123    0.001655\n",
      "82       v55    0.001636\n",
      "93       v65    0.001636\n",
      "86       v59    0.001616\n",
      "38       v15    0.001614\n",
      "89       v61    0.001614\n",
      "3       v101    0.001614\n",
      "88       v60    0.001611\n",
      "35      v130    0.001607\n",
      "102      v73    0.001599\n",
      "113      v83    0.001578\n",
      "76        v5    0.001572\n",
      "19      v116    0.001563\n",
      "109       v8    0.001536\n",
      "111      v81    0.001533\n",
      "49       v25    0.001526\n",
      "7       v105    0.001525\n",
      "50       v26    0.001520\n",
      "106      v77    0.001516\n",
      "57       v32    0.001502\n",
      "25      v121    0.001484\n",
      "32      v128    0.001473\n",
      "91       v63    0.001443\n",
      "124      v93    0.001417\n",
      "72       v46    0.001417\n",
      "95       v67    0.001397\n",
      "58       v33    0.001321\n",
      "74       v48    0.001318\n",
      "127      v96    0.001303\n",
      "105      v76    0.001262\n",
      "67       v41    0.001259\n",
      "92       v64    0.001150\n",
      "8       v106    0.001122\n",
      "40       v17    0.001115\n",
      "53       v29    0.001064\n",
      "101      v72    0.000407\n",
      "47       v23    0.000296\n",
      "63       v38    0.000274\n",
      "103      v74    0.000178\n",
      "100      v71    0.000167\n",
      "104      v75    0.000082\n",
      "54        v3    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Importância do Atributo com o Random Forest Regressor\n",
    "X_ = treino.drop(['ID','target'], axis=1)\n",
    "y_ = treino['target']\n",
    "\n",
    "# Criação do Modelo - Feature Selection\n",
    "modeloRF = RandomForestClassifier(bootstrap=False, \n",
    "                                 max_features=0.3, \n",
    "                                 min_samples_leaf=15, \n",
    "                                 min_samples_split=8, \n",
    "                                 n_estimators=50, \n",
    "                                 n_jobs=-1, \n",
    "                                 random_state=42)\n",
    "modeloRF.fit(X_, y_)\n",
    "\n",
    "# Convertendo o resultado em um dataframe\n",
    "feature_importance_df = pd.DataFrame(treino.drop(['ID','target'], axis=1).columns,columns=['Feature'])\n",
    "feature_importance_df['importance'] = pd.DataFrame(modeloRF.feature_importances_.astype(float))\n",
    "\n",
    "# Realizando a ordenacao por Importancia (Maior para Menor)\n",
    "result = feature_importance_df.sort_values('importance',ascending=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:35].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(18,16))\n",
    "sns.barplot(x=\"importance\",\n",
    "           y=\"Feature\",\n",
    "           data=best_features.sort_values(by=\"importance\",\n",
    "                                          ascending=False))\n",
    "plt.title('Importance Features')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo Neural Network MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((108764, 35), (108764, 2))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar um dataset somente com as colunas mais importantes conforme Feature Selection\n",
    "new_X = treino.loc[:,best_features['Feature']]\n",
    "\n",
    "# Padronizando os dados de treino\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(new_X)\n",
    "    \n",
    "# Separando features preditoras e target\n",
    "#train_x = new_df.drop(['ID','target'], axis=1)\n",
    "train_y = np_utils.to_categorical(treino['target'])\n",
    "\n",
    "# Verificando o shape dos datasets depois dos ajustes\n",
    "# Neste momento está pronto para ser usado pelo treinamento\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limpeza da memória\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando função para treinar a rede neural\n",
    "def get_nn(x_tr,y_tr,x_val,y_val,shape):\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Cria a estrutura da rede neural com 3 camadas ocultas\n",
    "    inp = Input(shape = (x_tr.shape[1],))\n",
    "\n",
    "    x = Dense(1024, input_dim=x_tr.shape[1], activation='relu')(inp)\n",
    "    x = Dropout(0.5)(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    out = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inp,out)\n",
    "    \n",
    "    model.compile(optimizer = optimizers.adam(lr = 0.09, decay = 0.01),\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['categorical_accuracy'])\n",
    "    \n",
    "    # Realiza a parada mais cedo quando percebe overfitting\n",
    "    es = EarlyStopping(monitor='val_loss', \n",
    "                       mode='min',\n",
    "                       restore_best_weights=True, \n",
    "                       verbose=1, \n",
    "                       patience=20)\n",
    "\n",
    "    # Realiza checkpoint durante o treinamento\n",
    "    mc = ModelCheckpoint('best_model.h5',\n",
    "                         monitor='val_loss',\n",
    "                         mode='min',\n",
    "                         save_best_only=True, \n",
    "                         verbose=1, \n",
    "                         save_weights_only=True)\n",
    "\n",
    "    # Realize o ajuste na Learning Rate durante o treinamento\n",
    "    rl = ReduceLROnPlateau(monitor='val_loss', \n",
    "                           factor=0.1, \n",
    "                           patience=10, \n",
    "                           verbose=1, \n",
    "                           epsilon=1e-4, \n",
    "                           mode='min')\n",
    "\n",
    "    # Realiza o fit do modelo\n",
    "    model.fit(x_tr, y_tr,\n",
    "              validation_data=[x_val, y_val],\n",
    "              callbacks=[es,mc,rl],\n",
    "              epochs=250, \n",
    "              batch_size=1024,\n",
    "              verbose=1,\n",
    "              shuffle=True)\n",
    "    \n",
    "    # Carrega os melhores pesos\n",
    "    model.load_weights(\"best_model.h5\")\n",
    "    \n",
    "    # Realiza as previsões\n",
    "    y_pred = model.predict(x_val)\n",
    "    y_valid = y_val\n",
    "             \n",
    "    # Calcula o log loss\n",
    "    logloss = log_loss(y_valid, y_pred, eps=1e-15)\n",
    "\n",
    "    return model, logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "Loop 1/2 Fold 1/5\n",
      "-----------\n",
      "Train on 87011 samples, validate on 21753 samples\n",
      "Epoch 1/250\n",
      "87011/87011 [==============================] - 3s 33us/step - loss: 0.9580 - categorical_accuracy: 0.7201 - val_loss: 1.4611 - val_categorical_accuracy: 0.4084\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.46107, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5188 - categorical_accuracy: 0.7565 - val_loss: 0.6158 - val_categorical_accuracy: 0.6438\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.46107 to 0.61580, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "87011/87011 [==============================] - 2s 27us/step - loss: 0.5143 - categorical_accuracy: 0.7575 - val_loss: 0.5222 - val_categorical_accuracy: 0.7525\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61580 to 0.52224, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "87011/87011 [==============================] - 2s 27us/step - loss: 0.5086 - categorical_accuracy: 0.7586 - val_loss: 0.5082 - val_categorical_accuracy: 0.7531\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52224 to 0.50821, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5067 - categorical_accuracy: 0.7589 - val_loss: 0.5041 - val_categorical_accuracy: 0.7566\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.50821 to 0.50412, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5035 - categorical_accuracy: 0.7599 - val_loss: 0.5028 - val_categorical_accuracy: 0.7611\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.50412 to 0.50282, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5009 - categorical_accuracy: 0.7621 - val_loss: 0.5156 - val_categorical_accuracy: 0.7523\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.50282\n",
      "Epoch 8/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.5021 - categorical_accuracy: 0.7613 - val_loss: 0.5027 - val_categorical_accuracy: 0.7645\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.50282 to 0.50265, saving model to best_model.h5\n",
      "Epoch 9/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4998 - categorical_accuracy: 0.7623 - val_loss: 0.5071 - val_categorical_accuracy: 0.7574\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.50265\n",
      "Epoch 10/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4989 - categorical_accuracy: 0.7635 - val_loss: 0.4974 - val_categorical_accuracy: 0.7672\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.50265 to 0.49740, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4969 - categorical_accuracy: 0.7647 - val_loss: 0.4976 - val_categorical_accuracy: 0.7622\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.49740\n",
      "Epoch 12/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4952 - categorical_accuracy: 0.7663 - val_loss: 0.4969 - val_categorical_accuracy: 0.7677\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.49740 to 0.49689, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4947 - categorical_accuracy: 0.7658 - val_loss: 0.4982 - val_categorical_accuracy: 0.7610\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49689\n",
      "Epoch 14/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4958 - categorical_accuracy: 0.7653 - val_loss: 0.4961 - val_categorical_accuracy: 0.7642\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.49689 to 0.49609, saving model to best_model.h5\n",
      "Epoch 15/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4937 - categorical_accuracy: 0.7663 - val_loss: 0.4933 - val_categorical_accuracy: 0.7695\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.49609 to 0.49332, saving model to best_model.h5\n",
      "Epoch 16/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4936 - categorical_accuracy: 0.7670 - val_loss: 0.4928 - val_categorical_accuracy: 0.7694\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.49332 to 0.49285, saving model to best_model.h5\n",
      "Epoch 17/250\n",
      "87011/87011 [==============================] - 2s 27us/step - loss: 0.4934 - categorical_accuracy: 0.7677 - val_loss: 0.4955 - val_categorical_accuracy: 0.7659\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.49285\n",
      "Epoch 18/250\n",
      "87011/87011 [==============================] - 2s 27us/step - loss: 0.4928 - categorical_accuracy: 0.7676 - val_loss: 0.4942 - val_categorical_accuracy: 0.7675\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.49285\n",
      "Epoch 19/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4924 - categorical_accuracy: 0.7690 - val_loss: 0.4932 - val_categorical_accuracy: 0.7695\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.49285\n",
      "Epoch 20/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4922 - categorical_accuracy: 0.7681 - val_loss: 0.4926 - val_categorical_accuracy: 0.7695\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.49285 to 0.49263, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4915 - categorical_accuracy: 0.7691 - val_loss: 0.4941 - val_categorical_accuracy: 0.7688\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.49263\n",
      "Epoch 22/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4919 - categorical_accuracy: 0.7681 - val_loss: 0.4914 - val_categorical_accuracy: 0.7706\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.49263 to 0.49140, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4904 - categorical_accuracy: 0.7697 - val_loss: 0.4933 - val_categorical_accuracy: 0.7702\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.49140\n",
      "Epoch 24/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4916 - categorical_accuracy: 0.7690 - val_loss: 0.4911 - val_categorical_accuracy: 0.7701\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.49140 to 0.49109, saving model to best_model.h5\n",
      "Epoch 25/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4911 - categorical_accuracy: 0.7690 - val_loss: 0.4913 - val_categorical_accuracy: 0.7708\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.49109\n",
      "Epoch 26/250\n",
      "87011/87011 [==============================] - 2s 28us/step - loss: 0.4901 - categorical_accuracy: 0.7704 - val_loss: 0.4931 - val_categorical_accuracy: 0.7680\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.49109\n",
      "Epoch 27/250\n",
      "87011/87011 [==============================] - 5s 59us/step - loss: 0.4907 - categorical_accuracy: 0.7706 - val_loss: 0.4903 - val_categorical_accuracy: 0.7708\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.49109 to 0.49031, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4898 - categorical_accuracy: 0.7703 - val_loss: 0.4900 - val_categorical_accuracy: 0.7716\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.49031 to 0.48996, saving model to best_model.h5\n",
      "Epoch 29/250\n",
      "87011/87011 [==============================] - 6s 64us/step - loss: 0.4896 - categorical_accuracy: 0.7701 - val_loss: 0.4896 - val_categorical_accuracy: 0.7707\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.48996 to 0.48964, saving model to best_model.h5\n",
      "Epoch 30/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4885 - categorical_accuracy: 0.7726 - val_loss: 0.4896 - val_categorical_accuracy: 0.7719\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.48964 to 0.48963, saving model to best_model.h5\n",
      "Epoch 31/250\n",
      "87011/87011 [==============================] - 6s 70us/step - loss: 0.4890 - categorical_accuracy: 0.7709 - val_loss: 0.4890 - val_categorical_accuracy: 0.7712\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.48963 to 0.48896, saving model to best_model.h5\n",
      "Epoch 32/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.4878 - categorical_accuracy: 0.7710 - val_loss: 0.4895 - val_categorical_accuracy: 0.7722\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.48896\n",
      "Epoch 33/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4876 - categorical_accuracy: 0.7715 - val_loss: 0.4910 - val_categorical_accuracy: 0.7708\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.48896\n",
      "Epoch 34/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 5s 59us/step - loss: 0.4878 - categorical_accuracy: 0.7725 - val_loss: 0.4888 - val_categorical_accuracy: 0.7722\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.48896 to 0.48885, saving model to best_model.h5\n",
      "Epoch 35/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4871 - categorical_accuracy: 0.7725 - val_loss: 0.4878 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.48885 to 0.48780, saving model to best_model.h5\n",
      "Epoch 36/250\n",
      "87011/87011 [==============================] - 6s 64us/step - loss: 0.4889 - categorical_accuracy: 0.7714 - val_loss: 0.4879 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.48780\n",
      "Epoch 37/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4866 - categorical_accuracy: 0.7727 - val_loss: 0.4892 - val_categorical_accuracy: 0.7725\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.48780\n",
      "Epoch 38/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4867 - categorical_accuracy: 0.7727 - val_loss: 0.4870 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.48780 to 0.48701, saving model to best_model.h5\n",
      "Epoch 39/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4860 - categorical_accuracy: 0.7736 - val_loss: 0.4872 - val_categorical_accuracy: 0.7731\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.48701\n",
      "Epoch 40/250\n",
      "87011/87011 [==============================] - 5s 63us/step - loss: 0.4852 - categorical_accuracy: 0.7739 - val_loss: 0.4918 - val_categorical_accuracy: 0.7725\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.48701\n",
      "Epoch 41/250\n",
      "87011/87011 [==============================] - 6s 64us/step - loss: 0.4870 - categorical_accuracy: 0.7725 - val_loss: 0.4880 - val_categorical_accuracy: 0.7725\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.48701\n",
      "Epoch 42/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4853 - categorical_accuracy: 0.7735 - val_loss: 0.4879 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.48701\n",
      "Epoch 43/250\n",
      "87011/87011 [==============================] - 6s 64us/step - loss: 0.4847 - categorical_accuracy: 0.7740 - val_loss: 0.4898 - val_categorical_accuracy: 0.7722\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.48701\n",
      "Epoch 44/250\n",
      "87011/87011 [==============================] - 6s 65us/step - loss: 0.4842 - categorical_accuracy: 0.7747 - val_loss: 0.4872 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.48701\n",
      "Epoch 45/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4850 - categorical_accuracy: 0.7742 - val_loss: 0.4876 - val_categorical_accuracy: 0.7728\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.48701\n",
      "Epoch 46/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4848 - categorical_accuracy: 0.7738 - val_loss: 0.4870 - val_categorical_accuracy: 0.7731\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.48701\n",
      "Epoch 47/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4841 - categorical_accuracy: 0.7743 - val_loss: 0.4858 - val_categorical_accuracy: 0.7734\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.48701 to 0.48582, saving model to best_model.h5\n",
      "Epoch 48/250\n",
      "87011/87011 [==============================] - 4s 48us/step - loss: 0.4844 - categorical_accuracy: 0.7743 - val_loss: 0.4846 - val_categorical_accuracy: 0.7740\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.48582 to 0.48455, saving model to best_model.h5\n",
      "Epoch 49/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4836 - categorical_accuracy: 0.7750 - val_loss: 0.4841 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.48455 to 0.48406, saving model to best_model.h5\n",
      "Epoch 50/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4835 - categorical_accuracy: 0.7752 - val_loss: 0.4856 - val_categorical_accuracy: 0.7738\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.48406\n",
      "Epoch 51/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4828 - categorical_accuracy: 0.7756 - val_loss: 0.4858 - val_categorical_accuracy: 0.7736\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.48406\n",
      "Epoch 52/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4839 - categorical_accuracy: 0.7745 - val_loss: 0.4855 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.48406\n",
      "Epoch 53/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4831 - categorical_accuracy: 0.7751 - val_loss: 0.4847 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.48406\n",
      "Epoch 54/250\n",
      "87011/87011 [==============================] - 2s 28us/step - loss: 0.4830 - categorical_accuracy: 0.7742 - val_loss: 0.4850 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.48406\n",
      "Epoch 55/250\n",
      "87011/87011 [==============================] - 6s 64us/step - loss: 0.4826 - categorical_accuracy: 0.7754 - val_loss: 0.4852 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.48406\n",
      "Epoch 56/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4830 - categorical_accuracy: 0.7745 - val_loss: 0.4843 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.48406\n",
      "Epoch 57/250\n",
      "87011/87011 [==============================] - 6s 70us/step - loss: 0.4825 - categorical_accuracy: 0.7754 - val_loss: 0.4846 - val_categorical_accuracy: 0.7739\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.48406\n",
      "Epoch 58/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.4820 - categorical_accuracy: 0.7752 - val_loss: 0.4845 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.48406\n",
      "Epoch 59/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4824 - categorical_accuracy: 0.7758 - val_loss: 0.4841 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.48406\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.00900000035762787.\n",
      "Epoch 60/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4818 - categorical_accuracy: 0.7747 - val_loss: 0.4842 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.48406\n",
      "Epoch 61/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4813 - categorical_accuracy: 0.7745 - val_loss: 0.4836 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.48406 to 0.48360, saving model to best_model.h5\n",
      "Epoch 62/250\n",
      "87011/87011 [==============================] - 6s 70us/step - loss: 0.4810 - categorical_accuracy: 0.7756 - val_loss: 0.4837 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.48360\n",
      "Epoch 63/250\n",
      "87011/87011 [==============================] - 6s 65us/step - loss: 0.4809 - categorical_accuracy: 0.7769 - val_loss: 0.4833 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.48360 to 0.48329, saving model to best_model.h5\n",
      "Epoch 64/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.4806 - categorical_accuracy: 0.7760 - val_loss: 0.4835 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.48329\n",
      "Epoch 65/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4811 - categorical_accuracy: 0.7754 - val_loss: 0.4835 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.48329\n",
      "Epoch 66/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4809 - categorical_accuracy: 0.7763 - val_loss: 0.4836 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.48329\n",
      "Epoch 67/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.4808 - categorical_accuracy: 0.7760 - val_loss: 0.4831 - val_categorical_accuracy: 0.7740\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.48329 to 0.48309, saving model to best_model.h5\n",
      "Epoch 68/250\n",
      "87011/87011 [==============================] - 6s 70us/step - loss: 0.4808 - categorical_accuracy: 0.7762 - val_loss: 0.4832 - val_categorical_accuracy: 0.7747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00068: val_loss did not improve from 0.48309\n",
      "Epoch 69/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4816 - categorical_accuracy: 0.7754 - val_loss: 0.4834 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.48309\n",
      "Epoch 70/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.4806 - categorical_accuracy: 0.7774 - val_loss: 0.4833 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.48309\n",
      "Epoch 71/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.4807 - categorical_accuracy: 0.7764 - val_loss: 0.4832 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.48309\n",
      "Epoch 72/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4808 - categorical_accuracy: 0.7766 - val_loss: 0.4835 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.48309\n",
      "Epoch 73/250\n",
      "87011/87011 [==============================] - 6s 72us/step - loss: 0.4813 - categorical_accuracy: 0.7758 - val_loss: 0.4829 - val_categorical_accuracy: 0.7737\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.48309 to 0.48290, saving model to best_model.h5\n",
      "Epoch 74/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4812 - categorical_accuracy: 0.7766 - val_loss: 0.4831 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.48290\n",
      "Epoch 75/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4804 - categorical_accuracy: 0.7757 - val_loss: 0.4831 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.48290\n",
      "Epoch 76/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.4803 - categorical_accuracy: 0.7761 - val_loss: 0.4833 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.48290\n",
      "Epoch 77/250\n",
      "87011/87011 [==============================] - 6s 65us/step - loss: 0.4806 - categorical_accuracy: 0.7760 - val_loss: 0.4833 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.48290\n",
      "Epoch 78/250\n",
      "87011/87011 [==============================] - 6s 71us/step - loss: 0.4805 - categorical_accuracy: 0.7757 - val_loss: 0.4830 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.48290\n",
      "Epoch 79/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.4810 - categorical_accuracy: 0.7756 - val_loss: 0.4834 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.48290\n",
      "Epoch 80/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4808 - categorical_accuracy: 0.7761 - val_loss: 0.4834 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.48290\n",
      "Epoch 81/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4813 - categorical_accuracy: 0.7761 - val_loss: 0.4834 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.48290\n",
      "Epoch 82/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4806 - categorical_accuracy: 0.7760 - val_loss: 0.4832 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.48290\n",
      "Epoch 83/250\n",
      "87011/87011 [==============================] - 6s 65us/step - loss: 0.4802 - categorical_accuracy: 0.7762 - val_loss: 0.4830 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.48290\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 0.0009000000543892384.\n",
      "Epoch 84/250\n",
      "87011/87011 [==============================] - 6s 65us/step - loss: 0.4810 - categorical_accuracy: 0.7756 - val_loss: 0.4830 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.48290\n",
      "Epoch 85/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4804 - categorical_accuracy: 0.7766 - val_loss: 0.4830 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.48290\n",
      "Epoch 86/250\n",
      "87011/87011 [==============================] - 6s 65us/step - loss: 0.4808 - categorical_accuracy: 0.7760 - val_loss: 0.4831 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.48290\n",
      "Epoch 87/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4809 - categorical_accuracy: 0.7758 - val_loss: 0.4830 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.48290\n",
      "Epoch 88/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.4808 - categorical_accuracy: 0.7764 - val_loss: 0.4830 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.48290\n",
      "Epoch 89/250\n",
      "87011/87011 [==============================] - 5s 62us/step - loss: 0.4802 - categorical_accuracy: 0.7770 - val_loss: 0.4831 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.48290\n",
      "Epoch 90/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4806 - categorical_accuracy: 0.7767 - val_loss: 0.4831 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.48290\n",
      "Epoch 91/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4804 - categorical_accuracy: 0.7763 - val_loss: 0.4831 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.48290\n",
      "Epoch 92/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4805 - categorical_accuracy: 0.7768 - val_loss: 0.4831 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.48290\n",
      "Epoch 93/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4804 - categorical_accuracy: 0.7770 - val_loss: 0.4831 - val_categorical_accuracy: 0.7743\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.48290\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 00093: early stopping\n",
      "the 1 fold Log-Loss (NN) is 0.482896\n",
      "-----------\n",
      "Loop 1/2 Fold 2/5\n",
      "-----------\n",
      "Train on 87011 samples, validate on 21753 samples\n",
      "Epoch 1/250\n",
      "87011/87011 [==============================] - 7s 80us/step - loss: 0.8651 - categorical_accuracy: 0.7194 - val_loss: 1.6497 - val_categorical_accuracy: 0.3687\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.64971, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.5196 - categorical_accuracy: 0.7549 - val_loss: 0.5854 - val_categorical_accuracy: 0.6723\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.64971 to 0.58544, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.5122 - categorical_accuracy: 0.7557 - val_loss: 0.5157 - val_categorical_accuracy: 0.7448\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.58544 to 0.51573, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.5107 - categorical_accuracy: 0.7560 - val_loss: 0.5352 - val_categorical_accuracy: 0.7582\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51573\n",
      "Epoch 5/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.5063 - categorical_accuracy: 0.7592 - val_loss: 0.5001 - val_categorical_accuracy: 0.7644\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51573 to 0.50014, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.5018 - categorical_accuracy: 0.7602 - val_loss: 0.4957 - val_categorical_accuracy: 0.7660\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.50014 to 0.49567, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "87011/87011 [==============================] - 6s 65us/step - loss: 0.5003 - categorical_accuracy: 0.7620 - val_loss: 0.4966 - val_categorical_accuracy: 0.7646\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.49567\n",
      "Epoch 8/250\n",
      "87011/87011 [==============================] - 6s 65us/step - loss: 0.4980 - categorical_accuracy: 0.7641 - val_loss: 0.4979 - val_categorical_accuracy: 0.7621\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.49567\n",
      "Epoch 9/250\n",
      "87011/87011 [==============================] - 6s 65us/step - loss: 0.4975 - categorical_accuracy: 0.7647 - val_loss: 0.4945 - val_categorical_accuracy: 0.7684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_loss improved from 0.49567 to 0.49452, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4963 - categorical_accuracy: 0.7658 - val_loss: 0.4921 - val_categorical_accuracy: 0.7695\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.49452 to 0.49210, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4954 - categorical_accuracy: 0.7666 - val_loss: 0.4905 - val_categorical_accuracy: 0.7706\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.49210 to 0.49046, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4948 - categorical_accuracy: 0.7676 - val_loss: 0.4898 - val_categorical_accuracy: 0.7691\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.49046 to 0.48983, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "87011/87011 [==============================] - 6s 71us/step - loss: 0.4931 - categorical_accuracy: 0.7680 - val_loss: 0.4900 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.48983\n",
      "Epoch 14/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.4936 - categorical_accuracy: 0.7674 - val_loss: 0.4916 - val_categorical_accuracy: 0.7702\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.48983\n",
      "Epoch 15/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.4926 - categorical_accuracy: 0.7683 - val_loss: 0.4883 - val_categorical_accuracy: 0.7706\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.48983 to 0.48834, saving model to best_model.h5\n",
      "Epoch 16/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.4916 - categorical_accuracy: 0.7701 - val_loss: 0.4902 - val_categorical_accuracy: 0.7705\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.48834\n",
      "Epoch 17/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4922 - categorical_accuracy: 0.7694 - val_loss: 0.4889 - val_categorical_accuracy: 0.7705\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.48834\n",
      "Epoch 18/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4916 - categorical_accuracy: 0.7697 - val_loss: 0.4880 - val_categorical_accuracy: 0.7707\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.48834 to 0.48797, saving model to best_model.h5\n",
      "Epoch 19/250\n",
      "87011/87011 [==============================] - 6s 65us/step - loss: 0.4911 - categorical_accuracy: 0.7688 - val_loss: 0.4877 - val_categorical_accuracy: 0.7717\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.48797 to 0.48772, saving model to best_model.h5\n",
      "Epoch 20/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4905 - categorical_accuracy: 0.7699 - val_loss: 0.4862 - val_categorical_accuracy: 0.7718\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.48772 to 0.48622, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4892 - categorical_accuracy: 0.7704 - val_loss: 0.4857 - val_categorical_accuracy: 0.7737\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.48622 to 0.48568, saving model to best_model.h5\n",
      "Epoch 22/250\n",
      "87011/87011 [==============================] - 5s 63us/step - loss: 0.4905 - categorical_accuracy: 0.7710 - val_loss: 0.4878 - val_categorical_accuracy: 0.7709\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.48568\n",
      "Epoch 23/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4882 - categorical_accuracy: 0.7718 - val_loss: 0.4863 - val_categorical_accuracy: 0.7720\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.48568\n",
      "Epoch 24/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4884 - categorical_accuracy: 0.7712 - val_loss: 0.4858 - val_categorical_accuracy: 0.7726\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48568\n",
      "Epoch 25/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4889 - categorical_accuracy: 0.7709 - val_loss: 0.4857 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.48568\n",
      "Epoch 26/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4878 - categorical_accuracy: 0.7724 - val_loss: 0.4900 - val_categorical_accuracy: 0.7718\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.48568\n",
      "Epoch 27/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4882 - categorical_accuracy: 0.7722 - val_loss: 0.4850 - val_categorical_accuracy: 0.7730\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.48568 to 0.48501, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4872 - categorical_accuracy: 0.7714 - val_loss: 0.4839 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.48501 to 0.48390, saving model to best_model.h5\n",
      "Epoch 29/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4874 - categorical_accuracy: 0.7716 - val_loss: 0.4837 - val_categorical_accuracy: 0.7740\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.48390 to 0.48368, saving model to best_model.h5\n",
      "Epoch 30/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4862 - categorical_accuracy: 0.7730 - val_loss: 0.4845 - val_categorical_accuracy: 0.7738\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.48368\n",
      "Epoch 31/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4862 - categorical_accuracy: 0.7724 - val_loss: 0.4842 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.48368\n",
      "Epoch 32/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4859 - categorical_accuracy: 0.7731 - val_loss: 0.4840 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.48368\n",
      "Epoch 33/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4866 - categorical_accuracy: 0.7735 - val_loss: 0.4864 - val_categorical_accuracy: 0.7729\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.48368\n",
      "Epoch 34/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4853 - categorical_accuracy: 0.7728 - val_loss: 0.4835 - val_categorical_accuracy: 0.7728\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.48368 to 0.48347, saving model to best_model.h5\n",
      "Epoch 35/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4853 - categorical_accuracy: 0.7739 - val_loss: 0.4833 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.48347 to 0.48328, saving model to best_model.h5\n",
      "Epoch 36/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4858 - categorical_accuracy: 0.7730 - val_loss: 0.4841 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.48328\n",
      "Epoch 37/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4855 - categorical_accuracy: 0.7737 - val_loss: 0.4832 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.48328 to 0.48320, saving model to best_model.h5\n",
      "Epoch 38/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4844 - categorical_accuracy: 0.7738 - val_loss: 0.4842 - val_categorical_accuracy: 0.7721\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.48320\n",
      "Epoch 39/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4839 - categorical_accuracy: 0.7739 - val_loss: 0.4841 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.48320\n",
      "Epoch 40/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4839 - categorical_accuracy: 0.7742 - val_loss: 0.4849 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.48320\n",
      "Epoch 41/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4842 - categorical_accuracy: 0.7745 - val_loss: 0.4878 - val_categorical_accuracy: 0.7719\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.48320\n",
      "Epoch 42/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4836 - categorical_accuracy: 0.7743 - val_loss: 0.4910 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.48320\n",
      "Epoch 43/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4841 - categorical_accuracy: 0.7740 - val_loss: 0.4834 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.48320\n",
      "Epoch 44/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4834 - categorical_accuracy: 0.7744 - val_loss: 0.4838 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.48320\n",
      "Epoch 45/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4837 - categorical_accuracy: 0.7741 - val_loss: 0.4819 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.48320 to 0.48194, saving model to best_model.h5\n",
      "Epoch 46/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4832 - categorical_accuracy: 0.7745 - val_loss: 0.4821 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.48194\n",
      "Epoch 47/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4829 - categorical_accuracy: 0.7747 - val_loss: 0.4826 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.48194\n",
      "Epoch 48/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4829 - categorical_accuracy: 0.7758 - val_loss: 0.4845 - val_categorical_accuracy: 0.7736\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.48194\n",
      "Epoch 49/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4827 - categorical_accuracy: 0.7749 - val_loss: 0.4828 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.48194\n",
      "Epoch 50/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4821 - categorical_accuracy: 0.7751 - val_loss: 0.4825 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.48194\n",
      "Epoch 51/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4817 - categorical_accuracy: 0.7750 - val_loss: 0.4855 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.48194\n",
      "Epoch 52/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4831 - categorical_accuracy: 0.7747 - val_loss: 0.4811 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.48194 to 0.48113, saving model to best_model.h5\n",
      "Epoch 53/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4824 - categorical_accuracy: 0.7751 - val_loss: 0.4811 - val_categorical_accuracy: 0.7740\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.48113 to 0.48110, saving model to best_model.h5\n",
      "Epoch 54/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4812 - categorical_accuracy: 0.7755 - val_loss: 0.4810 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.48110 to 0.48101, saving model to best_model.h5\n",
      "Epoch 55/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4819 - categorical_accuracy: 0.7751 - val_loss: 0.4815 - val_categorical_accuracy: 0.7732\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.48101\n",
      "Epoch 56/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4812 - categorical_accuracy: 0.7753 - val_loss: 0.4806 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.48101 to 0.48064, saving model to best_model.h5\n",
      "Epoch 57/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4815 - categorical_accuracy: 0.7755 - val_loss: 0.4804 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.48064 to 0.48044, saving model to best_model.h5\n",
      "Epoch 58/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4812 - categorical_accuracy: 0.7759 - val_loss: 0.4803 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.48044 to 0.48030, saving model to best_model.h5\n",
      "Epoch 59/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4808 - categorical_accuracy: 0.7761 - val_loss: 0.4798 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.48030 to 0.47977, saving model to best_model.h5\n",
      "Epoch 60/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4806 - categorical_accuracy: 0.7768 - val_loss: 0.4798 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47977\n",
      "Epoch 61/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4799 - categorical_accuracy: 0.7766 - val_loss: 0.4813 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47977\n",
      "Epoch 62/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4800 - categorical_accuracy: 0.7759 - val_loss: 0.4801 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47977\n",
      "Epoch 63/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4803 - categorical_accuracy: 0.7758 - val_loss: 0.4807 - val_categorical_accuracy: 0.7736\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47977\n",
      "Epoch 64/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4794 - categorical_accuracy: 0.7758 - val_loss: 0.4836 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47977\n",
      "Epoch 65/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4801 - categorical_accuracy: 0.7757 - val_loss: 0.4812 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47977\n",
      "Epoch 66/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4792 - categorical_accuracy: 0.7763 - val_loss: 0.4805 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47977\n",
      "Epoch 67/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4799 - categorical_accuracy: 0.7754 - val_loss: 0.4805 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.47977\n",
      "Epoch 68/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4791 - categorical_accuracy: 0.7770 - val_loss: 0.4798 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.47977\n",
      "Epoch 69/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4796 - categorical_accuracy: 0.7770 - val_loss: 0.4806 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47977\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.00900000035762787.\n",
      "Epoch 70/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4789 - categorical_accuracy: 0.7755 - val_loss: 0.4791 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.47977 to 0.47914, saving model to best_model.h5\n",
      "Epoch 71/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4787 - categorical_accuracy: 0.7769 - val_loss: 0.4797 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47914\n",
      "Epoch 72/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4788 - categorical_accuracy: 0.7770 - val_loss: 0.4790 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.47914 to 0.47897, saving model to best_model.h5\n",
      "Epoch 73/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4783 - categorical_accuracy: 0.7764 - val_loss: 0.4790 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47897\n",
      "Epoch 74/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4782 - categorical_accuracy: 0.7773 - val_loss: 0.4788 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.47897 to 0.47885, saving model to best_model.h5\n",
      "Epoch 75/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4779 - categorical_accuracy: 0.7776 - val_loss: 0.4791 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.47885\n",
      "Epoch 76/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4779 - categorical_accuracy: 0.7766 - val_loss: 0.4788 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.47885 to 0.47881, saving model to best_model.h5\n",
      "Epoch 77/250\n",
      "87011/87011 [==============================] - 2s 28us/step - loss: 0.4779 - categorical_accuracy: 0.7780 - val_loss: 0.4789 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.47881\n",
      "Epoch 78/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 28us/step - loss: 0.4782 - categorical_accuracy: 0.7771 - val_loss: 0.4790 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.47881\n",
      "Epoch 79/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4779 - categorical_accuracy: 0.7777 - val_loss: 0.4792 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.47881\n",
      "Epoch 80/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4779 - categorical_accuracy: 0.7772 - val_loss: 0.4792 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.47881\n",
      "Epoch 81/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4778 - categorical_accuracy: 0.7767 - val_loss: 0.4794 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.47881\n",
      "Epoch 82/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4784 - categorical_accuracy: 0.7770 - val_loss: 0.4790 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.47881\n",
      "Epoch 83/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4784 - categorical_accuracy: 0.7775 - val_loss: 0.4789 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.47881\n",
      "Epoch 84/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4778 - categorical_accuracy: 0.7769 - val_loss: 0.4793 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.47881\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.0009000000543892384.\n",
      "Epoch 85/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4785 - categorical_accuracy: 0.7764 - val_loss: 0.4792 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.47881\n",
      "Epoch 86/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4779 - categorical_accuracy: 0.7772 - val_loss: 0.4792 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.47881\n",
      "Epoch 87/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4784 - categorical_accuracy: 0.7769 - val_loss: 0.4788 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.47881 to 0.47880, saving model to best_model.h5\n",
      "Epoch 88/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4778 - categorical_accuracy: 0.7762 - val_loss: 0.4789 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.47880\n",
      "Epoch 89/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7771 - val_loss: 0.4790 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.47880\n",
      "Epoch 90/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4780 - categorical_accuracy: 0.7770 - val_loss: 0.4791 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.47880\n",
      "Epoch 91/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4778 - categorical_accuracy: 0.7769 - val_loss: 0.4788 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.47880 to 0.47878, saving model to best_model.h5\n",
      "Epoch 92/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4779 - categorical_accuracy: 0.7770 - val_loss: 0.4789 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.47878\n",
      "Epoch 93/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7770 - val_loss: 0.4789 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.47878\n",
      "Epoch 94/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7769 - val_loss: 0.4790 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.47878\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 95/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7776 - val_loss: 0.4792 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.47878\n",
      "Epoch 96/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7769 - val_loss: 0.4795 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.47878\n",
      "Epoch 97/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4778 - categorical_accuracy: 0.7771 - val_loss: 0.4793 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.47878\n",
      "Epoch 98/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4775 - categorical_accuracy: 0.7770 - val_loss: 0.4790 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.47878\n",
      "Epoch 99/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7775 - val_loss: 0.4789 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.47878\n",
      "Epoch 100/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4790 - categorical_accuracy: 0.7771 - val_loss: 0.4788 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.47878\n",
      "Epoch 101/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7767 - val_loss: 0.4788 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.47878\n",
      "Epoch 102/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4774 - categorical_accuracy: 0.7775 - val_loss: 0.4789 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.47878\n",
      "Epoch 103/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4783 - categorical_accuracy: 0.7767 - val_loss: 0.4791 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.47878\n",
      "Epoch 104/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4786 - categorical_accuracy: 0.7764 - val_loss: 0.4792 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.47878\n",
      "\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 9.000000136438758e-06.\n",
      "Epoch 105/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4780 - categorical_accuracy: 0.7773 - val_loss: 0.4794 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.47878\n",
      "Epoch 106/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4782 - categorical_accuracy: 0.7771 - val_loss: 0.4794 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.47878\n",
      "Epoch 107/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4782 - categorical_accuracy: 0.7767 - val_loss: 0.4797 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.47878\n",
      "Epoch 108/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4782 - categorical_accuracy: 0.7769 - val_loss: 0.4788 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.47878\n",
      "Epoch 109/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7780 - val_loss: 0.4788 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.47878\n",
      "Epoch 110/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4775 - categorical_accuracy: 0.7777 - val_loss: 0.4788 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.47878\n",
      "Epoch 111/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4786 - categorical_accuracy: 0.7766 - val_loss: 0.4790 - val_categorical_accuracy: 0.7756\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.47878\n",
      "Epoch 00111: early stopping\n",
      "the 2 fold Log-Loss (NN) is 0.478778\n",
      "-----------\n",
      "Loop 1/2 Fold 3/5\n",
      "-----------\n",
      "Train on 87011 samples, validate on 21753 samples\n",
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 3s 33us/step - loss: 0.9407 - categorical_accuracy: 0.7212 - val_loss: 0.9603 - val_categorical_accuracy: 0.5090\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.96032, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5177 - categorical_accuracy: 0.7548 - val_loss: 0.5117 - val_categorical_accuracy: 0.7601\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.96032 to 0.51166, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5114 - categorical_accuracy: 0.7562 - val_loss: 0.4953 - val_categorical_accuracy: 0.7644\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.51166 to 0.49533, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5103 - categorical_accuracy: 0.7569 - val_loss: 0.4933 - val_categorical_accuracy: 0.7632\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49533 to 0.49334, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5044 - categorical_accuracy: 0.7602 - val_loss: 0.4956 - val_categorical_accuracy: 0.7601\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49334\n",
      "Epoch 6/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5073 - categorical_accuracy: 0.7590 - val_loss: 0.4943 - val_categorical_accuracy: 0.7605\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49334\n",
      "Epoch 7/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5014 - categorical_accuracy: 0.7638 - val_loss: 0.4912 - val_categorical_accuracy: 0.7714\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.49334 to 0.49119, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5006 - categorical_accuracy: 0.7622 - val_loss: 0.4897 - val_categorical_accuracy: 0.7677\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.49119 to 0.48967, saving model to best_model.h5\n",
      "Epoch 9/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4988 - categorical_accuracy: 0.7635 - val_loss: 0.4883 - val_categorical_accuracy: 0.7711\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.48967 to 0.48833, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4986 - categorical_accuracy: 0.7647 - val_loss: 0.4883 - val_categorical_accuracy: 0.7677\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.48833 to 0.48825, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4983 - categorical_accuracy: 0.7655 - val_loss: 0.4887 - val_categorical_accuracy: 0.7680\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48825\n",
      "Epoch 12/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4964 - categorical_accuracy: 0.7655 - val_loss: 0.4871 - val_categorical_accuracy: 0.7707\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.48825 to 0.48714, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4968 - categorical_accuracy: 0.7646 - val_loss: 0.4848 - val_categorical_accuracy: 0.7727\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.48714 to 0.48483, saving model to best_model.h5\n",
      "Epoch 14/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4961 - categorical_accuracy: 0.7663 - val_loss: 0.4866 - val_categorical_accuracy: 0.7716\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.48483\n",
      "Epoch 15/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4946 - categorical_accuracy: 0.7671 - val_loss: 0.4843 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.48483 to 0.48430, saving model to best_model.h5\n",
      "Epoch 16/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4935 - categorical_accuracy: 0.7687 - val_loss: 0.4854 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.48430\n",
      "Epoch 17/250\n",
      "87011/87011 [==============================] - 3s 38us/step - loss: 0.4939 - categorical_accuracy: 0.7666 - val_loss: 0.4835 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.48430 to 0.48351, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "87011/87011 [==============================] - 6s 69us/step - loss: 0.4931 - categorical_accuracy: 0.7683 - val_loss: 0.4842 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.48351\n",
      "Epoch 19/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4927 - categorical_accuracy: 0.7688 - val_loss: 0.4850 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.48351\n",
      "Epoch 20/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4918 - categorical_accuracy: 0.7696 - val_loss: 0.4847 - val_categorical_accuracy: 0.7737\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.48351\n",
      "Epoch 21/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4919 - categorical_accuracy: 0.7687 - val_loss: 0.4858 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.48351\n",
      "Epoch 22/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4912 - categorical_accuracy: 0.7695 - val_loss: 0.4829 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.48351 to 0.48289, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "87011/87011 [==============================] - 5s 62us/step - loss: 0.4910 - categorical_accuracy: 0.7688 - val_loss: 0.4833 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.48289\n",
      "Epoch 24/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4913 - categorical_accuracy: 0.7700 - val_loss: 0.4838 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48289\n",
      "Epoch 25/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4907 - categorical_accuracy: 0.7695 - val_loss: 0.4854 - val_categorical_accuracy: 0.7738\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.48289\n",
      "Epoch 26/250\n",
      "87011/87011 [==============================] - 5s 61us/step - loss: 0.4904 - categorical_accuracy: 0.7708 - val_loss: 0.4808 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.48289 to 0.48084, saving model to best_model.h5\n",
      "Epoch 27/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4884 - categorical_accuracy: 0.7723 - val_loss: 0.4800 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.48084 to 0.48003, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "87011/87011 [==============================] - 5s 63us/step - loss: 0.4892 - categorical_accuracy: 0.7718 - val_loss: 0.4816 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.48003\n",
      "Epoch 29/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4888 - categorical_accuracy: 0.7711 - val_loss: 0.4806 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.48003\n",
      "Epoch 30/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4885 - categorical_accuracy: 0.7715 - val_loss: 0.4811 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.48003\n",
      "Epoch 31/250\n",
      "87011/87011 [==============================] - 6s 63us/step - loss: 0.4881 - categorical_accuracy: 0.7722 - val_loss: 0.4827 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.48003\n",
      "Epoch 32/250\n",
      "87011/87011 [==============================] - 6s 65us/step - loss: 0.4880 - categorical_accuracy: 0.7725 - val_loss: 0.4842 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.48003\n",
      "Epoch 33/250\n",
      "87011/87011 [==============================] - 6s 65us/step - loss: 0.4873 - categorical_accuracy: 0.7721 - val_loss: 0.4811 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.48003\n",
      "Epoch 34/250\n",
      "87011/87011 [==============================] - 6s 67us/step - loss: 0.4870 - categorical_accuracy: 0.7725 - val_loss: 0.4808 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.48003\n",
      "Epoch 35/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 5s 60us/step - loss: 0.4867 - categorical_accuracy: 0.7723 - val_loss: 0.4782 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.48003 to 0.47821, saving model to best_model.h5\n",
      "Epoch 36/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4865 - categorical_accuracy: 0.7730 - val_loss: 0.4791 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47821\n",
      "Epoch 37/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4864 - categorical_accuracy: 0.7732 - val_loss: 0.4779 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.47821 to 0.47786, saving model to best_model.h5\n",
      "Epoch 38/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4863 - categorical_accuracy: 0.7733 - val_loss: 0.4781 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47786\n",
      "Epoch 39/250\n",
      "87011/87011 [==============================] - 6s 70us/step - loss: 0.4859 - categorical_accuracy: 0.7730 - val_loss: 0.4774 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.47786 to 0.47745, saving model to best_model.h5\n",
      "Epoch 40/250\n",
      "87011/87011 [==============================] - 6s 65us/step - loss: 0.4857 - categorical_accuracy: 0.7734 - val_loss: 0.4836 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.47745\n",
      "Epoch 41/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4852 - categorical_accuracy: 0.7736 - val_loss: 0.4775 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47745\n",
      "Epoch 42/250\n",
      "87011/87011 [==============================] - 6s 66us/step - loss: 0.4852 - categorical_accuracy: 0.7740 - val_loss: 0.4793 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47745\n",
      "Epoch 43/250\n",
      "87011/87011 [==============================] - 6s 68us/step - loss: 0.4846 - categorical_accuracy: 0.7728 - val_loss: 0.4781 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47745\n",
      "Epoch 44/250\n",
      "87011/87011 [==============================] - 4s 44us/step - loss: 0.4842 - categorical_accuracy: 0.7742 - val_loss: 0.4771 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.47745 to 0.47708, saving model to best_model.h5\n",
      "Epoch 45/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4839 - categorical_accuracy: 0.7742 - val_loss: 0.4770 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.47708 to 0.47698, saving model to best_model.h5\n",
      "Epoch 46/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4846 - categorical_accuracy: 0.7747 - val_loss: 0.4770 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.47698 to 0.47697, saving model to best_model.h5\n",
      "Epoch 47/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4837 - categorical_accuracy: 0.7744 - val_loss: 0.4777 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47697\n",
      "Epoch 48/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4840 - categorical_accuracy: 0.7748 - val_loss: 0.4769 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.47697 to 0.47689, saving model to best_model.h5\n",
      "Epoch 49/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4841 - categorical_accuracy: 0.7744 - val_loss: 0.4781 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47689\n",
      "Epoch 50/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4837 - categorical_accuracy: 0.7749 - val_loss: 0.4810 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47689\n",
      "Epoch 51/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4842 - categorical_accuracy: 0.7751 - val_loss: 0.4768 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.47689 to 0.47676, saving model to best_model.h5\n",
      "Epoch 52/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4834 - categorical_accuracy: 0.7752 - val_loss: 0.4768 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47676\n",
      "Epoch 53/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4835 - categorical_accuracy: 0.7744 - val_loss: 0.4760 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.47676 to 0.47597, saving model to best_model.h5\n",
      "Epoch 54/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4821 - categorical_accuracy: 0.7755 - val_loss: 0.4764 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47597\n",
      "Epoch 55/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4824 - categorical_accuracy: 0.7745 - val_loss: 0.4765 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47597\n",
      "Epoch 56/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4831 - categorical_accuracy: 0.7751 - val_loss: 0.4777 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47597\n",
      "Epoch 57/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4822 - categorical_accuracy: 0.7749 - val_loss: 0.4765 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47597\n",
      "Epoch 58/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4817 - categorical_accuracy: 0.7746 - val_loss: 0.4773 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47597\n",
      "Epoch 59/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4824 - categorical_accuracy: 0.7748 - val_loss: 0.4765 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47597\n",
      "Epoch 60/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4822 - categorical_accuracy: 0.7756 - val_loss: 0.4753 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.47597 to 0.47526, saving model to best_model.h5\n",
      "Epoch 61/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4820 - categorical_accuracy: 0.7760 - val_loss: 0.4755 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47526\n",
      "Epoch 62/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4821 - categorical_accuracy: 0.7764 - val_loss: 0.4770 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47526\n",
      "Epoch 63/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4826 - categorical_accuracy: 0.7751 - val_loss: 0.4758 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47526\n",
      "Epoch 64/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4814 - categorical_accuracy: 0.7758 - val_loss: 0.4761 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47526\n",
      "Epoch 65/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4816 - categorical_accuracy: 0.7749 - val_loss: 0.4763 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47526\n",
      "Epoch 66/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4812 - categorical_accuracy: 0.7758 - val_loss: 0.4753 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47526\n",
      "Epoch 67/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4799 - categorical_accuracy: 0.7774 - val_loss: 0.4766 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.47526\n",
      "Epoch 68/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4807 - categorical_accuracy: 0.7769 - val_loss: 0.4748 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.47526 to 0.47484, saving model to best_model.h5\n",
      "Epoch 69/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4808 - categorical_accuracy: 0.7765 - val_loss: 0.4762 - val_categorical_accuracy: 0.7780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47484\n",
      "Epoch 70/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4804 - categorical_accuracy: 0.7767 - val_loss: 0.4754 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47484\n",
      "Epoch 71/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4804 - categorical_accuracy: 0.7766 - val_loss: 0.4752 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47484\n",
      "Epoch 72/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4801 - categorical_accuracy: 0.7767 - val_loss: 0.4757 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.47484\n",
      "Epoch 73/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4801 - categorical_accuracy: 0.7762 - val_loss: 0.4753 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47484\n",
      "Epoch 74/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4798 - categorical_accuracy: 0.7768 - val_loss: 0.4746 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.47484 to 0.47461, saving model to best_model.h5\n",
      "Epoch 75/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4796 - categorical_accuracy: 0.7770 - val_loss: 0.4754 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.47461\n",
      "Epoch 76/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4787 - categorical_accuracy: 0.7778 - val_loss: 0.4752 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.47461\n",
      "Epoch 77/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4794 - categorical_accuracy: 0.7764 - val_loss: 0.4767 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.47461\n",
      "Epoch 78/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4794 - categorical_accuracy: 0.7766 - val_loss: 0.4745 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.47461 to 0.47449, saving model to best_model.h5\n",
      "Epoch 79/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4800 - categorical_accuracy: 0.7763 - val_loss: 0.4745 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.47449 to 0.47446, saving model to best_model.h5\n",
      "Epoch 80/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4798 - categorical_accuracy: 0.7766 - val_loss: 0.4748 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.47446\n",
      "Epoch 81/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4795 - categorical_accuracy: 0.7761 - val_loss: 0.4750 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.47446\n",
      "Epoch 82/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4799 - categorical_accuracy: 0.7771 - val_loss: 0.4748 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.47446\n",
      "Epoch 83/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4797 - categorical_accuracy: 0.7762 - val_loss: 0.4747 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.47446\n",
      "Epoch 84/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4783 - categorical_accuracy: 0.7774 - val_loss: 0.4740 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.47446 to 0.47402, saving model to best_model.h5\n",
      "Epoch 85/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4799 - categorical_accuracy: 0.7762 - val_loss: 0.4741 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.47402\n",
      "Epoch 86/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4787 - categorical_accuracy: 0.7755 - val_loss: 0.4753 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.47402\n",
      "Epoch 87/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4791 - categorical_accuracy: 0.7773 - val_loss: 0.4758 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.47402\n",
      "Epoch 88/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4795 - categorical_accuracy: 0.7761 - val_loss: 0.4740 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.47402 to 0.47396, saving model to best_model.h5\n",
      "Epoch 89/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4785 - categorical_accuracy: 0.7766 - val_loss: 0.4761 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.47396\n",
      "Epoch 90/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4787 - categorical_accuracy: 0.7779 - val_loss: 0.4742 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.47396\n",
      "Epoch 91/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4785 - categorical_accuracy: 0.7773 - val_loss: 0.4740 - val_categorical_accuracy: 0.7805\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.47396\n",
      "Epoch 92/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4784 - categorical_accuracy: 0.7783 - val_loss: 0.4744 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.47396\n",
      "Epoch 93/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4789 - categorical_accuracy: 0.7770 - val_loss: 0.4750 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.47396\n",
      "Epoch 94/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7782 - val_loss: 0.4744 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.47396\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 0.00900000035762787.\n",
      "Epoch 95/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7778 - val_loss: 0.4745 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.47396\n",
      "Epoch 96/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4772 - categorical_accuracy: 0.7778 - val_loss: 0.4741 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.47396\n",
      "Epoch 97/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4787 - categorical_accuracy: 0.7772 - val_loss: 0.4741 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.47396\n",
      "Epoch 98/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7772 - val_loss: 0.4740 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.47396\n",
      "Epoch 99/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4785 - categorical_accuracy: 0.7781 - val_loss: 0.4741 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.47396\n",
      "Epoch 100/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4783 - categorical_accuracy: 0.7774 - val_loss: 0.4741 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.47396\n",
      "Epoch 101/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4773 - categorical_accuracy: 0.7775 - val_loss: 0.4740 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.47396\n",
      "Epoch 102/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4769 - categorical_accuracy: 0.7787 - val_loss: 0.4740 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.47396\n",
      "Epoch 103/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7779 - val_loss: 0.4741 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.47396\n",
      "Epoch 104/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4773 - categorical_accuracy: 0.7780 - val_loss: 0.4740 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.47396\n",
      "\n",
      "Epoch 00104: ReduceLROnPlateau reducing learning rate to 0.0009000000543892384.\n",
      "Epoch 105/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4771 - categorical_accuracy: 0.7776 - val_loss: 0.4740 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.47396\n",
      "Epoch 106/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4772 - categorical_accuracy: 0.7781 - val_loss: 0.4741 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.47396\n",
      "Epoch 107/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7775 - val_loss: 0.4740 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.47396\n",
      "Epoch 108/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4772 - categorical_accuracy: 0.7774 - val_loss: 0.4740 - val_categorical_accuracy: 0.7794\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.47396\n",
      "Epoch 00108: early stopping\n",
      "the 3 fold Log-Loss (NN) is 0.473958\n",
      "-----------\n",
      "Loop 1/2 Fold 4/5\n",
      "-----------\n",
      "Train on 87011 samples, validate on 21753 samples\n",
      "Epoch 1/250\n",
      "87011/87011 [==============================] - 3s 33us/step - loss: 0.9650 - categorical_accuracy: 0.7198 - val_loss: 1.0701 - val_categorical_accuracy: 0.4389\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.07013, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5191 - categorical_accuracy: 0.7553 - val_loss: 0.5089 - val_categorical_accuracy: 0.7620\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.07013 to 0.50885, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5126 - categorical_accuracy: 0.7572 - val_loss: 0.4994 - val_categorical_accuracy: 0.7650\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.50885 to 0.49935, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5043 - categorical_accuracy: 0.7607 - val_loss: 0.4968 - val_categorical_accuracy: 0.7644\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49935 to 0.49678, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5041 - categorical_accuracy: 0.7619 - val_loss: 0.4945 - val_categorical_accuracy: 0.7669\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49678 to 0.49446, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5061 - categorical_accuracy: 0.7614 - val_loss: 0.4989 - val_categorical_accuracy: 0.7648\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49446\n",
      "Epoch 7/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4988 - categorical_accuracy: 0.7645 - val_loss: 0.4919 - val_categorical_accuracy: 0.7690\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.49446 to 0.49191, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4965 - categorical_accuracy: 0.7669 - val_loss: 0.5087 - val_categorical_accuracy: 0.7608\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.49191\n",
      "Epoch 9/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4969 - categorical_accuracy: 0.7653 - val_loss: 0.4943 - val_categorical_accuracy: 0.7704\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.49191\n",
      "Epoch 10/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4944 - categorical_accuracy: 0.7687 - val_loss: 0.4890 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.49191 to 0.48898, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4933 - categorical_accuracy: 0.7692 - val_loss: 0.4878 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.48898 to 0.48776, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4934 - categorical_accuracy: 0.7681 - val_loss: 0.4870 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.48776 to 0.48696, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4923 - categorical_accuracy: 0.7697 - val_loss: 0.4867 - val_categorical_accuracy: 0.7733\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.48696 to 0.48669, saving model to best_model.h5\n",
      "Epoch 14/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4920 - categorical_accuracy: 0.7693 - val_loss: 0.4881 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.48669\n",
      "Epoch 15/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4927 - categorical_accuracy: 0.7700 - val_loss: 0.4860 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.48669 to 0.48596, saving model to best_model.h5\n",
      "Epoch 16/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4910 - categorical_accuracy: 0.7702 - val_loss: 0.4875 - val_categorical_accuracy: 0.7739\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.48596\n",
      "Epoch 17/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4899 - categorical_accuracy: 0.7712 - val_loss: 0.4851 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.48596 to 0.48507, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4893 - categorical_accuracy: 0.7712 - val_loss: 0.4879 - val_categorical_accuracy: 0.7721\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.48507\n",
      "Epoch 19/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4892 - categorical_accuracy: 0.7719 - val_loss: 0.4840 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.48507 to 0.48402, saving model to best_model.h5\n",
      "Epoch 20/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4877 - categorical_accuracy: 0.7731 - val_loss: 0.4868 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.48402\n",
      "Epoch 21/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4880 - categorical_accuracy: 0.7728 - val_loss: 0.4846 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.48402\n",
      "Epoch 22/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4883 - categorical_accuracy: 0.7714 - val_loss: 0.4834 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.48402 to 0.48344, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4864 - categorical_accuracy: 0.7722 - val_loss: 0.4843 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.48344\n",
      "Epoch 24/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4866 - categorical_accuracy: 0.7729 - val_loss: 0.4848 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48344\n",
      "Epoch 25/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4870 - categorical_accuracy: 0.7732 - val_loss: 0.4831 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.48344 to 0.48315, saving model to best_model.h5\n",
      "Epoch 26/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4861 - categorical_accuracy: 0.7727 - val_loss: 0.4829 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.48315 to 0.48287, saving model to best_model.h5\n",
      "Epoch 27/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4857 - categorical_accuracy: 0.7733 - val_loss: 0.4826 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.48287 to 0.48257, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4857 - categorical_accuracy: 0.7730 - val_loss: 0.4823 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.48257 to 0.48225, saving model to best_model.h5\n",
      "Epoch 29/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4847 - categorical_accuracy: 0.7743 - val_loss: 0.4818 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.48225 to 0.48183, saving model to best_model.h5\n",
      "Epoch 30/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4849 - categorical_accuracy: 0.7745 - val_loss: 0.4810 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.48183 to 0.48099, saving model to best_model.h5\n",
      "Epoch 31/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4850 - categorical_accuracy: 0.7732 - val_loss: 0.4812 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.48099\n",
      "Epoch 32/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4848 - categorical_accuracy: 0.7739 - val_loss: 0.4835 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.48099\n",
      "Epoch 33/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4842 - categorical_accuracy: 0.7745 - val_loss: 0.4812 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.48099\n",
      "Epoch 34/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4836 - categorical_accuracy: 0.7743 - val_loss: 0.4807 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.48099 to 0.48073, saving model to best_model.h5\n",
      "Epoch 35/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4840 - categorical_accuracy: 0.7747 - val_loss: 0.4806 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.48073 to 0.48064, saving model to best_model.h5\n",
      "Epoch 36/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4842 - categorical_accuracy: 0.7743 - val_loss: 0.4799 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.48064 to 0.47994, saving model to best_model.h5\n",
      "Epoch 37/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4831 - categorical_accuracy: 0.7753 - val_loss: 0.4801 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47994\n",
      "Epoch 38/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4829 - categorical_accuracy: 0.7745 - val_loss: 0.4840 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47994\n",
      "Epoch 39/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4830 - categorical_accuracy: 0.7749 - val_loss: 0.4812 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.47994\n",
      "Epoch 40/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4831 - categorical_accuracy: 0.7747 - val_loss: 0.4800 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.47994\n",
      "Epoch 41/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4825 - categorical_accuracy: 0.7760 - val_loss: 0.4810 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47994\n",
      "Epoch 42/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4838 - categorical_accuracy: 0.7746 - val_loss: 0.4799 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.47994 to 0.47992, saving model to best_model.h5\n",
      "Epoch 43/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4821 - categorical_accuracy: 0.7747 - val_loss: 0.4803 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47992\n",
      "Epoch 44/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4820 - categorical_accuracy: 0.7758 - val_loss: 0.4800 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47992\n",
      "Epoch 45/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4819 - categorical_accuracy: 0.7761 - val_loss: 0.4794 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.47992 to 0.47944, saving model to best_model.h5\n",
      "Epoch 46/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4823 - categorical_accuracy: 0.7749 - val_loss: 0.4795 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47944\n",
      "Epoch 47/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4816 - categorical_accuracy: 0.7758 - val_loss: 0.4796 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47944\n",
      "Epoch 48/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4809 - categorical_accuracy: 0.7766 - val_loss: 0.4796 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47944\n",
      "Epoch 49/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4815 - categorical_accuracy: 0.7755 - val_loss: 0.4791 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.47944 to 0.47905, saving model to best_model.h5\n",
      "Epoch 50/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4821 - categorical_accuracy: 0.7761 - val_loss: 0.4790 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.47905 to 0.47895, saving model to best_model.h5\n",
      "Epoch 51/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4816 - categorical_accuracy: 0.7762 - val_loss: 0.4798 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47895\n",
      "Epoch 52/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4811 - categorical_accuracy: 0.7761 - val_loss: 0.4792 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47895\n",
      "Epoch 53/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4807 - categorical_accuracy: 0.7760 - val_loss: 0.4806 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47895\n",
      "Epoch 54/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4815 - categorical_accuracy: 0.7749 - val_loss: 0.4808 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47895\n",
      "Epoch 55/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4813 - categorical_accuracy: 0.7758 - val_loss: 0.4805 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47895\n",
      "Epoch 56/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4803 - categorical_accuracy: 0.7762 - val_loss: 0.4788 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.47895 to 0.47877, saving model to best_model.h5\n",
      "Epoch 57/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4805 - categorical_accuracy: 0.7759 - val_loss: 0.4786 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.47877 to 0.47865, saving model to best_model.h5\n",
      "Epoch 58/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4801 - categorical_accuracy: 0.7767 - val_loss: 0.4793 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47865\n",
      "Epoch 59/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4802 - categorical_accuracy: 0.7764 - val_loss: 0.4795 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47865\n",
      "Epoch 60/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4799 - categorical_accuracy: 0.7770 - val_loss: 0.4791 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47865\n",
      "Epoch 61/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4792 - categorical_accuracy: 0.7771 - val_loss: 0.4789 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47865\n",
      "Epoch 62/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4795 - categorical_accuracy: 0.7765 - val_loss: 0.4787 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47865\n",
      "Epoch 63/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4799 - categorical_accuracy: 0.7763 - val_loss: 0.4792 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47865\n",
      "Epoch 64/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4798 - categorical_accuracy: 0.7765 - val_loss: 0.4791 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47865\n",
      "Epoch 65/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4807 - categorical_accuracy: 0.7766 - val_loss: 0.4786 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.47865 to 0.47864, saving model to best_model.h5\n",
      "Epoch 66/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4803 - categorical_accuracy: 0.7771 - val_loss: 0.4799 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47864\n",
      "Epoch 67/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4794 - categorical_accuracy: 0.7766 - val_loss: 0.4805 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.47864\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.00900000035762787.\n",
      "Epoch 68/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4787 - categorical_accuracy: 0.7769 - val_loss: 0.4781 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.47864 to 0.47811, saving model to best_model.h5\n",
      "Epoch 69/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4779 - categorical_accuracy: 0.7780 - val_loss: 0.4781 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47811\n",
      "Epoch 70/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4783 - categorical_accuracy: 0.7781 - val_loss: 0.4782 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47811\n",
      "Epoch 71/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4773 - categorical_accuracy: 0.7773 - val_loss: 0.4781 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.47811 to 0.47809, saving model to best_model.h5\n",
      "Epoch 72/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4782 - categorical_accuracy: 0.7771 - val_loss: 0.4780 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.47809 to 0.47803, saving model to best_model.h5\n",
      "Epoch 73/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4782 - categorical_accuracy: 0.7773 - val_loss: 0.4782 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47803\n",
      "Epoch 74/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4782 - categorical_accuracy: 0.7773 - val_loss: 0.4780 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.47803 to 0.47796, saving model to best_model.h5\n",
      "Epoch 75/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4774 - categorical_accuracy: 0.7782 - val_loss: 0.4780 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.47796\n",
      "Epoch 76/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4786 - categorical_accuracy: 0.7778 - val_loss: 0.4780 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.47796\n",
      "Epoch 77/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4776 - categorical_accuracy: 0.7777 - val_loss: 0.4780 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.47796\n",
      "Epoch 78/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4779 - categorical_accuracy: 0.7770 - val_loss: 0.4781 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.47796\n",
      "Epoch 79/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4784 - categorical_accuracy: 0.7769 - val_loss: 0.4780 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.47796\n",
      "Epoch 80/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4773 - categorical_accuracy: 0.7771 - val_loss: 0.4780 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.47796\n",
      "Epoch 81/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7778 - val_loss: 0.4781 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.47796\n",
      "Epoch 82/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4779 - categorical_accuracy: 0.7784 - val_loss: 0.4781 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.47796\n",
      "Epoch 83/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4779 - categorical_accuracy: 0.7772 - val_loss: 0.4782 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.47796\n",
      "Epoch 84/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4784 - categorical_accuracy: 0.7768 - val_loss: 0.4781 - val_categorical_accuracy: 0.7774\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.47796\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.0009000000543892384.\n",
      "Epoch 85/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4777 - categorical_accuracy: 0.7773 - val_loss: 0.4781 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.47796\n",
      "Epoch 86/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4775 - categorical_accuracy: 0.7786 - val_loss: 0.4780 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.47796\n",
      "Epoch 87/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7776 - val_loss: 0.4780 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.47796\n",
      "Epoch 88/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4773 - categorical_accuracy: 0.7779 - val_loss: 0.4780 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.47796\n",
      "Epoch 89/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4778 - categorical_accuracy: 0.7782 - val_loss: 0.4780 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.47796\n",
      "Epoch 90/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4774 - categorical_accuracy: 0.7780 - val_loss: 0.4780 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.47796\n",
      "Epoch 91/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4774 - categorical_accuracy: 0.7785 - val_loss: 0.4780 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.47796\n",
      "Epoch 92/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4773 - categorical_accuracy: 0.7776 - val_loss: 0.4780 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.47796\n",
      "Epoch 93/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4774 - categorical_accuracy: 0.7768 - val_loss: 0.4780 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.47796\n",
      "Epoch 94/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4773 - categorical_accuracy: 0.7778 - val_loss: 0.4780 - val_categorical_accuracy: 0.7771\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.47796\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 00094: early stopping\n",
      "the 4 fold Log-Loss (NN) is 0.477957\n",
      "-----------\n",
      "Loop 1/2 Fold 5/5\n",
      "-----------\n",
      "Train on 87012 samples, validate on 21752 samples\n",
      "Epoch 1/250\n",
      "87012/87012 [==============================] - 3s 33us/step - loss: 0.9591 - categorical_accuracy: 0.7194 - val_loss: 1.7816 - val_categorical_accuracy: 0.3410\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.78162, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.5271 - categorical_accuracy: 0.7549 - val_loss: 0.5585 - val_categorical_accuracy: 0.7017\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.78162 to 0.55853, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.5125 - categorical_accuracy: 0.7578 - val_loss: 0.5234 - val_categorical_accuracy: 0.7576\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55853 to 0.52339, saving model to best_model.h5\n",
      "Epoch 4/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.5074 - categorical_accuracy: 0.7583 - val_loss: 0.5003 - val_categorical_accuracy: 0.7602\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52339 to 0.50027, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.5039 - categorical_accuracy: 0.7606 - val_loss: 0.5086 - val_categorical_accuracy: 0.7571\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.50027\n",
      "Epoch 6/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.5012 - categorical_accuracy: 0.7624 - val_loss: 0.5022 - val_categorical_accuracy: 0.7595\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.50027\n",
      "Epoch 7/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4978 - categorical_accuracy: 0.7644 - val_loss: 0.4980 - val_categorical_accuracy: 0.7621\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.50027 to 0.49804, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4983 - categorical_accuracy: 0.7640 - val_loss: 0.5038 - val_categorical_accuracy: 0.7608\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.49804\n",
      "Epoch 9/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4983 - categorical_accuracy: 0.7641 - val_loss: 0.5028 - val_categorical_accuracy: 0.7605\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.49804\n",
      "Epoch 10/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4968 - categorical_accuracy: 0.7649 - val_loss: 0.4933 - val_categorical_accuracy: 0.7688\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.49804 to 0.49325, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4951 - categorical_accuracy: 0.7670 - val_loss: 0.4922 - val_categorical_accuracy: 0.7695\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.49325 to 0.49221, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4945 - categorical_accuracy: 0.7664 - val_loss: 0.4965 - val_categorical_accuracy: 0.7662\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.49221\n",
      "Epoch 13/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4951 - categorical_accuracy: 0.7665 - val_loss: 0.4966 - val_categorical_accuracy: 0.7655\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49221\n",
      "Epoch 14/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4933 - categorical_accuracy: 0.7687 - val_loss: 0.4911 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.49221 to 0.49109, saving model to best_model.h5\n",
      "Epoch 15/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4927 - categorical_accuracy: 0.7676 - val_loss: 0.4944 - val_categorical_accuracy: 0.7677\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.49109\n",
      "Epoch 16/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4936 - categorical_accuracy: 0.7673 - val_loss: 0.4895 - val_categorical_accuracy: 0.7705\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.49109 to 0.48954, saving model to best_model.h5\n",
      "Epoch 17/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4907 - categorical_accuracy: 0.7703 - val_loss: 0.4914 - val_categorical_accuracy: 0.7697\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.48954\n",
      "Epoch 18/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4922 - categorical_accuracy: 0.7690 - val_loss: 0.4897 - val_categorical_accuracy: 0.7716\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.48954\n",
      "Epoch 19/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4900 - categorical_accuracy: 0.7700 - val_loss: 0.4883 - val_categorical_accuracy: 0.7719\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.48954 to 0.48826, saving model to best_model.h5\n",
      "Epoch 20/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4913 - categorical_accuracy: 0.7693 - val_loss: 0.4880 - val_categorical_accuracy: 0.7716\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.48826 to 0.48802, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4910 - categorical_accuracy: 0.7697 - val_loss: 0.4919 - val_categorical_accuracy: 0.7693\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.48802\n",
      "Epoch 22/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4900 - categorical_accuracy: 0.7702 - val_loss: 0.4870 - val_categorical_accuracy: 0.7728\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.48802 to 0.48703, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4901 - categorical_accuracy: 0.7702 - val_loss: 0.4881 - val_categorical_accuracy: 0.7713\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.48703\n",
      "Epoch 24/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4899 - categorical_accuracy: 0.7703 - val_loss: 0.4916 - val_categorical_accuracy: 0.7716\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48703\n",
      "Epoch 25/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4893 - categorical_accuracy: 0.7716 - val_loss: 0.4873 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.48703\n",
      "Epoch 26/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4880 - categorical_accuracy: 0.7719 - val_loss: 0.4880 - val_categorical_accuracy: 0.7719\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.48703\n",
      "Epoch 27/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4879 - categorical_accuracy: 0.7713 - val_loss: 0.4854 - val_categorical_accuracy: 0.7734\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.48703 to 0.48540, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4881 - categorical_accuracy: 0.7716 - val_loss: 0.4941 - val_categorical_accuracy: 0.7709\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.48540\n",
      "Epoch 29/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4879 - categorical_accuracy: 0.7719 - val_loss: 0.4854 - val_categorical_accuracy: 0.7737\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.48540 to 0.48537, saving model to best_model.h5\n",
      "Epoch 30/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4881 - categorical_accuracy: 0.7718 - val_loss: 0.4877 - val_categorical_accuracy: 0.7719\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.48537\n",
      "Epoch 31/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4860 - categorical_accuracy: 0.7736 - val_loss: 0.4853 - val_categorical_accuracy: 0.7740\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.48537 to 0.48526, saving model to best_model.h5\n",
      "Epoch 32/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4864 - categorical_accuracy: 0.7723 - val_loss: 0.4844 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.48526 to 0.48439, saving model to best_model.h5\n",
      "Epoch 33/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4855 - categorical_accuracy: 0.7737 - val_loss: 0.4871 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.48439\n",
      "Epoch 34/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4852 - categorical_accuracy: 0.7728 - val_loss: 0.4855 - val_categorical_accuracy: 0.7728\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.48439\n",
      "Epoch 35/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4854 - categorical_accuracy: 0.7737 - val_loss: 0.4840 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.48439 to 0.48400, saving model to best_model.h5\n",
      "Epoch 36/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4851 - categorical_accuracy: 0.7734 - val_loss: 0.4830 - val_categorical_accuracy: 0.7740\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.48400 to 0.48300, saving model to best_model.h5\n",
      "Epoch 37/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4841 - categorical_accuracy: 0.7741 - val_loss: 0.4857 - val_categorical_accuracy: 0.7725\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.48300\n",
      "Epoch 38/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4855 - categorical_accuracy: 0.7734 - val_loss: 0.4866 - val_categorical_accuracy: 0.7738\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.48300\n",
      "Epoch 39/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4844 - categorical_accuracy: 0.7741 - val_loss: 0.4851 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.48300\n",
      "Epoch 40/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4839 - categorical_accuracy: 0.7737 - val_loss: 0.4853 - val_categorical_accuracy: 0.7734\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.48300\n",
      "Epoch 41/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4851 - categorical_accuracy: 0.7744 - val_loss: 0.4847 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.48300\n",
      "Epoch 42/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4842 - categorical_accuracy: 0.7737 - val_loss: 0.4827 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.48300 to 0.48274, saving model to best_model.h5\n",
      "Epoch 43/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4834 - categorical_accuracy: 0.7749 - val_loss: 0.4827 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.48274 to 0.48267, saving model to best_model.h5\n",
      "Epoch 44/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4828 - categorical_accuracy: 0.7752 - val_loss: 0.4833 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.48267\n",
      "Epoch 45/250\n",
      "87012/87012 [==============================] - 4s 42us/step - loss: 0.4835 - categorical_accuracy: 0.7737 - val_loss: 0.4843 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.48267\n",
      "Epoch 46/250\n",
      "87012/87012 [==============================] - 6s 69us/step - loss: 0.4824 - categorical_accuracy: 0.7747 - val_loss: 0.4846 - val_categorical_accuracy: 0.7739\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.48267\n",
      "Epoch 47/250\n",
      "87012/87012 [==============================] - 6s 71us/step - loss: 0.4830 - categorical_accuracy: 0.7737 - val_loss: 0.4836 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.48267\n",
      "Epoch 48/250\n",
      "87012/87012 [==============================] - 6s 70us/step - loss: 0.4826 - categorical_accuracy: 0.7754 - val_loss: 0.4839 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.48267\n",
      "Epoch 49/250\n",
      "87012/87012 [==============================] - 6s 66us/step - loss: 0.4819 - categorical_accuracy: 0.7753 - val_loss: 0.4822 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.48267 to 0.48216, saving model to best_model.h5\n",
      "Epoch 50/250\n",
      "87012/87012 [==============================] - 6s 69us/step - loss: 0.4831 - categorical_accuracy: 0.7754 - val_loss: 0.4850 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.48216\n",
      "Epoch 51/250\n",
      "87012/87012 [==============================] - 6s 65us/step - loss: 0.4818 - categorical_accuracy: 0.7760 - val_loss: 0.4841 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.48216\n",
      "Epoch 52/250\n",
      "87012/87012 [==============================] - 6s 63us/step - loss: 0.4821 - categorical_accuracy: 0.7752 - val_loss: 0.4823 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.48216\n",
      "Epoch 53/250\n",
      "87012/87012 [==============================] - 6s 69us/step - loss: 0.4820 - categorical_accuracy: 0.7760 - val_loss: 0.4844 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.48216\n",
      "Epoch 54/250\n",
      "87012/87012 [==============================] - 6s 70us/step - loss: 0.4820 - categorical_accuracy: 0.7757 - val_loss: 0.4833 - val_categorical_accuracy: 0.7737\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.48216\n",
      "Epoch 55/250\n",
      "87012/87012 [==============================] - 6s 63us/step - loss: 0.4814 - categorical_accuracy: 0.7750 - val_loss: 0.4865 - val_categorical_accuracy: 0.7738\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.48216\n",
      "Epoch 56/250\n",
      "87012/87012 [==============================] - 6s 71us/step - loss: 0.4810 - categorical_accuracy: 0.7759 - val_loss: 0.4821 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.48216 to 0.48205, saving model to best_model.h5\n",
      "Epoch 57/250\n",
      "87012/87012 [==============================] - 5s 62us/step - loss: 0.4817 - categorical_accuracy: 0.7755 - val_loss: 0.4822 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.48205\n",
      "Epoch 58/250\n",
      "87012/87012 [==============================] - 6s 70us/step - loss: 0.4810 - categorical_accuracy: 0.7752 - val_loss: 0.4826 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.48205\n",
      "Epoch 59/250\n",
      "87012/87012 [==============================] - 6s 69us/step - loss: 0.4813 - categorical_accuracy: 0.7751 - val_loss: 0.4813 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.48205 to 0.48135, saving model to best_model.h5\n",
      "Epoch 60/250\n",
      "87012/87012 [==============================] - 6s 66us/step - loss: 0.4800 - categorical_accuracy: 0.7764 - val_loss: 0.4828 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.48135\n",
      "Epoch 61/250\n",
      "87012/87012 [==============================] - 6s 71us/step - loss: 0.4805 - categorical_accuracy: 0.7758 - val_loss: 0.4821 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.48135\n",
      "Epoch 62/250\n",
      "87012/87012 [==============================] - 6s 66us/step - loss: 0.4793 - categorical_accuracy: 0.7765 - val_loss: 0.4844 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.48135\n",
      "Epoch 63/250\n",
      "87012/87012 [==============================] - 6s 64us/step - loss: 0.4803 - categorical_accuracy: 0.7761 - val_loss: 0.4814 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.48135\n",
      "Epoch 64/250\n",
      "87012/87012 [==============================] - 6s 64us/step - loss: 0.4804 - categorical_accuracy: 0.7752 - val_loss: 0.4833 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.48135\n",
      "Epoch 65/250\n",
      "87012/87012 [==============================] - 6s 66us/step - loss: 0.4799 - categorical_accuracy: 0.7772 - val_loss: 0.4844 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.48135\n",
      "Epoch 66/250\n",
      "87012/87012 [==============================] - 6s 66us/step - loss: 0.4794 - categorical_accuracy: 0.7762 - val_loss: 0.4843 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.48135\n",
      "Epoch 67/250\n",
      "87012/87012 [==============================] - 6s 69us/step - loss: 0.4796 - categorical_accuracy: 0.7760 - val_loss: 0.4841 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.48135\n",
      "Epoch 68/250\n",
      "87012/87012 [==============================] - 6s 66us/step - loss: 0.4797 - categorical_accuracy: 0.7759 - val_loss: 0.4834 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.48135\n",
      "Epoch 69/250\n",
      "87012/87012 [==============================] - 5s 63us/step - loss: 0.4792 - categorical_accuracy: 0.7758 - val_loss: 0.4847 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.48135\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.00900000035762787.\n",
      "Epoch 70/250\n",
      "87012/87012 [==============================] - 6s 69us/step - loss: 0.4782 - categorical_accuracy: 0.7773 - val_loss: 0.4848 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.48135\n",
      "Epoch 71/250\n",
      "87012/87012 [==============================] - 5s 54us/step - loss: 0.4777 - categorical_accuracy: 0.7776 - val_loss: 0.4848 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.48135\n",
      "Epoch 72/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4786 - categorical_accuracy: 0.7765 - val_loss: 0.4814 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.48135\n",
      "Epoch 73/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4782 - categorical_accuracy: 0.7767 - val_loss: 0.4816 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.48135\n",
      "Epoch 74/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7771 - val_loss: 0.4812 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.48135 to 0.48125, saving model to best_model.h5\n",
      "Epoch 75/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4780 - categorical_accuracy: 0.7773 - val_loss: 0.4820 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.48125\n",
      "Epoch 76/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4786 - categorical_accuracy: 0.7766 - val_loss: 0.4823 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.48125\n",
      "Epoch 77/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4787 - categorical_accuracy: 0.7781 - val_loss: 0.4831 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.48125\n",
      "Epoch 78/250\n",
      "87012/87012 [==============================] - 5s 53us/step - loss: 0.4780 - categorical_accuracy: 0.7770 - val_loss: 0.4839 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.48125\n",
      "Epoch 79/250\n",
      "87012/87012 [==============================] - 6s 64us/step - loss: 0.4778 - categorical_accuracy: 0.7775 - val_loss: 0.4815 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.48125\n",
      "Epoch 80/250\n",
      "87012/87012 [==============================] - 6s 66us/step - loss: 0.4778 - categorical_accuracy: 0.7770 - val_loss: 0.4817 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.48125\n",
      "Epoch 81/250\n",
      "87012/87012 [==============================] - 6s 65us/step - loss: 0.4783 - categorical_accuracy: 0.7769 - val_loss: 0.4820 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.48125\n",
      "Epoch 82/250\n",
      "87012/87012 [==============================] - 6s 64us/step - loss: 0.4780 - categorical_accuracy: 0.7777 - val_loss: 0.4820 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.48125\n",
      "Epoch 83/250\n",
      "87012/87012 [==============================] - 6s 66us/step - loss: 0.4776 - categorical_accuracy: 0.7766 - val_loss: 0.4816 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.48125\n",
      "Epoch 84/250\n",
      "87012/87012 [==============================] - 6s 64us/step - loss: 0.4784 - categorical_accuracy: 0.7775 - val_loss: 0.4819 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.48125\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.0009000000543892384.\n",
      "Epoch 85/250\n",
      "87012/87012 [==============================] - 6s 66us/step - loss: 0.4774 - categorical_accuracy: 0.7777 - val_loss: 0.4827 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.48125\n",
      "Epoch 86/250\n",
      "87012/87012 [==============================] - 6s 68us/step - loss: 0.4772 - categorical_accuracy: 0.7773 - val_loss: 0.4830 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.48125\n",
      "Epoch 87/250\n",
      "87012/87012 [==============================] - 6s 64us/step - loss: 0.4777 - categorical_accuracy: 0.7766 - val_loss: 0.4820 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.48125\n",
      "Epoch 88/250\n",
      "87012/87012 [==============================] - 6s 68us/step - loss: 0.4775 - categorical_accuracy: 0.7772 - val_loss: 0.4827 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.48125\n",
      "Epoch 89/250\n",
      "87012/87012 [==============================] - 6s 66us/step - loss: 0.4774 - categorical_accuracy: 0.7771 - val_loss: 0.4817 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.48125\n",
      "Epoch 90/250\n",
      "87012/87012 [==============================] - 6s 64us/step - loss: 0.4777 - categorical_accuracy: 0.7780 - val_loss: 0.4813 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.48125\n",
      "Epoch 91/250\n",
      "87012/87012 [==============================] - 6s 67us/step - loss: 0.4782 - categorical_accuracy: 0.7768 - val_loss: 0.4811 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.48125 to 0.48111, saving model to best_model.h5\n",
      "Epoch 92/250\n",
      "87012/87012 [==============================] - 6s 69us/step - loss: 0.4776 - categorical_accuracy: 0.7773 - val_loss: 0.4815 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.48111\n",
      "Epoch 93/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4783 - categorical_accuracy: 0.7765 - val_loss: 0.4819 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.48111\n",
      "Epoch 94/250\n",
      "87012/87012 [==============================] - 2s 28us/step - loss: 0.4781 - categorical_accuracy: 0.7770 - val_loss: 0.4825 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.48111\n",
      "Epoch 95/250\n",
      "87012/87012 [==============================] - 3s 29us/step - loss: 0.4775 - categorical_accuracy: 0.7768 - val_loss: 0.4829 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.48111\n",
      "Epoch 96/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4771 - categorical_accuracy: 0.7782 - val_loss: 0.4839 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.48111\n",
      "Epoch 97/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4783 - categorical_accuracy: 0.7769 - val_loss: 0.4841 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.48111\n",
      "Epoch 98/250\n",
      "87012/87012 [==============================] - 3s 40us/step - loss: 0.4780 - categorical_accuracy: 0.7772 - val_loss: 0.4828 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.48111\n",
      "Epoch 99/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4772 - categorical_accuracy: 0.7772 - val_loss: 0.4827 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.48111\n",
      "Epoch 100/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4783 - categorical_accuracy: 0.7762 - val_loss: 0.4814 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.48111\n",
      "Epoch 101/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4776 - categorical_accuracy: 0.7779 - val_loss: 0.4817 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.48111\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 102/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4777 - categorical_accuracy: 0.7769 - val_loss: 0.4820 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.48111\n",
      "Epoch 103/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4778 - categorical_accuracy: 0.7777 - val_loss: 0.4826 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.48111\n",
      "Epoch 104/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4778 - categorical_accuracy: 0.7774 - val_loss: 0.4835 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.48111\n",
      "Epoch 105/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7768 - val_loss: 0.4822 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.48111\n",
      "Epoch 106/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4771 - categorical_accuracy: 0.7772 - val_loss: 0.4828 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.48111\n",
      "Epoch 107/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7769 - val_loss: 0.4810 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.48111 to 0.48101, saving model to best_model.h5\n",
      "Epoch 108/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4772 - categorical_accuracy: 0.7772 - val_loss: 0.4814 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.48101\n",
      "Epoch 109/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7777 - val_loss: 0.4817 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.48101\n",
      "Epoch 110/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7769 - val_loss: 0.4819 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.48101\n",
      "Epoch 111/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4775 - categorical_accuracy: 0.7770 - val_loss: 0.4824 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.48101\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 9.000000136438758e-06.\n",
      "Epoch 112/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4774 - categorical_accuracy: 0.7767 - val_loss: 0.4821 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.48101\n",
      "Epoch 113/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4779 - categorical_accuracy: 0.7771 - val_loss: 0.4820 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.48101\n",
      "Epoch 114/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4776 - categorical_accuracy: 0.7772 - val_loss: 0.4826 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.48101\n",
      "Epoch 115/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4779 - categorical_accuracy: 0.7770 - val_loss: 0.4835 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.48101\n",
      "Epoch 116/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4767 - categorical_accuracy: 0.7780 - val_loss: 0.4832 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.48101\n",
      "Epoch 117/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7770 - val_loss: 0.4824 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.48101\n",
      "Epoch 118/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4774 - categorical_accuracy: 0.7773 - val_loss: 0.4827 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.48101\n",
      "Epoch 119/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4776 - categorical_accuracy: 0.7774 - val_loss: 0.4812 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.48101\n",
      "Epoch 120/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4774 - categorical_accuracy: 0.7771 - val_loss: 0.4816 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.48101\n",
      "Epoch 121/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4779 - categorical_accuracy: 0.7779 - val_loss: 0.4819 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.48101\n",
      "\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 9.000000318337698e-07.\n",
      "Epoch 122/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4784 - categorical_accuracy: 0.7767 - val_loss: 0.4816 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.48101\n",
      "Epoch 123/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4776 - categorical_accuracy: 0.7775 - val_loss: 0.4818 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.48101\n",
      "Epoch 124/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4783 - categorical_accuracy: 0.7776 - val_loss: 0.4822 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.48101\n",
      "Epoch 125/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4777 - categorical_accuracy: 0.7772 - val_loss: 0.4829 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.48101\n",
      "Epoch 126/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4777 - categorical_accuracy: 0.7764 - val_loss: 0.4828 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.48101\n",
      "Epoch 127/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4774 - categorical_accuracy: 0.7778 - val_loss: 0.4816 - val_categorical_accuracy: 0.7753\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.48101\n",
      "Epoch 00127: early stopping\n",
      "the 5 fold Log-Loss (NN) is 0.481009\n",
      "PARTIAL: mean Log-Loss (NN) is 0.478920\n",
      "-----------\n",
      "Loop 2/2 Fold 1/5\n",
      "-----------\n",
      "Train on 87011 samples, validate on 21753 samples\n",
      "Epoch 1/250\n",
      "87011/87011 [==============================] - 3s 33us/step - loss: 0.8634 - categorical_accuracy: 0.7218 - val_loss: 1.4119 - val_categorical_accuracy: 0.4057\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.41190, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5163 - categorical_accuracy: 0.7583 - val_loss: 0.5484 - val_categorical_accuracy: 0.7215\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.41190 to 0.54844, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5105 - categorical_accuracy: 0.7570 - val_loss: 0.5102 - val_categorical_accuracy: 0.7553\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54844 to 0.51016, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5051 - categorical_accuracy: 0.7593 - val_loss: 0.5136 - val_categorical_accuracy: 0.7533\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.51016\n",
      "Epoch 5/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.5037 - categorical_accuracy: 0.7608 - val_loss: 0.5029 - val_categorical_accuracy: 0.7596\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51016 to 0.50292, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5018 - categorical_accuracy: 0.7623 - val_loss: 0.5007 - val_categorical_accuracy: 0.7604\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.50292 to 0.50074, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4985 - categorical_accuracy: 0.7649 - val_loss: 0.5024 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.50074\n",
      "Epoch 8/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4962 - categorical_accuracy: 0.7663 - val_loss: 0.5004 - val_categorical_accuracy: 0.7622\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.50074 to 0.50038, saving model to best_model.h5\n",
      "Epoch 9/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4959 - categorical_accuracy: 0.7657 - val_loss: 0.4985 - val_categorical_accuracy: 0.7633\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50038 to 0.49854, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4959 - categorical_accuracy: 0.7654 - val_loss: 0.4968 - val_categorical_accuracy: 0.7651\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.49854 to 0.49682, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4952 - categorical_accuracy: 0.7667 - val_loss: 0.5010 - val_categorical_accuracy: 0.7621\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.49682\n",
      "Epoch 12/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4935 - categorical_accuracy: 0.7683 - val_loss: 0.4965 - val_categorical_accuracy: 0.7666\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.49682 to 0.49653, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4933 - categorical_accuracy: 0.7680 - val_loss: 0.4974 - val_categorical_accuracy: 0.7658\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49653\n",
      "Epoch 14/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4926 - categorical_accuracy: 0.7686 - val_loss: 0.4948 - val_categorical_accuracy: 0.7675\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.49653 to 0.49476, saving model to best_model.h5\n",
      "Epoch 15/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4917 - categorical_accuracy: 0.7696 - val_loss: 0.4986 - val_categorical_accuracy: 0.7620\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.49476\n",
      "Epoch 16/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4911 - categorical_accuracy: 0.7690 - val_loss: 0.4943 - val_categorical_accuracy: 0.7671\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.49476 to 0.49426, saving model to best_model.h5\n",
      "Epoch 17/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4901 - categorical_accuracy: 0.7709 - val_loss: 0.4947 - val_categorical_accuracy: 0.7676\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.49426\n",
      "Epoch 18/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4913 - categorical_accuracy: 0.7692 - val_loss: 0.4962 - val_categorical_accuracy: 0.7656\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.49426\n",
      "Epoch 19/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4903 - categorical_accuracy: 0.7705 - val_loss: 0.4933 - val_categorical_accuracy: 0.7674\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.49426 to 0.49325, saving model to best_model.h5\n",
      "Epoch 20/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4891 - categorical_accuracy: 0.7718 - val_loss: 0.4965 - val_categorical_accuracy: 0.7670\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.49325\n",
      "Epoch 21/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4883 - categorical_accuracy: 0.7717 - val_loss: 0.4949 - val_categorical_accuracy: 0.7687\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.49325\n",
      "Epoch 22/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4883 - categorical_accuracy: 0.7713 - val_loss: 0.4921 - val_categorical_accuracy: 0.7674\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.49325 to 0.49214, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4883 - categorical_accuracy: 0.7712 - val_loss: 0.4971 - val_categorical_accuracy: 0.7653\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.49214\n",
      "Epoch 24/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4874 - categorical_accuracy: 0.7728 - val_loss: 0.4911 - val_categorical_accuracy: 0.7684\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.49214 to 0.49110, saving model to best_model.h5\n",
      "Epoch 25/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4879 - categorical_accuracy: 0.7726 - val_loss: 0.4910 - val_categorical_accuracy: 0.7682\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.49110 to 0.49103, saving model to best_model.h5\n",
      "Epoch 26/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4878 - categorical_accuracy: 0.7728 - val_loss: 0.4919 - val_categorical_accuracy: 0.7692\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.49103\n",
      "Epoch 27/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4874 - categorical_accuracy: 0.7726 - val_loss: 0.4915 - val_categorical_accuracy: 0.7682\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.49103\n",
      "Epoch 28/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4869 - categorical_accuracy: 0.7727 - val_loss: 0.4919 - val_categorical_accuracy: 0.7689\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.49103\n",
      "Epoch 29/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4864 - categorical_accuracy: 0.7725 - val_loss: 0.4904 - val_categorical_accuracy: 0.7690\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.49103 to 0.49040, saving model to best_model.h5\n",
      "Epoch 30/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4863 - categorical_accuracy: 0.7736 - val_loss: 0.4903 - val_categorical_accuracy: 0.7679\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.49040 to 0.49027, saving model to best_model.h5\n",
      "Epoch 31/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4858 - categorical_accuracy: 0.7728 - val_loss: 0.4919 - val_categorical_accuracy: 0.7685\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.49027\n",
      "Epoch 32/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4859 - categorical_accuracy: 0.7737 - val_loss: 0.4900 - val_categorical_accuracy: 0.7687\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.49027 to 0.48997, saving model to best_model.h5\n",
      "Epoch 33/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4852 - categorical_accuracy: 0.7742 - val_loss: 0.4907 - val_categorical_accuracy: 0.7689\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.48997\n",
      "Epoch 34/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4857 - categorical_accuracy: 0.7736 - val_loss: 0.4930 - val_categorical_accuracy: 0.7680\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.48997\n",
      "Epoch 35/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4850 - categorical_accuracy: 0.7737 - val_loss: 0.4889 - val_categorical_accuracy: 0.7684\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.48997 to 0.48893, saving model to best_model.h5\n",
      "Epoch 36/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4851 - categorical_accuracy: 0.7732 - val_loss: 0.4902 - val_categorical_accuracy: 0.7684\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.48893\n",
      "Epoch 37/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4837 - categorical_accuracy: 0.7745 - val_loss: 0.4889 - val_categorical_accuracy: 0.7694\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.48893 to 0.48889, saving model to best_model.h5\n",
      "Epoch 38/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4837 - categorical_accuracy: 0.7747 - val_loss: 0.4882 - val_categorical_accuracy: 0.7695\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.48889 to 0.48824, saving model to best_model.h5\n",
      "Epoch 39/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4831 - categorical_accuracy: 0.7744 - val_loss: 0.4879 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.48824 to 0.48788, saving model to best_model.h5\n",
      "Epoch 40/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4836 - categorical_accuracy: 0.7754 - val_loss: 0.4880 - val_categorical_accuracy: 0.7689\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.48788\n",
      "Epoch 41/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4830 - categorical_accuracy: 0.7746 - val_loss: 0.4918 - val_categorical_accuracy: 0.7690\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.48788\n",
      "Epoch 42/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4829 - categorical_accuracy: 0.7750 - val_loss: 0.4880 - val_categorical_accuracy: 0.7695\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.48788\n",
      "Epoch 43/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4829 - categorical_accuracy: 0.7759 - val_loss: 0.4906 - val_categorical_accuracy: 0.7692\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.48788\n",
      "Epoch 44/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4822 - categorical_accuracy: 0.7756 - val_loss: 0.4875 - val_categorical_accuracy: 0.7697\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.48788 to 0.48755, saving model to best_model.h5\n",
      "Epoch 45/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4820 - categorical_accuracy: 0.7749 - val_loss: 0.4872 - val_categorical_accuracy: 0.7701\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.48755 to 0.48716, saving model to best_model.h5\n",
      "Epoch 46/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4818 - categorical_accuracy: 0.7765 - val_loss: 0.4875 - val_categorical_accuracy: 0.7699\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.48716\n",
      "Epoch 47/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4823 - categorical_accuracy: 0.7744 - val_loss: 0.4876 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.48716\n",
      "Epoch 48/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4815 - categorical_accuracy: 0.7759 - val_loss: 0.4874 - val_categorical_accuracy: 0.7703\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.48716\n",
      "Epoch 49/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4814 - categorical_accuracy: 0.7759 - val_loss: 0.4870 - val_categorical_accuracy: 0.7701\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.48716 to 0.48703, saving model to best_model.h5\n",
      "Epoch 50/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4811 - categorical_accuracy: 0.7768 - val_loss: 0.4876 - val_categorical_accuracy: 0.7702\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.48703\n",
      "Epoch 51/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4804 - categorical_accuracy: 0.7769 - val_loss: 0.4894 - val_categorical_accuracy: 0.7695\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.48703\n",
      "Epoch 52/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4802 - categorical_accuracy: 0.7772 - val_loss: 0.4864 - val_categorical_accuracy: 0.7709\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.48703 to 0.48636, saving model to best_model.h5\n",
      "Epoch 53/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4808 - categorical_accuracy: 0.7764 - val_loss: 0.4890 - val_categorical_accuracy: 0.7693\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.48636\n",
      "Epoch 54/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4803 - categorical_accuracy: 0.7765 - val_loss: 0.4868 - val_categorical_accuracy: 0.7703\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.48636\n",
      "Epoch 55/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4802 - categorical_accuracy: 0.7765 - val_loss: 0.4897 - val_categorical_accuracy: 0.7692\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.48636\n",
      "Epoch 56/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4805 - categorical_accuracy: 0.7768 - val_loss: 0.4862 - val_categorical_accuracy: 0.7704\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.48636 to 0.48623, saving model to best_model.h5\n",
      "Epoch 57/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4801 - categorical_accuracy: 0.7757 - val_loss: 0.4862 - val_categorical_accuracy: 0.7701\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.48623 to 0.48622, saving model to best_model.h5\n",
      "Epoch 58/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4800 - categorical_accuracy: 0.7772 - val_loss: 0.4875 - val_categorical_accuracy: 0.7711\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.48622\n",
      "Epoch 59/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4795 - categorical_accuracy: 0.7771 - val_loss: 0.4869 - val_categorical_accuracy: 0.7702\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.48622\n",
      "Epoch 60/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4792 - categorical_accuracy: 0.7767 - val_loss: 0.4866 - val_categorical_accuracy: 0.7699\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.48622\n",
      "Epoch 61/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4790 - categorical_accuracy: 0.7771 - val_loss: 0.4871 - val_categorical_accuracy: 0.7708\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.48622\n",
      "Epoch 62/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4796 - categorical_accuracy: 0.7764 - val_loss: 0.4862 - val_categorical_accuracy: 0.7703\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.48622 to 0.48615, saving model to best_model.h5\n",
      "Epoch 63/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4787 - categorical_accuracy: 0.7779 - val_loss: 0.4866 - val_categorical_accuracy: 0.7701\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.48615\n",
      "Epoch 64/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4792 - categorical_accuracy: 0.7770 - val_loss: 0.4852 - val_categorical_accuracy: 0.7708\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.48615 to 0.48518, saving model to best_model.h5\n",
      "Epoch 65/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4790 - categorical_accuracy: 0.7771 - val_loss: 0.4853 - val_categorical_accuracy: 0.7712\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.48518\n",
      "Epoch 66/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4784 - categorical_accuracy: 0.7768 - val_loss: 0.4882 - val_categorical_accuracy: 0.7701\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.48518\n",
      "Epoch 67/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7770 - val_loss: 0.4851 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.48518 to 0.48507, saving model to best_model.h5\n",
      "Epoch 68/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4790 - categorical_accuracy: 0.7769 - val_loss: 0.4861 - val_categorical_accuracy: 0.7713\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.48507\n",
      "Epoch 69/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7766 - val_loss: 0.4864 - val_categorical_accuracy: 0.7706\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.48507\n",
      "Epoch 70/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4778 - categorical_accuracy: 0.7778 - val_loss: 0.4865 - val_categorical_accuracy: 0.7705\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.48507\n",
      "Epoch 71/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4788 - categorical_accuracy: 0.7772 - val_loss: 0.4859 - val_categorical_accuracy: 0.7705\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.48507\n",
      "Epoch 72/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7782 - val_loss: 0.4857 - val_categorical_accuracy: 0.7711\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.48507\n",
      "Epoch 73/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4776 - categorical_accuracy: 0.7770 - val_loss: 0.4856 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.48507\n",
      "Epoch 74/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4778 - categorical_accuracy: 0.7773 - val_loss: 0.4848 - val_categorical_accuracy: 0.7718\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.48507 to 0.48482, saving model to best_model.h5\n",
      "Epoch 75/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4771 - categorical_accuracy: 0.7779 - val_loss: 0.4882 - val_categorical_accuracy: 0.7714\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.48482\n",
      "Epoch 76/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4773 - categorical_accuracy: 0.7777 - val_loss: 0.4854 - val_categorical_accuracy: 0.7710\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.48482\n",
      "Epoch 77/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4773 - categorical_accuracy: 0.7778 - val_loss: 0.4844 - val_categorical_accuracy: 0.7717\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.48482 to 0.48444, saving model to best_model.h5\n",
      "Epoch 78/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4774 - categorical_accuracy: 0.7768 - val_loss: 0.4855 - val_categorical_accuracy: 0.7709\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.48444\n",
      "Epoch 79/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4766 - categorical_accuracy: 0.7784 - val_loss: 0.4847 - val_categorical_accuracy: 0.7724\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.48444\n",
      "Epoch 80/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4768 - categorical_accuracy: 0.7775 - val_loss: 0.4855 - val_categorical_accuracy: 0.7718\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.48444\n",
      "Epoch 81/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4769 - categorical_accuracy: 0.7784 - val_loss: 0.4903 - val_categorical_accuracy: 0.7706\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.48444\n",
      "Epoch 82/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4765 - categorical_accuracy: 0.7775 - val_loss: 0.4851 - val_categorical_accuracy: 0.7712\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.48444\n",
      "Epoch 83/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4763 - categorical_accuracy: 0.7781 - val_loss: 0.4853 - val_categorical_accuracy: 0.7712\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.48444\n",
      "Epoch 84/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4764 - categorical_accuracy: 0.7787 - val_loss: 0.4850 - val_categorical_accuracy: 0.7712\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.48444\n",
      "Epoch 85/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4764 - categorical_accuracy: 0.7781 - val_loss: 0.4845 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.48444\n",
      "Epoch 86/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4768 - categorical_accuracy: 0.7784 - val_loss: 0.4853 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.48444\n",
      "Epoch 87/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4759 - categorical_accuracy: 0.7792 - val_loss: 0.4868 - val_categorical_accuracy: 0.7706\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.48444\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.00900000035762787.\n",
      "Epoch 88/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4752 - categorical_accuracy: 0.7789 - val_loss: 0.4847 - val_categorical_accuracy: 0.7710\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.48444\n",
      "Epoch 89/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4752 - categorical_accuracy: 0.7793 - val_loss: 0.4848 - val_categorical_accuracy: 0.7712\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.48444\n",
      "Epoch 90/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4747 - categorical_accuracy: 0.7785 - val_loss: 0.4846 - val_categorical_accuracy: 0.7713\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.48444\n",
      "Epoch 91/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4749 - categorical_accuracy: 0.7796 - val_loss: 0.4847 - val_categorical_accuracy: 0.7712\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.48444\n",
      "Epoch 92/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4743 - categorical_accuracy: 0.7791 - val_loss: 0.4847 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.48444\n",
      "Epoch 93/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4755 - categorical_accuracy: 0.7781 - val_loss: 0.4847 - val_categorical_accuracy: 0.7714\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.48444\n",
      "Epoch 94/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4751 - categorical_accuracy: 0.7786 - val_loss: 0.4845 - val_categorical_accuracy: 0.7712\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.48444\n",
      "Epoch 95/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4750 - categorical_accuracy: 0.7788 - val_loss: 0.4847 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.48444\n",
      "Epoch 96/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4751 - categorical_accuracy: 0.7790 - val_loss: 0.4845 - val_categorical_accuracy: 0.7713\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.48444\n",
      "Epoch 97/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4754 - categorical_accuracy: 0.7781 - val_loss: 0.4847 - val_categorical_accuracy: 0.7715\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.48444\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.0009000000543892384.\n",
      "Epoch 00097: early stopping\n",
      "the 1 fold Log-Loss (NN) is 0.484438\n",
      "-----------\n",
      "Loop 2/2 Fold 2/5\n",
      "-----------\n",
      "Train on 87011 samples, validate on 21753 samples\n",
      "Epoch 1/250\n",
      "87011/87011 [==============================] - 3s 33us/step - loss: 0.9552 - categorical_accuracy: 0.7186 - val_loss: 0.5695 - val_categorical_accuracy: 0.7079\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.56948, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5158 - categorical_accuracy: 0.7542 - val_loss: 0.5014 - val_categorical_accuracy: 0.7645\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.56948 to 0.50136, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5091 - categorical_accuracy: 0.7574 - val_loss: 0.5158 - val_categorical_accuracy: 0.7612\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.50136\n",
      "Epoch 4/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.5033 - categorical_accuracy: 0.7596 - val_loss: 0.4984 - val_categorical_accuracy: 0.7613\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.50136 to 0.49838, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5035 - categorical_accuracy: 0.7593 - val_loss: 0.4914 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.49838 to 0.49144, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5007 - categorical_accuracy: 0.7615 - val_loss: 0.4941 - val_categorical_accuracy: 0.7616\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49144\n",
      "Epoch 7/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4986 - categorical_accuracy: 0.7634 - val_loss: 0.4889 - val_categorical_accuracy: 0.7713\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.49144 to 0.48891, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4975 - categorical_accuracy: 0.7635 - val_loss: 0.4968 - val_categorical_accuracy: 0.7614\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48891\n",
      "Epoch 9/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4957 - categorical_accuracy: 0.7648 - val_loss: 0.4879 - val_categorical_accuracy: 0.7711\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.48891 to 0.48790, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4943 - categorical_accuracy: 0.7674 - val_loss: 0.4862 - val_categorical_accuracy: 0.7732\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.48790 to 0.48617, saving model to best_model.h5\n",
      "Epoch 11/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4954 - categorical_accuracy: 0.7665 - val_loss: 0.4913 - val_categorical_accuracy: 0.7691\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48617\n",
      "Epoch 12/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4928 - categorical_accuracy: 0.7670 - val_loss: 0.4854 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.48617 to 0.48538, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4916 - categorical_accuracy: 0.7694 - val_loss: 0.4847 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.48538 to 0.48472, saving model to best_model.h5\n",
      "Epoch 14/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4912 - categorical_accuracy: 0.7691 - val_loss: 0.4837 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.48472 to 0.48373, saving model to best_model.h5\n",
      "Epoch 15/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4909 - categorical_accuracy: 0.7703 - val_loss: 0.4834 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.48373 to 0.48342, saving model to best_model.h5\n",
      "Epoch 16/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4907 - categorical_accuracy: 0.7700 - val_loss: 0.4828 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.48342 to 0.48276, saving model to best_model.h5\n",
      "Epoch 17/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4907 - categorical_accuracy: 0.7694 - val_loss: 0.4825 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.48276 to 0.48245, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4891 - categorical_accuracy: 0.7715 - val_loss: 0.4820 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.48245 to 0.48202, saving model to best_model.h5\n",
      "Epoch 19/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4894 - categorical_accuracy: 0.7702 - val_loss: 0.4823 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.48202\n",
      "Epoch 20/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4887 - categorical_accuracy: 0.7717 - val_loss: 0.4857 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.48202\n",
      "Epoch 21/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4894 - categorical_accuracy: 0.7702 - val_loss: 0.4855 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.48202\n",
      "Epoch 22/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4892 - categorical_accuracy: 0.7702 - val_loss: 0.4842 - val_categorical_accuracy: 0.7770\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.48202\n",
      "Epoch 23/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4882 - categorical_accuracy: 0.7720 - val_loss: 0.4806 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.48202 to 0.48059, saving model to best_model.h5\n",
      "Epoch 24/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4872 - categorical_accuracy: 0.7724 - val_loss: 0.4807 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.48059\n",
      "Epoch 25/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4874 - categorical_accuracy: 0.7713 - val_loss: 0.4825 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.48059\n",
      "Epoch 26/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4866 - categorical_accuracy: 0.7724 - val_loss: 0.4814 - val_categorical_accuracy: 0.7768\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.48059\n",
      "Epoch 27/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4855 - categorical_accuracy: 0.7732 - val_loss: 0.4817 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.48059\n",
      "Epoch 28/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4863 - categorical_accuracy: 0.7729 - val_loss: 0.4820 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.48059\n",
      "Epoch 29/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4860 - categorical_accuracy: 0.7717 - val_loss: 0.4806 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.48059\n",
      "Epoch 30/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4857 - categorical_accuracy: 0.7726 - val_loss: 0.4806 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.48059\n",
      "Epoch 31/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4857 - categorical_accuracy: 0.7731 - val_loss: 0.4796 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.48059 to 0.47960, saving model to best_model.h5\n",
      "Epoch 32/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4862 - categorical_accuracy: 0.7726 - val_loss: 0.4811 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.47960\n",
      "Epoch 33/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4854 - categorical_accuracy: 0.7741 - val_loss: 0.4870 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47960\n",
      "Epoch 34/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4844 - categorical_accuracy: 0.7737 - val_loss: 0.4796 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.47960\n",
      "Epoch 35/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4848 - categorical_accuracy: 0.7738 - val_loss: 0.4789 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.47960 to 0.47887, saving model to best_model.h5\n",
      "Epoch 36/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4839 - categorical_accuracy: 0.7745 - val_loss: 0.4785 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.47887 to 0.47853, saving model to best_model.h5\n",
      "Epoch 37/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4834 - categorical_accuracy: 0.7744 - val_loss: 0.4813 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47853\n",
      "Epoch 38/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4835 - categorical_accuracy: 0.7747 - val_loss: 0.4791 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.47853\n",
      "Epoch 39/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4833 - categorical_accuracy: 0.7745 - val_loss: 0.4787 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.47853\n",
      "Epoch 40/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4828 - categorical_accuracy: 0.7743 - val_loss: 0.4825 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.47853\n",
      "Epoch 41/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4840 - categorical_accuracy: 0.7739 - val_loss: 0.4780 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.47853 to 0.47802, saving model to best_model.h5\n",
      "Epoch 42/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4826 - categorical_accuracy: 0.7734 - val_loss: 0.4777 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.47802 to 0.47771, saving model to best_model.h5\n",
      "Epoch 43/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4828 - categorical_accuracy: 0.7755 - val_loss: 0.4774 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.47771 to 0.47745, saving model to best_model.h5\n",
      "Epoch 44/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4820 - categorical_accuracy: 0.7746 - val_loss: 0.4781 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47745\n",
      "Epoch 45/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4822 - categorical_accuracy: 0.7747 - val_loss: 0.4771 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.47745 to 0.47706, saving model to best_model.h5\n",
      "Epoch 46/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4811 - categorical_accuracy: 0.7753 - val_loss: 0.4775 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47706\n",
      "Epoch 47/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4818 - categorical_accuracy: 0.7754 - val_loss: 0.4775 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47706\n",
      "Epoch 48/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4811 - categorical_accuracy: 0.7757 - val_loss: 0.4788 - val_categorical_accuracy: 0.7782\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.47706\n",
      "Epoch 49/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4811 - categorical_accuracy: 0.7750 - val_loss: 0.4779 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47706\n",
      "Epoch 50/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4804 - categorical_accuracy: 0.7751 - val_loss: 0.4772 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47706\n",
      "Epoch 51/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4824 - categorical_accuracy: 0.7744 - val_loss: 0.4797 - val_categorical_accuracy: 0.7775\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47706\n",
      "Epoch 52/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4813 - categorical_accuracy: 0.7756 - val_loss: 0.4772 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47706\n",
      "Epoch 53/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4802 - categorical_accuracy: 0.7764 - val_loss: 0.4767 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.47706 to 0.47668, saving model to best_model.h5\n",
      "Epoch 54/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4801 - categorical_accuracy: 0.7756 - val_loss: 0.4764 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.47668 to 0.47645, saving model to best_model.h5\n",
      "Epoch 55/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4803 - categorical_accuracy: 0.7756 - val_loss: 0.4772 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47645\n",
      "Epoch 56/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4797 - categorical_accuracy: 0.7763 - val_loss: 0.4781 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47645\n",
      "Epoch 57/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4804 - categorical_accuracy: 0.7763 - val_loss: 0.4767 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47645\n",
      "Epoch 58/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4799 - categorical_accuracy: 0.7759 - val_loss: 0.4764 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.47645 to 0.47644, saving model to best_model.h5\n",
      "Epoch 59/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4792 - categorical_accuracy: 0.7771 - val_loss: 0.4759 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.47644 to 0.47594, saving model to best_model.h5\n",
      "Epoch 60/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4796 - categorical_accuracy: 0.7759 - val_loss: 0.4761 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47594\n",
      "Epoch 61/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4791 - categorical_accuracy: 0.7762 - val_loss: 0.4762 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47594\n",
      "Epoch 62/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4791 - categorical_accuracy: 0.7770 - val_loss: 0.4767 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47594\n",
      "Epoch 63/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4797 - categorical_accuracy: 0.7760 - val_loss: 0.4762 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47594\n",
      "Epoch 64/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4795 - categorical_accuracy: 0.7756 - val_loss: 0.4762 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47594\n",
      "Epoch 65/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4785 - categorical_accuracy: 0.7757 - val_loss: 0.4758 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.47594 to 0.47577, saving model to best_model.h5\n",
      "Epoch 66/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4781 - categorical_accuracy: 0.7769 - val_loss: 0.4755 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.47577 to 0.47547, saving model to best_model.h5\n",
      "Epoch 67/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4781 - categorical_accuracy: 0.7771 - val_loss: 0.4757 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.47547\n",
      "Epoch 68/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4788 - categorical_accuracy: 0.7766 - val_loss: 0.4757 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.47547\n",
      "Epoch 69/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4777 - categorical_accuracy: 0.7763 - val_loss: 0.4759 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47547\n",
      "Epoch 70/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4784 - categorical_accuracy: 0.7760 - val_loss: 0.4757 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47547\n",
      "Epoch 71/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7766 - val_loss: 0.4756 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47547\n",
      "Epoch 72/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4785 - categorical_accuracy: 0.7766 - val_loss: 0.4760 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.47547\n",
      "Epoch 73/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4775 - categorical_accuracy: 0.7772 - val_loss: 0.4757 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47547\n",
      "Epoch 74/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4778 - categorical_accuracy: 0.7777 - val_loss: 0.4764 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.47547\n",
      "Epoch 75/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4775 - categorical_accuracy: 0.7773 - val_loss: 0.4757 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.47547\n",
      "Epoch 76/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4780 - categorical_accuracy: 0.7768 - val_loss: 0.4754 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.47547 to 0.47540, saving model to best_model.h5\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.00900000035762787.\n",
      "Epoch 77/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4765 - categorical_accuracy: 0.7781 - val_loss: 0.4752 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.47540 to 0.47522, saving model to best_model.h5\n",
      "Epoch 78/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4769 - categorical_accuracy: 0.7782 - val_loss: 0.4752 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.47522\n",
      "Epoch 79/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4764 - categorical_accuracy: 0.7775 - val_loss: 0.4752 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.47522 to 0.47521, saving model to best_model.h5\n",
      "Epoch 80/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4762 - categorical_accuracy: 0.7780 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.47521 to 0.47520, saving model to best_model.h5\n",
      "Epoch 81/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4763 - categorical_accuracy: 0.7781 - val_loss: 0.4752 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.47520 to 0.47518, saving model to best_model.h5\n",
      "Epoch 82/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4753 - categorical_accuracy: 0.7786 - val_loss: 0.4752 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.47518 to 0.47517, saving model to best_model.h5\n",
      "Epoch 83/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4761 - categorical_accuracy: 0.7779 - val_loss: 0.4753 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.47517\n",
      "Epoch 84/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4764 - categorical_accuracy: 0.7766 - val_loss: 0.4752 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.47517\n",
      "Epoch 85/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4758 - categorical_accuracy: 0.7780 - val_loss: 0.4752 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.47517\n",
      "Epoch 86/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4764 - categorical_accuracy: 0.7781 - val_loss: 0.4753 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.47517\n",
      "Epoch 87/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4757 - categorical_accuracy: 0.7774 - val_loss: 0.4752 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.47517\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.0009000000543892384.\n",
      "Epoch 88/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4757 - categorical_accuracy: 0.7776 - val_loss: 0.4752 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.47517\n",
      "Epoch 89/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4760 - categorical_accuracy: 0.7775 - val_loss: 0.4752 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.47517\n",
      "Epoch 90/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4758 - categorical_accuracy: 0.7769 - val_loss: 0.4752 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00090: val_loss improved from 0.47517 to 0.47517, saving model to best_model.h5\n",
      "Epoch 91/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4756 - categorical_accuracy: 0.7779 - val_loss: 0.4752 - val_categorical_accuracy: 0.7790\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.47517\n",
      "Epoch 92/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4766 - categorical_accuracy: 0.7776 - val_loss: 0.4752 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.47517\n",
      "Epoch 93/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4760 - categorical_accuracy: 0.7777 - val_loss: 0.4752 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.47517\n",
      "Epoch 94/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4757 - categorical_accuracy: 0.7776 - val_loss: 0.4752 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.47517 to 0.47517, saving model to best_model.h5\n",
      "Epoch 95/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4759 - categorical_accuracy: 0.7778 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.47517\n",
      "Epoch 96/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4760 - categorical_accuracy: 0.7778 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.47517\n",
      "Epoch 97/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4762 - categorical_accuracy: 0.7785 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.47517\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 98/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4761 - categorical_accuracy: 0.7781 - val_loss: 0.4752 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.47517 to 0.47516, saving model to best_model.h5\n",
      "Epoch 99/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4762 - categorical_accuracy: 0.7783 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.47516\n",
      "Epoch 100/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4763 - categorical_accuracy: 0.7777 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.47516\n",
      "Epoch 101/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4755 - categorical_accuracy: 0.7779 - val_loss: 0.4752 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.47516\n",
      "Epoch 102/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4762 - categorical_accuracy: 0.7776 - val_loss: 0.4752 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.47516 to 0.47516, saving model to best_model.h5\n",
      "Epoch 103/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4766 - categorical_accuracy: 0.7777 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.47516\n",
      "Epoch 104/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4755 - categorical_accuracy: 0.7778 - val_loss: 0.4751 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.47516 to 0.47514, saving model to best_model.h5\n",
      "Epoch 105/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4765 - categorical_accuracy: 0.7777 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.47514\n",
      "Epoch 106/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4754 - categorical_accuracy: 0.7780 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.47514\n",
      "Epoch 107/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4753 - categorical_accuracy: 0.7780 - val_loss: 0.4752 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.47514\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 9.000000136438758e-06.\n",
      "Epoch 108/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4760 - categorical_accuracy: 0.7776 - val_loss: 0.4752 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.47514\n",
      "Epoch 109/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4767 - categorical_accuracy: 0.7775 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.47514\n",
      "Epoch 110/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4760 - categorical_accuracy: 0.7776 - val_loss: 0.4752 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.47514\n",
      "Epoch 111/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4759 - categorical_accuracy: 0.7777 - val_loss: 0.4751 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.47514\n",
      "Epoch 112/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4761 - categorical_accuracy: 0.7776 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.47514\n",
      "Epoch 113/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4757 - categorical_accuracy: 0.7774 - val_loss: 0.4752 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.47514\n",
      "Epoch 114/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4757 - categorical_accuracy: 0.7778 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.47514\n",
      "Epoch 115/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4772 - categorical_accuracy: 0.7773 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.47514\n",
      "Epoch 116/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4764 - categorical_accuracy: 0.7784 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.47514\n",
      "Epoch 117/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4766 - categorical_accuracy: 0.7781 - val_loss: 0.4752 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.47514\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 9.000000318337698e-07.\n",
      "Epoch 118/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4762 - categorical_accuracy: 0.7782 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.47514\n",
      "Epoch 119/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4761 - categorical_accuracy: 0.7773 - val_loss: 0.4752 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.47514\n",
      "Epoch 120/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4760 - categorical_accuracy: 0.7776 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.47514\n",
      "Epoch 121/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4766 - categorical_accuracy: 0.7766 - val_loss: 0.4752 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.47514\n",
      "Epoch 122/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4759 - categorical_accuracy: 0.7770 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00122: val_loss did not improve from 0.47514\n",
      "Epoch 123/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4768 - categorical_accuracy: 0.7774 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.47514\n",
      "Epoch 124/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4760 - categorical_accuracy: 0.7776 - val_loss: 0.4752 - val_categorical_accuracy: 0.7787\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.47514\n",
      "Epoch 00124: early stopping\n",
      "the 2 fold Log-Loss (NN) is 0.475140\n",
      "-----------\n",
      "Loop 2/2 Fold 3/5\n",
      "-----------\n",
      "Train on 87011 samples, validate on 21753 samples\n",
      "Epoch 1/250\n",
      "87011/87011 [==============================] - 3s 33us/step - loss: 0.8808 - categorical_accuracy: 0.7216 - val_loss: 0.7038 - val_categorical_accuracy: 0.6318\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70385, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5145 - categorical_accuracy: 0.7572 - val_loss: 0.5228 - val_categorical_accuracy: 0.7339\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70385 to 0.52282, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5056 - categorical_accuracy: 0.7583 - val_loss: 0.4959 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52282 to 0.49590, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5016 - categorical_accuracy: 0.7616 - val_loss: 0.4938 - val_categorical_accuracy: 0.7629\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49590 to 0.49383, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5000 - categorical_accuracy: 0.7637 - val_loss: 0.4952 - val_categorical_accuracy: 0.7612\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49383\n",
      "Epoch 6/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4985 - categorical_accuracy: 0.7644 - val_loss: 0.5022 - val_categorical_accuracy: 0.7616\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49383\n",
      "Epoch 7/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4985 - categorical_accuracy: 0.7648 - val_loss: 0.4888 - val_categorical_accuracy: 0.7691\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.49383 to 0.48884, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4950 - categorical_accuracy: 0.7678 - val_loss: 0.4895 - val_categorical_accuracy: 0.7684\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48884\n",
      "Epoch 9/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4956 - categorical_accuracy: 0.7667 - val_loss: 0.4883 - val_categorical_accuracy: 0.7708\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.48884 to 0.48833, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4949 - categorical_accuracy: 0.7674 - val_loss: 0.4896 - val_categorical_accuracy: 0.7685\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48833\n",
      "Epoch 11/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4936 - categorical_accuracy: 0.7694 - val_loss: 0.4885 - val_categorical_accuracy: 0.7707\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.48833\n",
      "Epoch 12/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4926 - categorical_accuracy: 0.7688 - val_loss: 0.4872 - val_categorical_accuracy: 0.7714\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.48833 to 0.48718, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4917 - categorical_accuracy: 0.7697 - val_loss: 0.4876 - val_categorical_accuracy: 0.7713\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.48718\n",
      "Epoch 14/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4910 - categorical_accuracy: 0.7709 - val_loss: 0.4846 - val_categorical_accuracy: 0.7734\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.48718 to 0.48464, saving model to best_model.h5\n",
      "Epoch 15/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4915 - categorical_accuracy: 0.7698 - val_loss: 0.4849 - val_categorical_accuracy: 0.7732\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.48464\n",
      "Epoch 16/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4905 - categorical_accuracy: 0.7700 - val_loss: 0.4853 - val_categorical_accuracy: 0.7739\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.48464\n",
      "Epoch 17/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4896 - categorical_accuracy: 0.7708 - val_loss: 0.4843 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.48464 to 0.48435, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4894 - categorical_accuracy: 0.7716 - val_loss: 0.4838 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.48435 to 0.48379, saving model to best_model.h5\n",
      "Epoch 19/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4883 - categorical_accuracy: 0.7723 - val_loss: 0.4822 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.48379 to 0.48222, saving model to best_model.h5\n",
      "Epoch 20/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4882 - categorical_accuracy: 0.7725 - val_loss: 0.4861 - val_categorical_accuracy: 0.7737\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.48222\n",
      "Epoch 21/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4881 - categorical_accuracy: 0.7718 - val_loss: 0.4868 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.48222\n",
      "Epoch 22/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4874 - categorical_accuracy: 0.7724 - val_loss: 0.4836 - val_categorical_accuracy: 0.7732\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.48222\n",
      "Epoch 23/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4864 - categorical_accuracy: 0.7736 - val_loss: 0.4817 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.48222 to 0.48167, saving model to best_model.h5\n",
      "Epoch 24/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4867 - categorical_accuracy: 0.7728 - val_loss: 0.4805 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.48167 to 0.48049, saving model to best_model.h5\n",
      "Epoch 25/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4865 - categorical_accuracy: 0.7730 - val_loss: 0.4807 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.48049\n",
      "Epoch 26/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4859 - categorical_accuracy: 0.7742 - val_loss: 0.4854 - val_categorical_accuracy: 0.7731\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.48049\n",
      "Epoch 27/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4844 - categorical_accuracy: 0.7750 - val_loss: 0.4807 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.48049\n",
      "Epoch 28/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4849 - categorical_accuracy: 0.7746 - val_loss: 0.4848 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.48049\n",
      "Epoch 29/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4854 - categorical_accuracy: 0.7735 - val_loss: 0.4802 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.48049 to 0.48019, saving model to best_model.h5\n",
      "Epoch 30/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4834 - categorical_accuracy: 0.7750 - val_loss: 0.4815 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.48019\n",
      "Epoch 31/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4845 - categorical_accuracy: 0.7741 - val_loss: 0.4807 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.48019\n",
      "Epoch 32/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4841 - categorical_accuracy: 0.7750 - val_loss: 0.4813 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.48019\n",
      "Epoch 33/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4838 - categorical_accuracy: 0.7743 - val_loss: 0.4800 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.48019 to 0.48005, saving model to best_model.h5\n",
      "Epoch 34/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4838 - categorical_accuracy: 0.7744 - val_loss: 0.4807 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.48005\n",
      "Epoch 35/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4834 - categorical_accuracy: 0.7746 - val_loss: 0.4791 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.48005 to 0.47914, saving model to best_model.h5\n",
      "Epoch 36/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4830 - categorical_accuracy: 0.7751 - val_loss: 0.4795 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47914\n",
      "Epoch 37/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4831 - categorical_accuracy: 0.7750 - val_loss: 0.4806 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.47914\n",
      "Epoch 38/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4824 - categorical_accuracy: 0.7757 - val_loss: 0.4775 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.47914 to 0.47749, saving model to best_model.h5\n",
      "Epoch 39/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4821 - categorical_accuracy: 0.7757 - val_loss: 0.4792 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.47749\n",
      "Epoch 40/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4814 - categorical_accuracy: 0.7756 - val_loss: 0.4784 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.47749\n",
      "Epoch 41/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4813 - categorical_accuracy: 0.7752 - val_loss: 0.4788 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.47749\n",
      "Epoch 42/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4813 - categorical_accuracy: 0.7764 - val_loss: 0.4770 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.47749 to 0.47704, saving model to best_model.h5\n",
      "Epoch 43/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4807 - categorical_accuracy: 0.7765 - val_loss: 0.4782 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47704\n",
      "Epoch 44/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4808 - categorical_accuracy: 0.7759 - val_loss: 0.4787 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.47704\n",
      "Epoch 45/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4798 - categorical_accuracy: 0.7765 - val_loss: 0.4777 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47704\n",
      "Epoch 46/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4805 - categorical_accuracy: 0.7761 - val_loss: 0.4783 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.47704\n",
      "Epoch 47/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4811 - categorical_accuracy: 0.7755 - val_loss: 0.4767 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.47704 to 0.47669, saving model to best_model.h5\n",
      "Epoch 48/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4798 - categorical_accuracy: 0.7768 - val_loss: 0.4765 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.47669 to 0.47651, saving model to best_model.h5\n",
      "Epoch 49/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4799 - categorical_accuracy: 0.7761 - val_loss: 0.4776 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47651\n",
      "Epoch 50/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4796 - categorical_accuracy: 0.7767 - val_loss: 0.4762 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.47651 to 0.47617, saving model to best_model.h5\n",
      "Epoch 51/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4800 - categorical_accuracy: 0.7762 - val_loss: 0.4771 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47617\n",
      "Epoch 52/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4793 - categorical_accuracy: 0.7759 - val_loss: 0.4773 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.47617\n",
      "Epoch 53/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4796 - categorical_accuracy: 0.7776 - val_loss: 0.4765 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.47617\n",
      "Epoch 54/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4798 - categorical_accuracy: 0.7767 - val_loss: 0.4767 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47617\n",
      "Epoch 55/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4793 - categorical_accuracy: 0.7768 - val_loss: 0.4776 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47617\n",
      "Epoch 56/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4785 - categorical_accuracy: 0.7769 - val_loss: 0.4770 - val_categorical_accuracy: 0.7772\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47617\n",
      "Epoch 57/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7767 - val_loss: 0.4782 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.47617\n",
      "Epoch 58/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4793 - categorical_accuracy: 0.7766 - val_loss: 0.4776 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47617\n",
      "Epoch 59/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4787 - categorical_accuracy: 0.7767 - val_loss: 0.4769 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.47617\n",
      "Epoch 60/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4775 - categorical_accuracy: 0.7772 - val_loss: 0.4771 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47617\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.00900000035762787.\n",
      "Epoch 61/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4772 - categorical_accuracy: 0.7767 - val_loss: 0.4763 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.47617\n",
      "Epoch 62/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4771 - categorical_accuracy: 0.7780 - val_loss: 0.4758 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.47617 to 0.47583, saving model to best_model.h5\n",
      "Epoch 63/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4771 - categorical_accuracy: 0.7776 - val_loss: 0.4762 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47583\n",
      "Epoch 64/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4764 - categorical_accuracy: 0.7781 - val_loss: 0.4759 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47583\n",
      "Epoch 65/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4769 - categorical_accuracy: 0.7773 - val_loss: 0.4759 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47583\n",
      "Epoch 66/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4768 - categorical_accuracy: 0.7783 - val_loss: 0.4758 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.47583 to 0.47575, saving model to best_model.h5\n",
      "Epoch 67/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4773 - categorical_accuracy: 0.7778 - val_loss: 0.4761 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.47575\n",
      "Epoch 68/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4764 - categorical_accuracy: 0.7784 - val_loss: 0.4760 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.47575\n",
      "Epoch 69/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4767 - categorical_accuracy: 0.7777 - val_loss: 0.4758 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47575\n",
      "Epoch 70/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4760 - categorical_accuracy: 0.7788 - val_loss: 0.4758 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47575\n",
      "Epoch 71/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4765 - categorical_accuracy: 0.7787 - val_loss: 0.4757 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.47575 to 0.47572, saving model to best_model.h5\n",
      "Epoch 72/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4768 - categorical_accuracy: 0.7771 - val_loss: 0.4757 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.47572 to 0.47567, saving model to best_model.h5\n",
      "Epoch 73/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4767 - categorical_accuracy: 0.7779 - val_loss: 0.4761 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.47567\n",
      "Epoch 74/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4763 - categorical_accuracy: 0.7791 - val_loss: 0.4757 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.47567\n",
      "Epoch 75/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4761 - categorical_accuracy: 0.7789 - val_loss: 0.4757 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.47567\n",
      "Epoch 76/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4770 - categorical_accuracy: 0.7770 - val_loss: 0.4759 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.47567\n",
      "Epoch 77/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4764 - categorical_accuracy: 0.7786 - val_loss: 0.4759 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.47567\n",
      "Epoch 78/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4767 - categorical_accuracy: 0.7784 - val_loss: 0.4765 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.47567\n",
      "Epoch 79/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4769 - categorical_accuracy: 0.7776 - val_loss: 0.4759 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.47567\n",
      "Epoch 80/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4769 - categorical_accuracy: 0.7783 - val_loss: 0.4758 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.47567\n",
      "Epoch 81/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4765 - categorical_accuracy: 0.7785 - val_loss: 0.4762 - val_categorical_accuracy: 0.7765\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.47567\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.0009000000543892384.\n",
      "Epoch 82/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4768 - categorical_accuracy: 0.7785 - val_loss: 0.4760 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.47567\n",
      "Epoch 83/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4764 - categorical_accuracy: 0.7783 - val_loss: 0.4759 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.47567\n",
      "Epoch 84/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4762 - categorical_accuracy: 0.7785 - val_loss: 0.4759 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.47567\n",
      "Epoch 85/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4769 - categorical_accuracy: 0.7775 - val_loss: 0.4759 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.47567\n",
      "Epoch 86/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4763 - categorical_accuracy: 0.7780 - val_loss: 0.4759 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.47567\n",
      "Epoch 87/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4765 - categorical_accuracy: 0.7778 - val_loss: 0.4759 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.47567\n",
      "Epoch 88/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4765 - categorical_accuracy: 0.7776 - val_loss: 0.4759 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.47567\n",
      "Epoch 89/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4763 - categorical_accuracy: 0.7772 - val_loss: 0.4759 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.47567\n",
      "Epoch 90/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4759 - categorical_accuracy: 0.7776 - val_loss: 0.4758 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.47567\n",
      "Epoch 91/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4766 - categorical_accuracy: 0.7786 - val_loss: 0.4758 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.47567\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 92/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4764 - categorical_accuracy: 0.7786 - val_loss: 0.4758 - val_categorical_accuracy: 0.7761\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.47567\n",
      "Epoch 00092: early stopping\n",
      "the 3 fold Log-Loss (NN) is 0.475668\n",
      "-----------\n",
      "Loop 2/2 Fold 4/5\n",
      "-----------\n",
      "Train on 87011 samples, validate on 21753 samples\n",
      "Epoch 1/250\n",
      "87011/87011 [==============================] - 3s 33us/step - loss: 0.9656 - categorical_accuracy: 0.7210 - val_loss: 1.0099 - val_categorical_accuracy: 0.4444\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00986, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5202 - categorical_accuracy: 0.7551 - val_loss: 0.5287 - val_categorical_accuracy: 0.7296\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00986 to 0.52874, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5098 - categorical_accuracy: 0.7571 - val_loss: 0.5034 - val_categorical_accuracy: 0.7638\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52874 to 0.50342, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5043 - categorical_accuracy: 0.7595 - val_loss: 0.5042 - val_categorical_accuracy: 0.7648\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.50342\n",
      "Epoch 5/250\n",
      "87011/87011 [==============================] - 2s 28us/step - loss: 0.5041 - categorical_accuracy: 0.7590 - val_loss: 0.4932 - val_categorical_accuracy: 0.7665\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.50342 to 0.49319, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5010 - categorical_accuracy: 0.7619 - val_loss: 0.4910 - val_categorical_accuracy: 0.7707\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.49319 to 0.49105, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.5002 - categorical_accuracy: 0.7613 - val_loss: 0.4952 - val_categorical_accuracy: 0.7628\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.49105\n",
      "Epoch 8/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4981 - categorical_accuracy: 0.7638 - val_loss: 0.4891 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.49105 to 0.48905, saving model to best_model.h5\n",
      "Epoch 9/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4983 - categorical_accuracy: 0.7639 - val_loss: 0.4935 - val_categorical_accuracy: 0.7673\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48905\n",
      "Epoch 10/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4961 - categorical_accuracy: 0.7657 - val_loss: 0.4923 - val_categorical_accuracy: 0.7666\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48905\n",
      "Epoch 11/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4961 - categorical_accuracy: 0.7653 - val_loss: 0.4868 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.48905 to 0.48683, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4943 - categorical_accuracy: 0.7665 - val_loss: 0.4860 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.48683 to 0.48604, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4938 - categorical_accuracy: 0.7670 - val_loss: 0.4854 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.48604 to 0.48536, saving model to best_model.h5\n",
      "Epoch 14/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4938 - categorical_accuracy: 0.7681 - val_loss: 0.4848 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.48536 to 0.48475, saving model to best_model.h5\n",
      "Epoch 15/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4929 - categorical_accuracy: 0.7668 - val_loss: 0.4842 - val_categorical_accuracy: 0.7761\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.48475 to 0.48417, saving model to best_model.h5\n",
      "Epoch 16/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4925 - categorical_accuracy: 0.7682 - val_loss: 0.4856 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.48417\n",
      "Epoch 17/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4913 - categorical_accuracy: 0.7685 - val_loss: 0.4842 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.48417\n",
      "Epoch 18/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4924 - categorical_accuracy: 0.7690 - val_loss: 0.4841 - val_categorical_accuracy: 0.7763\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.48417 to 0.48409, saving model to best_model.h5\n",
      "Epoch 19/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4908 - categorical_accuracy: 0.7689 - val_loss: 0.4888 - val_categorical_accuracy: 0.7703\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.48409\n",
      "Epoch 20/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4911 - categorical_accuracy: 0.7696 - val_loss: 0.4824 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.48409 to 0.48237, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4894 - categorical_accuracy: 0.7700 - val_loss: 0.4832 - val_categorical_accuracy: 0.7767\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.48237\n",
      "Epoch 22/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4888 - categorical_accuracy: 0.7707 - val_loss: 0.4817 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.48237 to 0.48171, saving model to best_model.h5\n",
      "Epoch 23/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4890 - categorical_accuracy: 0.7709 - val_loss: 0.4823 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.48171\n",
      "Epoch 24/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4878 - categorical_accuracy: 0.7716 - val_loss: 0.4815 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.48171 to 0.48153, saving model to best_model.h5\n",
      "Epoch 25/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4882 - categorical_accuracy: 0.7709 - val_loss: 0.4815 - val_categorical_accuracy: 0.7769\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.48153 to 0.48147, saving model to best_model.h5\n",
      "Epoch 26/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4886 - categorical_accuracy: 0.7715 - val_loss: 0.4813 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.48147 to 0.48128, saving model to best_model.h5\n",
      "Epoch 27/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4873 - categorical_accuracy: 0.7724 - val_loss: 0.4803 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.48128 to 0.48026, saving model to best_model.h5\n",
      "Epoch 28/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4884 - categorical_accuracy: 0.7715 - val_loss: 0.4811 - val_categorical_accuracy: 0.7779\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.48026\n",
      "Epoch 29/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4870 - categorical_accuracy: 0.7720 - val_loss: 0.4808 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.48026\n",
      "Epoch 30/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4863 - categorical_accuracy: 0.7724 - val_loss: 0.4802 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.48026 to 0.48024, saving model to best_model.h5\n",
      "Epoch 31/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4860 - categorical_accuracy: 0.7719 - val_loss: 0.4798 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.48024 to 0.47975, saving model to best_model.h5\n",
      "Epoch 32/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4861 - categorical_accuracy: 0.7722 - val_loss: 0.4795 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.47975 to 0.47952, saving model to best_model.h5\n",
      "Epoch 33/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4862 - categorical_accuracy: 0.7725 - val_loss: 0.4800 - val_categorical_accuracy: 0.7778\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.47952\n",
      "Epoch 34/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4867 - categorical_accuracy: 0.7720 - val_loss: 0.4799 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.47952\n",
      "Epoch 35/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4857 - categorical_accuracy: 0.7731 - val_loss: 0.4800 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.47952\n",
      "Epoch 36/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4848 - categorical_accuracy: 0.7737 - val_loss: 0.4826 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.47952\n",
      "Epoch 37/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4843 - categorical_accuracy: 0.7736 - val_loss: 0.4792 - val_categorical_accuracy: 0.7771\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.47952 to 0.47922, saving model to best_model.h5\n",
      "Epoch 38/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4847 - categorical_accuracy: 0.7727 - val_loss: 0.4790 - val_categorical_accuracy: 0.7783\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.47922 to 0.47903, saving model to best_model.h5\n",
      "Epoch 39/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4848 - categorical_accuracy: 0.7739 - val_loss: 0.4787 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.47903 to 0.47869, saving model to best_model.h5\n",
      "Epoch 40/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4852 - categorical_accuracy: 0.7721 - val_loss: 0.4782 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.47869 to 0.47822, saving model to best_model.h5\n",
      "Epoch 41/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4841 - categorical_accuracy: 0.7748 - val_loss: 0.4782 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.47822 to 0.47821, saving model to best_model.h5\n",
      "Epoch 42/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4835 - categorical_accuracy: 0.7741 - val_loss: 0.4783 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.47821\n",
      "Epoch 43/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4839 - categorical_accuracy: 0.7737 - val_loss: 0.4787 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.47821\n",
      "Epoch 44/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4833 - categorical_accuracy: 0.7741 - val_loss: 0.4780 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.47821 to 0.47800, saving model to best_model.h5\n",
      "Epoch 45/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4833 - categorical_accuracy: 0.7737 - val_loss: 0.4784 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.47800\n",
      "Epoch 46/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4830 - categorical_accuracy: 0.7741 - val_loss: 0.4776 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.47800 to 0.47757, saving model to best_model.h5\n",
      "Epoch 47/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4831 - categorical_accuracy: 0.7733 - val_loss: 0.4786 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.47757\n",
      "Epoch 48/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4818 - categorical_accuracy: 0.7757 - val_loss: 0.4775 - val_categorical_accuracy: 0.7787\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.47757 to 0.47749, saving model to best_model.h5\n",
      "Epoch 49/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4831 - categorical_accuracy: 0.7734 - val_loss: 0.4780 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.47749\n",
      "Epoch 50/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4828 - categorical_accuracy: 0.7745 - val_loss: 0.4797 - val_categorical_accuracy: 0.7806\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.47749\n",
      "Epoch 51/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4821 - categorical_accuracy: 0.7746 - val_loss: 0.4804 - val_categorical_accuracy: 0.7766\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.47749\n",
      "Epoch 52/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4820 - categorical_accuracy: 0.7745 - val_loss: 0.4774 - val_categorical_accuracy: 0.7794\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.47749 to 0.47743, saving model to best_model.h5\n",
      "Epoch 53/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4817 - categorical_accuracy: 0.7750 - val_loss: 0.4771 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.47743 to 0.47711, saving model to best_model.h5\n",
      "Epoch 54/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4814 - categorical_accuracy: 0.7748 - val_loss: 0.4776 - val_categorical_accuracy: 0.7780\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.47711\n",
      "Epoch 55/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4814 - categorical_accuracy: 0.7748 - val_loss: 0.4777 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.47711\n",
      "Epoch 56/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4810 - categorical_accuracy: 0.7756 - val_loss: 0.4774 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.47711\n",
      "Epoch 57/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4810 - categorical_accuracy: 0.7743 - val_loss: 0.4770 - val_categorical_accuracy: 0.7792\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.47711 to 0.47704, saving model to best_model.h5\n",
      "Epoch 58/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4815 - categorical_accuracy: 0.7758 - val_loss: 0.4774 - val_categorical_accuracy: 0.7788\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.47704\n",
      "Epoch 59/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4804 - categorical_accuracy: 0.7756 - val_loss: 0.4766 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.47704 to 0.47660, saving model to best_model.h5\n",
      "Epoch 60/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4802 - categorical_accuracy: 0.7761 - val_loss: 0.4771 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.47660\n",
      "Epoch 61/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4805 - categorical_accuracy: 0.7761 - val_loss: 0.4765 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.47660 to 0.47649, saving model to best_model.h5\n",
      "Epoch 62/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4813 - categorical_accuracy: 0.7753 - val_loss: 0.4769 - val_categorical_accuracy: 0.7789\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.47649\n",
      "Epoch 63/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4793 - categorical_accuracy: 0.7755 - val_loss: 0.4766 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.47649\n",
      "Epoch 64/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4795 - categorical_accuracy: 0.7764 - val_loss: 0.4770 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.47649\n",
      "Epoch 65/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4795 - categorical_accuracy: 0.7761 - val_loss: 0.4769 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47649\n",
      "Epoch 66/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4798 - categorical_accuracy: 0.7763 - val_loss: 0.4765 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.47649 to 0.47648, saving model to best_model.h5\n",
      "Epoch 67/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4808 - categorical_accuracy: 0.7753 - val_loss: 0.4763 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.47648 to 0.47630, saving model to best_model.h5\n",
      "Epoch 68/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4798 - categorical_accuracy: 0.7756 - val_loss: 0.4773 - val_categorical_accuracy: 0.7784\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.47630\n",
      "Epoch 69/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4792 - categorical_accuracy: 0.7764 - val_loss: 0.4767 - val_categorical_accuracy: 0.7785\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47630\n",
      "Epoch 70/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4797 - categorical_accuracy: 0.7768 - val_loss: 0.4766 - val_categorical_accuracy: 0.7786\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47630\n",
      "Epoch 71/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4793 - categorical_accuracy: 0.7768 - val_loss: 0.4769 - val_categorical_accuracy: 0.7781\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47630\n",
      "Epoch 72/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4800 - categorical_accuracy: 0.7765 - val_loss: 0.4766 - val_categorical_accuracy: 0.7807\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.47630\n",
      "Epoch 73/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4794 - categorical_accuracy: 0.7761 - val_loss: 0.4762 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.47630 to 0.47619, saving model to best_model.h5\n",
      "Epoch 74/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4795 - categorical_accuracy: 0.7765 - val_loss: 0.4760 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.47619 to 0.47595, saving model to best_model.h5\n",
      "Epoch 75/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4794 - categorical_accuracy: 0.7750 - val_loss: 0.4778 - val_categorical_accuracy: 0.7773\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.47595\n",
      "Epoch 76/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4788 - categorical_accuracy: 0.7758 - val_loss: 0.4761 - val_categorical_accuracy: 0.7794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00076: val_loss did not improve from 0.47595\n",
      "Epoch 77/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4789 - categorical_accuracy: 0.7760 - val_loss: 0.4769 - val_categorical_accuracy: 0.7807\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.47595\n",
      "Epoch 78/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4786 - categorical_accuracy: 0.7752 - val_loss: 0.4761 - val_categorical_accuracy: 0.7798\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.47595\n",
      "Epoch 79/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4783 - categorical_accuracy: 0.7759 - val_loss: 0.4763 - val_categorical_accuracy: 0.7795\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.47595\n",
      "Epoch 80/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7767 - val_loss: 0.4759 - val_categorical_accuracy: 0.7791\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.47595 to 0.47592, saving model to best_model.h5\n",
      "Epoch 81/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4781 - categorical_accuracy: 0.7765 - val_loss: 0.4760 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.47592\n",
      "Epoch 82/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4783 - categorical_accuracy: 0.7765 - val_loss: 0.4759 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.47592\n",
      "Epoch 83/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7762 - val_loss: 0.4770 - val_categorical_accuracy: 0.7776\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.47592\n",
      "Epoch 84/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4777 - categorical_accuracy: 0.7762 - val_loss: 0.4763 - val_categorical_accuracy: 0.7777\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.47592\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.00900000035762787.\n",
      "Epoch 85/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4771 - categorical_accuracy: 0.7776 - val_loss: 0.4756 - val_categorical_accuracy: 0.7793\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.47592 to 0.47556, saving model to best_model.h5\n",
      "Epoch 86/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4777 - categorical_accuracy: 0.7769 - val_loss: 0.4755 - val_categorical_accuracy: 0.7796\n",
      "\n",
      "Epoch 00086: val_loss improved from 0.47556 to 0.47551, saving model to best_model.h5\n",
      "Epoch 87/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4775 - categorical_accuracy: 0.7770 - val_loss: 0.4754 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.47551 to 0.47543, saving model to best_model.h5\n",
      "Epoch 88/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4769 - categorical_accuracy: 0.7776 - val_loss: 0.4754 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.47543\n",
      "Epoch 89/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4773 - categorical_accuracy: 0.7775 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.47543 to 0.47543, saving model to best_model.h5\n",
      "Epoch 90/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4770 - categorical_accuracy: 0.7773 - val_loss: 0.4755 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.47543\n",
      "Epoch 91/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4774 - categorical_accuracy: 0.7768 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.47543\n",
      "Epoch 92/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4770 - categorical_accuracy: 0.7772 - val_loss: 0.4754 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.47543\n",
      "Epoch 93/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4769 - categorical_accuracy: 0.7778 - val_loss: 0.4754 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.47543 to 0.47541, saving model to best_model.h5\n",
      "Epoch 94/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4779 - categorical_accuracy: 0.7774 - val_loss: 0.4755 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.47541\n",
      "Epoch 95/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4769 - categorical_accuracy: 0.7779 - val_loss: 0.4755 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.47541\n",
      "Epoch 96/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4768 - categorical_accuracy: 0.7783 - val_loss: 0.4755 - val_categorical_accuracy: 0.7797\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.47541\n",
      "Epoch 97/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4776 - categorical_accuracy: 0.7771 - val_loss: 0.4754 - val_categorical_accuracy: 0.7799\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.47541\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.0009000000543892384.\n",
      "Epoch 98/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4773 - categorical_accuracy: 0.7770 - val_loss: 0.4754 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.47541 to 0.47541, saving model to best_model.h5\n",
      "Epoch 99/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4764 - categorical_accuracy: 0.7776 - val_loss: 0.4754 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.47541\n",
      "Epoch 100/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4767 - categorical_accuracy: 0.7776 - val_loss: 0.4754 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.47541 to 0.47540, saving model to best_model.h5\n",
      "Epoch 101/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4776 - categorical_accuracy: 0.7774 - val_loss: 0.4754 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.47540 to 0.47540, saving model to best_model.h5\n",
      "Epoch 102/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4768 - categorical_accuracy: 0.7772 - val_loss: 0.4754 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.47540 to 0.47540, saving model to best_model.h5\n",
      "Epoch 103/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4769 - categorical_accuracy: 0.7773 - val_loss: 0.4754 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.47540 to 0.47539, saving model to best_model.h5\n",
      "Epoch 104/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4764 - categorical_accuracy: 0.7776 - val_loss: 0.4754 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.47539\n",
      "Epoch 105/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4765 - categorical_accuracy: 0.7779 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.47539\n",
      "Epoch 106/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4768 - categorical_accuracy: 0.7778 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.47539 to 0.47539, saving model to best_model.h5\n",
      "Epoch 107/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4763 - categorical_accuracy: 0.7778 - val_loss: 0.4754 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.47539\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 108/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4768 - categorical_accuracy: 0.7781 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.47539\n",
      "Epoch 109/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4772 - categorical_accuracy: 0.7768 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.47539\n",
      "Epoch 110/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4771 - categorical_accuracy: 0.7775 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.47539 to 0.47538, saving model to best_model.h5\n",
      "Epoch 111/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4766 - categorical_accuracy: 0.7780 - val_loss: 0.4754 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.47538\n",
      "Epoch 112/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4768 - categorical_accuracy: 0.7784 - val_loss: 0.4754 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.47538 to 0.47538, saving model to best_model.h5\n",
      "Epoch 113/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4764 - categorical_accuracy: 0.7776 - val_loss: 0.4754 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.47538\n",
      "Epoch 114/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4774 - categorical_accuracy: 0.7771 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.47538\n",
      "Epoch 115/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4768 - categorical_accuracy: 0.7773 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.47538\n",
      "Epoch 116/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4769 - categorical_accuracy: 0.7781 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.47538\n",
      "Epoch 117/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4772 - categorical_accuracy: 0.7768 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.47538\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 9.000000136438758e-06.\n",
      "Epoch 118/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4770 - categorical_accuracy: 0.7769 - val_loss: 0.4754 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.47538\n",
      "Epoch 119/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4769 - categorical_accuracy: 0.7772 - val_loss: 0.4754 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.47538\n",
      "Epoch 120/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4769 - categorical_accuracy: 0.7779 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.47538\n",
      "Epoch 121/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4765 - categorical_accuracy: 0.7773 - val_loss: 0.4754 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.47538\n",
      "Epoch 122/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4767 - categorical_accuracy: 0.7781 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.47538\n",
      "Epoch 123/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4773 - categorical_accuracy: 0.7768 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.47538\n",
      "Epoch 124/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4771 - categorical_accuracy: 0.7769 - val_loss: 0.4754 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.47538\n",
      "Epoch 125/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4765 - categorical_accuracy: 0.7776 - val_loss: 0.4754 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.47538\n",
      "Epoch 126/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4767 - categorical_accuracy: 0.7771 - val_loss: 0.4754 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.47538\n",
      "Epoch 127/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4775 - categorical_accuracy: 0.7767 - val_loss: 0.4754 - val_categorical_accuracy: 0.7803\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.47538\n",
      "\n",
      "Epoch 00127: ReduceLROnPlateau reducing learning rate to 9.000000318337698e-07.\n",
      "Epoch 128/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4771 - categorical_accuracy: 0.7780 - val_loss: 0.4754 - val_categorical_accuracy: 0.7801\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.47538\n",
      "Epoch 129/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4772 - categorical_accuracy: 0.7777 - val_loss: 0.4754 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.47538\n",
      "Epoch 130/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4772 - categorical_accuracy: 0.7772 - val_loss: 0.4754 - val_categorical_accuracy: 0.7804\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.47538\n",
      "Epoch 131/250\n",
      "87011/87011 [==============================] - 2s 25us/step - loss: 0.4773 - categorical_accuracy: 0.7769 - val_loss: 0.4754 - val_categorical_accuracy: 0.7802\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.47538\n",
      "Epoch 132/250\n",
      "87011/87011 [==============================] - 2s 26us/step - loss: 0.4773 - categorical_accuracy: 0.7776 - val_loss: 0.4754 - val_categorical_accuracy: 0.7804\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.47538\n",
      "Epoch 00132: early stopping\n",
      "the 4 fold Log-Loss (NN) is 0.475382\n",
      "-----------\n",
      "Loop 2/2 Fold 5/5\n",
      "-----------\n",
      "Train on 87012 samples, validate on 21752 samples\n",
      "Epoch 1/250\n",
      "87012/87012 [==============================] - 3s 33us/step - loss: 0.9817 - categorical_accuracy: 0.7252 - val_loss: 1.6653 - val_categorical_accuracy: 0.3640\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.66532, saving model to best_model.h5\n",
      "Epoch 2/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.5182 - categorical_accuracy: 0.7558 - val_loss: 0.5754 - val_categorical_accuracy: 0.7280\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.66532 to 0.57544, saving model to best_model.h5\n",
      "Epoch 3/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.5098 - categorical_accuracy: 0.7585 - val_loss: 0.5144 - val_categorical_accuracy: 0.7622\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57544 to 0.51439, saving model to best_model.h5\n",
      "Epoch 4/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.5055 - categorical_accuracy: 0.7601 - val_loss: 0.5127 - val_categorical_accuracy: 0.7584\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.51439 to 0.51271, saving model to best_model.h5\n",
      "Epoch 5/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.5040 - categorical_accuracy: 0.7605 - val_loss: 0.5075 - val_categorical_accuracy: 0.7591\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51271 to 0.50749, saving model to best_model.h5\n",
      "Epoch 6/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4994 - categorical_accuracy: 0.7634 - val_loss: 0.5065 - val_categorical_accuracy: 0.7630\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.50749 to 0.50649, saving model to best_model.h5\n",
      "Epoch 7/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4991 - categorical_accuracy: 0.7643 - val_loss: 0.4998 - val_categorical_accuracy: 0.7648\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.50649 to 0.49983, saving model to best_model.h5\n",
      "Epoch 8/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4973 - categorical_accuracy: 0.7637 - val_loss: 0.5004 - val_categorical_accuracy: 0.7645\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.49983\n",
      "Epoch 9/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4961 - categorical_accuracy: 0.7654 - val_loss: 0.4966 - val_categorical_accuracy: 0.7676\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.49983 to 0.49662, saving model to best_model.h5\n",
      "Epoch 10/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4944 - categorical_accuracy: 0.7678 - val_loss: 0.4991 - val_categorical_accuracy: 0.7668\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.49662\n",
      "Epoch 11/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4937 - categorical_accuracy: 0.7673 - val_loss: 0.4965 - val_categorical_accuracy: 0.7683\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.49662 to 0.49646, saving model to best_model.h5\n",
      "Epoch 12/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4937 - categorical_accuracy: 0.7685 - val_loss: 0.4931 - val_categorical_accuracy: 0.7683\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.49646 to 0.49315, saving model to best_model.h5\n",
      "Epoch 13/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4936 - categorical_accuracy: 0.7682 - val_loss: 0.4971 - val_categorical_accuracy: 0.7659\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49315\n",
      "Epoch 14/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4923 - categorical_accuracy: 0.7686 - val_loss: 0.4942 - val_categorical_accuracy: 0.7698\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.49315\n",
      "Epoch 15/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4921 - categorical_accuracy: 0.7695 - val_loss: 0.4918 - val_categorical_accuracy: 0.7705\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.49315 to 0.49184, saving model to best_model.h5\n",
      "Epoch 16/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4916 - categorical_accuracy: 0.7701 - val_loss: 0.4941 - val_categorical_accuracy: 0.7704\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.49184\n",
      "Epoch 17/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4906 - categorical_accuracy: 0.7696 - val_loss: 0.4916 - val_categorical_accuracy: 0.7715\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.49184 to 0.49159, saving model to best_model.h5\n",
      "Epoch 18/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4900 - categorical_accuracy: 0.7710 - val_loss: 0.4916 - val_categorical_accuracy: 0.7716\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.49159\n",
      "Epoch 19/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4892 - categorical_accuracy: 0.7710 - val_loss: 0.4909 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.49159 to 0.49086, saving model to best_model.h5\n",
      "Epoch 20/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4892 - categorical_accuracy: 0.7715 - val_loss: 0.4901 - val_categorical_accuracy: 0.7708\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.49086 to 0.49008, saving model to best_model.h5\n",
      "Epoch 21/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4880 - categorical_accuracy: 0.7712 - val_loss: 0.4966 - val_categorical_accuracy: 0.7671\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.49008\n",
      "Epoch 22/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4874 - categorical_accuracy: 0.7727 - val_loss: 0.4941 - val_categorical_accuracy: 0.7699\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.49008\n",
      "Epoch 23/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4876 - categorical_accuracy: 0.7718 - val_loss: 0.4894 - val_categorical_accuracy: 0.7729\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.49008 to 0.48938, saving model to best_model.h5\n",
      "Epoch 24/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4875 - categorical_accuracy: 0.7726 - val_loss: 0.4891 - val_categorical_accuracy: 0.7725\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.48938 to 0.48913, saving model to best_model.h5\n",
      "Epoch 25/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4873 - categorical_accuracy: 0.7730 - val_loss: 0.4896 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.48913\n",
      "Epoch 26/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4864 - categorical_accuracy: 0.7731 - val_loss: 0.4910 - val_categorical_accuracy: 0.7719\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.48913\n",
      "Epoch 27/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4872 - categorical_accuracy: 0.7720 - val_loss: 0.4893 - val_categorical_accuracy: 0.7733\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.48913\n",
      "Epoch 28/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4869 - categorical_accuracy: 0.7725 - val_loss: 0.4890 - val_categorical_accuracy: 0.7731\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.48913 to 0.48902, saving model to best_model.h5\n",
      "Epoch 29/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4866 - categorical_accuracy: 0.7727 - val_loss: 0.4906 - val_categorical_accuracy: 0.7729\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.48902\n",
      "Epoch 30/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4867 - categorical_accuracy: 0.7726 - val_loss: 0.4871 - val_categorical_accuracy: 0.7738\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.48902 to 0.48708, saving model to best_model.h5\n",
      "Epoch 31/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4854 - categorical_accuracy: 0.7738 - val_loss: 0.4911 - val_categorical_accuracy: 0.7697\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.48708\n",
      "Epoch 32/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4849 - categorical_accuracy: 0.7738 - val_loss: 0.4863 - val_categorical_accuracy: 0.7740\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.48708 to 0.48634, saving model to best_model.h5\n",
      "Epoch 33/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4847 - categorical_accuracy: 0.7732 - val_loss: 0.4872 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.48634\n",
      "Epoch 34/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4845 - categorical_accuracy: 0.7743 - val_loss: 0.4867 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.48634\n",
      "Epoch 35/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4838 - categorical_accuracy: 0.7756 - val_loss: 0.4870 - val_categorical_accuracy: 0.7729\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.48634\n",
      "Epoch 36/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4840 - categorical_accuracy: 0.7746 - val_loss: 0.4856 - val_categorical_accuracy: 0.7736\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.48634 to 0.48559, saving model to best_model.h5\n",
      "Epoch 37/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4834 - categorical_accuracy: 0.7747 - val_loss: 0.4849 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.48559 to 0.48494, saving model to best_model.h5\n",
      "Epoch 38/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4838 - categorical_accuracy: 0.7753 - val_loss: 0.4857 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.48494\n",
      "Epoch 39/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4833 - categorical_accuracy: 0.7744 - val_loss: 0.4920 - val_categorical_accuracy: 0.7723\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.48494\n",
      "Epoch 40/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4832 - categorical_accuracy: 0.7751 - val_loss: 0.4845 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.48494 to 0.48451, saving model to best_model.h5\n",
      "Epoch 41/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4826 - categorical_accuracy: 0.7749 - val_loss: 0.4846 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.48451\n",
      "Epoch 42/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4822 - categorical_accuracy: 0.7757 - val_loss: 0.4851 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.48451\n",
      "Epoch 43/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4829 - categorical_accuracy: 0.7749 - val_loss: 0.4847 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.48451\n",
      "Epoch 44/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4821 - categorical_accuracy: 0.7760 - val_loss: 0.4875 - val_categorical_accuracy: 0.7716\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.48451\n",
      "Epoch 45/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4821 - categorical_accuracy: 0.7749 - val_loss: 0.4828 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.48451 to 0.48282, saving model to best_model.h5\n",
      "Epoch 46/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4820 - categorical_accuracy: 0.7755 - val_loss: 0.4833 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.48282\n",
      "Epoch 47/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4827 - categorical_accuracy: 0.7748 - val_loss: 0.4881 - val_categorical_accuracy: 0.7725\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.48282\n",
      "Epoch 48/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4821 - categorical_accuracy: 0.7759 - val_loss: 0.4829 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.48282\n",
      "Epoch 49/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4808 - categorical_accuracy: 0.7760 - val_loss: 0.4831 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.48282\n",
      "Epoch 50/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4810 - categorical_accuracy: 0.7760 - val_loss: 0.4837 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.48282\n",
      "Epoch 51/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4809 - categorical_accuracy: 0.7760 - val_loss: 0.4829 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.48282\n",
      "Epoch 52/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4805 - categorical_accuracy: 0.7766 - val_loss: 0.4819 - val_categorical_accuracy: 0.7758\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.48282 to 0.48194, saving model to best_model.h5\n",
      "Epoch 53/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4805 - categorical_accuracy: 0.7766 - val_loss: 0.4826 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.48194\n",
      "Epoch 54/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4800 - categorical_accuracy: 0.7770 - val_loss: 0.4850 - val_categorical_accuracy: 0.7740\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.48194\n",
      "Epoch 55/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4801 - categorical_accuracy: 0.7765 - val_loss: 0.4852 - val_categorical_accuracy: 0.7743\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.48194\n",
      "Epoch 56/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4797 - categorical_accuracy: 0.7771 - val_loss: 0.4819 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.48194\n",
      "Epoch 57/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4802 - categorical_accuracy: 0.7762 - val_loss: 0.4835 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.48194\n",
      "Epoch 58/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4801 - categorical_accuracy: 0.7769 - val_loss: 0.4827 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.48194\n",
      "Epoch 59/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4799 - categorical_accuracy: 0.7775 - val_loss: 0.4848 - val_categorical_accuracy: 0.7734\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.48194\n",
      "Epoch 60/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4790 - categorical_accuracy: 0.7771 - val_loss: 0.4837 - val_categorical_accuracy: 0.7735\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.48194\n",
      "Epoch 61/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4797 - categorical_accuracy: 0.7767 - val_loss: 0.4820 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.48194\n",
      "Epoch 62/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4791 - categorical_accuracy: 0.7775 - val_loss: 0.4815 - val_categorical_accuracy: 0.7759\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.48194 to 0.48149, saving model to best_model.h5\n",
      "Epoch 63/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4789 - categorical_accuracy: 0.7767 - val_loss: 0.4818 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.48149\n",
      "Epoch 64/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4790 - categorical_accuracy: 0.7772 - val_loss: 0.4829 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.48149\n",
      "Epoch 65/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4784 - categorical_accuracy: 0.7770 - val_loss: 0.4836 - val_categorical_accuracy: 0.7738\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.48149\n",
      "Epoch 66/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4796 - categorical_accuracy: 0.7761 - val_loss: 0.4819 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.48149\n",
      "Epoch 67/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4787 - categorical_accuracy: 0.7778 - val_loss: 0.4810 - val_categorical_accuracy: 0.7764\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.48149 to 0.48098, saving model to best_model.h5\n",
      "Epoch 68/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4785 - categorical_accuracy: 0.7770 - val_loss: 0.4816 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.48098\n",
      "Epoch 69/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4788 - categorical_accuracy: 0.7765 - val_loss: 0.4812 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.48098\n",
      "Epoch 70/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7784 - val_loss: 0.4820 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.48098\n",
      "Epoch 71/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4782 - categorical_accuracy: 0.7770 - val_loss: 0.4812 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.48098\n",
      "Epoch 72/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4776 - categorical_accuracy: 0.7781 - val_loss: 0.4827 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.48098\n",
      "Epoch 73/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4783 - categorical_accuracy: 0.7772 - val_loss: 0.4815 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.48098\n",
      "Epoch 74/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4785 - categorical_accuracy: 0.7771 - val_loss: 0.4826 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.48098\n",
      "Epoch 75/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4772 - categorical_accuracy: 0.7783 - val_loss: 0.4805 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.48098 to 0.48049, saving model to best_model.h5\n",
      "Epoch 76/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4779 - categorical_accuracy: 0.7781 - val_loss: 0.4813 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.48049\n",
      "Epoch 77/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4775 - categorical_accuracy: 0.7784 - val_loss: 0.4817 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.48049\n",
      "Epoch 78/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4781 - categorical_accuracy: 0.7780 - val_loss: 0.4811 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.48049\n",
      "Epoch 79/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4769 - categorical_accuracy: 0.7788 - val_loss: 0.4819 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.48049\n",
      "Epoch 80/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4769 - categorical_accuracy: 0.7778 - val_loss: 0.4818 - val_categorical_accuracy: 0.7750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00080: val_loss did not improve from 0.48049\n",
      "Epoch 81/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4778 - categorical_accuracy: 0.7773 - val_loss: 0.4806 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.48049\n",
      "Epoch 82/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4765 - categorical_accuracy: 0.7781 - val_loss: 0.4805 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.48049 to 0.48048, saving model to best_model.h5\n",
      "Epoch 83/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4774 - categorical_accuracy: 0.7775 - val_loss: 0.4813 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.48048\n",
      "Epoch 84/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4767 - categorical_accuracy: 0.7783 - val_loss: 0.4806 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.48048\n",
      "Epoch 85/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4768 - categorical_accuracy: 0.7787 - val_loss: 0.4803 - val_categorical_accuracy: 0.7762\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.48048 to 0.48029, saving model to best_model.h5\n",
      "Epoch 86/250\n",
      "87012/87012 [==============================] - 2s 27us/step - loss: 0.4768 - categorical_accuracy: 0.7776 - val_loss: 0.4811 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.48029\n",
      "Epoch 87/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4767 - categorical_accuracy: 0.7781 - val_loss: 0.4812 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.48029\n",
      "Epoch 88/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4758 - categorical_accuracy: 0.7781 - val_loss: 0.4824 - val_categorical_accuracy: 0.7740\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.48029\n",
      "Epoch 89/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4762 - categorical_accuracy: 0.7782 - val_loss: 0.4806 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.48029\n",
      "Epoch 90/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4760 - categorical_accuracy: 0.7783 - val_loss: 0.4836 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.48029\n",
      "Epoch 91/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4767 - categorical_accuracy: 0.7781 - val_loss: 0.4805 - val_categorical_accuracy: 0.7755\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.48029\n",
      "Epoch 92/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4761 - categorical_accuracy: 0.7778 - val_loss: 0.4803 - val_categorical_accuracy: 0.7756\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.48029\n",
      "Epoch 93/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4764 - categorical_accuracy: 0.7787 - val_loss: 0.4805 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.48029\n",
      "Epoch 94/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4757 - categorical_accuracy: 0.7781 - val_loss: 0.4806 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.48029\n",
      "Epoch 95/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4762 - categorical_accuracy: 0.7796 - val_loss: 0.4801 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.48029 to 0.48013, saving model to best_model.h5\n",
      "Epoch 96/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4754 - categorical_accuracy: 0.7788 - val_loss: 0.4806 - val_categorical_accuracy: 0.7757\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.48013\n",
      "Epoch 97/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4748 - categorical_accuracy: 0.7788 - val_loss: 0.4804 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.48013\n",
      "Epoch 98/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4760 - categorical_accuracy: 0.7782 - val_loss: 0.4801 - val_categorical_accuracy: 0.7760\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.48013 to 0.48006, saving model to best_model.h5\n",
      "Epoch 99/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4750 - categorical_accuracy: 0.7786 - val_loss: 0.4805 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.48006\n",
      "Epoch 100/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4757 - categorical_accuracy: 0.7785 - val_loss: 0.4803 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.48006\n",
      "Epoch 101/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4750 - categorical_accuracy: 0.7787 - val_loss: 0.4810 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.48006\n",
      "Epoch 102/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4752 - categorical_accuracy: 0.7778 - val_loss: 0.4812 - val_categorical_accuracy: 0.7744\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.48006\n",
      "Epoch 103/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4746 - categorical_accuracy: 0.7788 - val_loss: 0.4808 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.48006\n",
      "Epoch 104/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4751 - categorical_accuracy: 0.7784 - val_loss: 0.4803 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.48006\n",
      "Epoch 105/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4756 - categorical_accuracy: 0.7789 - val_loss: 0.4813 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.48006\n",
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 0.00900000035762787.\n",
      "Epoch 106/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4752 - categorical_accuracy: 0.7786 - val_loss: 0.4805 - val_categorical_accuracy: 0.7742\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.48006\n",
      "Epoch 107/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4750 - categorical_accuracy: 0.7793 - val_loss: 0.4802 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.48006\n",
      "Epoch 108/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4743 - categorical_accuracy: 0.7801 - val_loss: 0.4800 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.48006 to 0.47997, saving model to best_model.h5\n",
      "Epoch 109/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4744 - categorical_accuracy: 0.7799 - val_loss: 0.4800 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.47997\n",
      "Epoch 110/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4740 - categorical_accuracy: 0.7797 - val_loss: 0.4803 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.47997\n",
      "Epoch 111/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4742 - categorical_accuracy: 0.7795 - val_loss: 0.4799 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.47997 to 0.47990, saving model to best_model.h5\n",
      "Epoch 112/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4744 - categorical_accuracy: 0.7790 - val_loss: 0.4805 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.47990\n",
      "Epoch 113/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4746 - categorical_accuracy: 0.7790 - val_loss: 0.4805 - val_categorical_accuracy: 0.7741\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.47990\n",
      "Epoch 114/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4750 - categorical_accuracy: 0.7785 - val_loss: 0.4802 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.47990\n",
      "Epoch 115/250\n",
      "87012/87012 [==============================] - 2s 28us/step - loss: 0.4739 - categorical_accuracy: 0.7798 - val_loss: 0.4800 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.47990\n",
      "Epoch 116/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4747 - categorical_accuracy: 0.7800 - val_loss: 0.4800 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.47990\n",
      "Epoch 117/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4741 - categorical_accuracy: 0.7800 - val_loss: 0.4798 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00117: val_loss improved from 0.47990 to 0.47976, saving model to best_model.h5\n",
      "Epoch 118/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4745 - categorical_accuracy: 0.7793 - val_loss: 0.4801 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.47976\n",
      "Epoch 119/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4748 - categorical_accuracy: 0.7792 - val_loss: 0.4802 - val_categorical_accuracy: 0.7747\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.47976\n",
      "Epoch 120/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4740 - categorical_accuracy: 0.7799 - val_loss: 0.4804 - val_categorical_accuracy: 0.7745\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.47976\n",
      "Epoch 121/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4746 - categorical_accuracy: 0.7794 - val_loss: 0.4800 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.47976\n",
      "Epoch 122/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4740 - categorical_accuracy: 0.7803 - val_loss: 0.4802 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.47976\n",
      "Epoch 123/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4742 - categorical_accuracy: 0.7794 - val_loss: 0.4800 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.47976\n",
      "Epoch 124/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4744 - categorical_accuracy: 0.7800 - val_loss: 0.4801 - val_categorical_accuracy: 0.7746\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.47976\n",
      "Epoch 125/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4733 - categorical_accuracy: 0.7809 - val_loss: 0.4798 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.47976\n",
      "Epoch 126/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4746 - categorical_accuracy: 0.7794 - val_loss: 0.4800 - val_categorical_accuracy: 0.7748\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.47976\n",
      "Epoch 127/250\n",
      "87012/87012 [==============================] - 3s 33us/step - loss: 0.4742 - categorical_accuracy: 0.7801 - val_loss: 0.4799 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.47976\n",
      "\n",
      "Epoch 00127: ReduceLROnPlateau reducing learning rate to 0.0009000000543892384.\n",
      "Epoch 128/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4744 - categorical_accuracy: 0.7788 - val_loss: 0.4799 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.47976\n",
      "Epoch 129/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4734 - categorical_accuracy: 0.7812 - val_loss: 0.4799 - val_categorical_accuracy: 0.7754\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.47976\n",
      "Epoch 130/250\n",
      "87012/87012 [==============================] - 2s 26us/step - loss: 0.4745 - categorical_accuracy: 0.7802 - val_loss: 0.4799 - val_categorical_accuracy: 0.7752\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.47976\n",
      "Epoch 131/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4737 - categorical_accuracy: 0.7794 - val_loss: 0.4800 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.47976\n",
      "Epoch 132/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4741 - categorical_accuracy: 0.7791 - val_loss: 0.4800 - val_categorical_accuracy: 0.7749\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.47976\n",
      "Epoch 133/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4732 - categorical_accuracy: 0.7804 - val_loss: 0.4800 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.47976\n",
      "Epoch 134/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4744 - categorical_accuracy: 0.7785 - val_loss: 0.4800 - val_categorical_accuracy: 0.7753\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.47976\n",
      "Epoch 135/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4745 - categorical_accuracy: 0.7784 - val_loss: 0.4799 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.47976\n",
      "Epoch 136/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4740 - categorical_accuracy: 0.7790 - val_loss: 0.4799 - val_categorical_accuracy: 0.7751\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.47976\n",
      "Epoch 137/250\n",
      "87012/87012 [==============================] - 2s 25us/step - loss: 0.4744 - categorical_accuracy: 0.7786 - val_loss: 0.4799 - val_categorical_accuracy: 0.7750\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.47976\n",
      "\n",
      "Epoch 00137: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 00137: early stopping\n",
      "the 5 fold Log-Loss (NN) is 0.479758\n",
      "PARTIAL: mean Log-Loss (NN) is 0.478498\n",
      "CPU times: user 6h 11min 23s, sys: 6min 5s, total: 6h 17min 28s\n",
      "Wall time: 51min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Bloco para executar a rede neural a cada passada do KFold\n",
    "# Vamos realizar 2 loops com 5 kfolds e apurar a média\n",
    "loop = 2\n",
    "fold = 5\n",
    "\n",
    "# Definindo listas que serão preenchidas durante o loop for\n",
    "oof_nn = np.zeros([loop, train_y.shape[0], train_y.shape[1]])\n",
    "models_nn = []\n",
    "logloss_csv_nn = []\n",
    "\n",
    "# Treinando o modelo\n",
    "for k in range(loop):\n",
    "    kfold = KFold(fold, random_state = 42 + k, shuffle = True)\n",
    "    for k_fold, (tr_inds, val_inds) in enumerate(kfold.split(train_y)):\n",
    "        print(\"-----------\")\n",
    "        print(f'Loop {k+1}/{loop}' + f' Fold {k_fold+1}/{fold}')\n",
    "        print(\"-----------\")\n",
    "        \n",
    "        tr_x, tr_y = train_x[tr_inds], train_y[tr_inds]\n",
    "        val_x, val_y = train_x[val_inds], train_y[val_inds]\n",
    "        \n",
    "        # Train NN\n",
    "        nn, logloss_nn = get_nn(tr_x, tr_y, val_x, val_y, shape=val_x.shape[0])\n",
    "        models_nn.append(nn)\n",
    "        print(\"the %d fold Log-Loss (NN) is %f\"%((k_fold+1), logloss_nn))\n",
    "        logloss_csv_nn.append(logloss_nn)\n",
    "        \n",
    "        #Predict OOF\n",
    "        oof_nn[k, val_inds, :] = nn.predict(val_x)\n",
    "        \n",
    "    print(\"PARTIAL: mean Log-Loss (NN) is %f\"%np.mean(logloss_csv_nn))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média log-loss:  0.478498\n",
      "Média OOF log-loss: 0.478498\n"
     ]
    }
   ],
   "source": [
    "# Verificando o resultado médio do Log Loss para cada passada do Kfold\n",
    "loss_oof_nn = []\n",
    "\n",
    "for k in range(loop):\n",
    "    loss_oof_nn.append(log_loss(train_y, oof_nn[k,...], eps=1e-15))\n",
    "    \n",
    "print(\"Média log-loss:  %f\"%np.mean(logloss_csv_nn))\n",
    "print(\"Média OOF log-loss: %f\"%np.mean(loss_oof_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apenas para acompanhar o resultado visual\n",
    "# Exibir o treinamento somente do primeiro kfold\n",
    "plt.figure(figsize=(18, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(models_nn[0].history.history[\"loss\"], \"o-\", alpha=.9, label=\"loss\")\n",
    "plt.plot(models_nn[0].history.history[\"val_loss\"], \"o-\", alpha=.9, label=\"val_loss\")\n",
    "plt.axhline(1, linestyle=\"--\", c=\"C2\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(models_nn[0].history.history[\"categorical_accuracy\"], \"o-\", alpha=.9, label=\"accuracy\")\n",
    "plt.plot(models_nn[0].history.history[\"val_categorical_accuracy\"], \"o-\", alpha=.9, label=\"val_accuracy\")\n",
    "plt.axhline(.7, linestyle=\"--\", c=\"C2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um dataset somente com as colunas mais importantes conforme Feature Selection\n",
    "new_test = teste.loc[:,best_features['Feature']]\n",
    "\n",
    "# Padronizando os dados de treino\n",
    "new_test = scaler.fit_transform(new_test)\n",
    "\n",
    "new_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcao para realizar as previsoes baseado em todos os modelos do Kfold\n",
    "def predict_proba(model, x, batch_size=32, verbose=0):\n",
    "    preds = model.predict(x, batch_size, verbose)\n",
    "    if preds.min() < 0. or preds.max() > 1.:\n",
    "        warnings.warn('Network returning invalid probability values.')\n",
    "    return preds\n",
    "\n",
    "def predict(x_te, models_nn):\n",
    "    model_num_nn = len(models_nn)\n",
    "\n",
    "    for k,m in enumerate(models_nn):\n",
    "        if k==0:\n",
    "            y_pred_nn = predict_proba(m, x_te, batch_size=1024)\n",
    "        else:\n",
    "            y_pred_nn += predict_proba(m, x_te, batch_size=1024)\n",
    "            \n",
    "    y_pred_nn = y_pred_nn / model_num_nn\n",
    "    return y_pred_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando as previsões no dataset de teste\n",
    "test_pred = predict(new_test, models_nn)\n",
    "test_pred[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o dataset de exemplo de submission e carrega as previsões das probabilidades\n",
    "#submission = pd.read_csv('/kaggle/input/competicao-dsa-machine-learning-dec-2019/sample_submission.csv')\n",
    "submission = pd.read_csv('../dataset/sample_submission.csv')\n",
    "submission['PredictedProb'] = test_pred[:,1]\n",
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera o arquivo de saída para submeter no Kaggle\n",
    "submission.to_csv('../submission/submission_nn_v1.0.2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apenas para visualizar a distribuição das previsões\n",
    "submission['PredictedProb'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma com as previsões\n",
    "plt.hist(submission.PredictedProb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Continua...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03f293c5aaf34fd7a50a3b2bf14dab7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0af695c1d5e24a17b3a2aebc133ddd39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6782fd7c22664c55904d66122a64843b",
        "IPY_MODEL_9163f58ecd4f4a84922abc60cc69d789"
       ],
       "layout": "IPY_MODEL_d8264eeec6af442097559d8923081902"
      }
     },
     "1c88767c318e4d86a9ab43245f5daa77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "219092655bed428f83aeb751ccaa2a4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "44521ec2de994d5186cdf7d1e445d158": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6782fd7c22664c55904d66122a64843b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c5e6acfe3f654e7ca86e4cddf862e984",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_219092655bed428f83aeb751ccaa2a4d",
       "value": 1000
      }
     },
     "9163f58ecd4f4a84922abc60cc69d789": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9a5c32201e5c447b829c604d757794df",
       "placeholder": "​",
       "style": "IPY_MODEL_b90e11ceb23b4a84a2f4908689dff66f",
       "value": " 1000/1000 [01:50&lt;00:00,  9.06it/s]"
      }
     },
     "9a5c32201e5c447b829c604d757794df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a8bfc018378d49efa1570fcb43b496f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b90e11ceb23b4a84a2f4908689dff66f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c5e6acfe3f654e7ca86e4cddf862e984": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c95734415f404758ba787c86b58e7cc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d0cb9d8667c340c4b49fb7cf32acaf7c",
        "IPY_MODEL_e1aabb08150346cc9871c9f6b3bb5d36"
       ],
       "layout": "IPY_MODEL_1c88767c318e4d86a9ab43245f5daa77"
      }
     },
     "d0cb9d8667c340c4b49fb7cf32acaf7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_44521ec2de994d5186cdf7d1e445d158",
       "max": 17000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_eec34a1f17e2477fb10e7c11419a1382",
       "value": 17000
      }
     },
     "d8264eeec6af442097559d8923081902": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1aabb08150346cc9871c9f6b3bb5d36": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a8bfc018378d49efa1570fcb43b496f3",
       "placeholder": "​",
       "style": "IPY_MODEL_03f293c5aaf34fd7a50a3b2bf14dab7d",
       "value": " 17000/17000 [11:01&lt;00:00, 25.68it/s]"
      }
     },
     "eec34a1f17e2477fb10e7c11419a1382": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
