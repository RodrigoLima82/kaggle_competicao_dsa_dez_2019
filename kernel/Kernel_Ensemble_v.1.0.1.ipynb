{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle\n",
    "## Competição DSA de Machine Learning - Dezembro 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versão 1.0.9: LB = ??? CV = 0.464466 / CV com calibration = \n",
    "- modelo: LightGBM (com algumas otimizações)\n",
    "- features engineering: gerado pelo Sandro\n",
    "- usando calibrateCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Import libraries to store data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Import libraries to visualize data\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, interact_manual\n",
    "# Import libraries to process data\n",
    "import tsfresh\n",
    "from scipy.signal import welch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import libraries to classify data and score results\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import xgboost as xgb\n",
    "# Import libraries used in functions and for feedback\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Setting values on a Pandas DataFrame \n",
    "# sometimes throws errors, so I'll silence the warnings.\n",
    "\n",
    "# The below has to be set because matplotlib and XGB crash \n",
    "# by causing a duplicate copy of a library to be run. It's a \n",
    "# known issue, unfortunately.\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar os principais pacotes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "# Evitar que aparece os warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Seta algumas opções no Jupyter para exibição dos datasets\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "# Variavel para controlar o treinamento no Kaggle\n",
    "TRAIN_OFFLINE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa os pacotes de algoritmos\n",
    "import lightgbm as lgb\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "# Importa pacotes do sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, StandardScaler\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Importa pacotes do sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando arquivo dataset_treino.csv....\n",
      "dataset_treino.csv tem 114321 linhas and 133 colunas\n",
      "Carregando arquivo dataset_teste.csv....\n",
      "dataset_teste.csv tem 114393 linhas and 132 colunas\n"
     ]
    }
   ],
   "source": [
    "def read_data():\n",
    "    \n",
    "    print('Carregando arquivo dataset_treino.csv....')\n",
    "    train = pd.read_csv('../dataset/dataset_treino.csv')\n",
    "    print('dataset_treino.csv tem {} linhas and {} colunas'.format(train.shape[0], train.shape[1]))\n",
    "\n",
    "    print('Carregando arquivo dataset_teste.csv....')\n",
    "    test = pd.read_csv('../dataset/dataset_teste.csv')\n",
    "    print('dataset_teste.csv tem {} linhas and {} colunas'.format(test.shape[0], test.shape[1]))\n",
    "\n",
    "    return train, test\n",
    "\n",
    "# Leitura dos dados\n",
    "train, test = read_data()\n",
    "\n",
    "df = train.append(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>v1</th>\n",
       "      <th>v10</th>\n",
       "      <th>v100</th>\n",
       "      <th>v101</th>\n",
       "      <th>v102</th>\n",
       "      <th>v103</th>\n",
       "      <th>v104</th>\n",
       "      <th>v105</th>\n",
       "      <th>v106</th>\n",
       "      <th>v107</th>\n",
       "      <th>v108</th>\n",
       "      <th>v109</th>\n",
       "      <th>v11</th>\n",
       "      <th>v110</th>\n",
       "      <th>v111</th>\n",
       "      <th>v112</th>\n",
       "      <th>v113</th>\n",
       "      <th>v114</th>\n",
       "      <th>v115</th>\n",
       "      <th>v116</th>\n",
       "      <th>v117</th>\n",
       "      <th>v118</th>\n",
       "      <th>v119</th>\n",
       "      <th>v12</th>\n",
       "      <th>v120</th>\n",
       "      <th>v121</th>\n",
       "      <th>v122</th>\n",
       "      <th>v123</th>\n",
       "      <th>v124</th>\n",
       "      <th>v125</th>\n",
       "      <th>v126</th>\n",
       "      <th>v127</th>\n",
       "      <th>v128</th>\n",
       "      <th>v129</th>\n",
       "      <th>v13</th>\n",
       "      <th>v130</th>\n",
       "      <th>v131</th>\n",
       "      <th>v14</th>\n",
       "      <th>v15</th>\n",
       "      <th>v16</th>\n",
       "      <th>v17</th>\n",
       "      <th>v18</th>\n",
       "      <th>v19</th>\n",
       "      <th>v2</th>\n",
       "      <th>v20</th>\n",
       "      <th>v21</th>\n",
       "      <th>v22</th>\n",
       "      <th>v23</th>\n",
       "      <th>v24</th>\n",
       "      <th>v25</th>\n",
       "      <th>v26</th>\n",
       "      <th>v27</th>\n",
       "      <th>v28</th>\n",
       "      <th>v29</th>\n",
       "      <th>v3</th>\n",
       "      <th>v30</th>\n",
       "      <th>v31</th>\n",
       "      <th>v32</th>\n",
       "      <th>v33</th>\n",
       "      <th>v34</th>\n",
       "      <th>v35</th>\n",
       "      <th>v36</th>\n",
       "      <th>v37</th>\n",
       "      <th>v38</th>\n",
       "      <th>v39</th>\n",
       "      <th>v4</th>\n",
       "      <th>v40</th>\n",
       "      <th>v41</th>\n",
       "      <th>v42</th>\n",
       "      <th>v43</th>\n",
       "      <th>v44</th>\n",
       "      <th>v45</th>\n",
       "      <th>v46</th>\n",
       "      <th>v47</th>\n",
       "      <th>v48</th>\n",
       "      <th>v49</th>\n",
       "      <th>v5</th>\n",
       "      <th>v50</th>\n",
       "      <th>v51</th>\n",
       "      <th>v52</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>v55</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57</th>\n",
       "      <th>v58</th>\n",
       "      <th>v59</th>\n",
       "      <th>v6</th>\n",
       "      <th>v60</th>\n",
       "      <th>v61</th>\n",
       "      <th>v62</th>\n",
       "      <th>v63</th>\n",
       "      <th>v64</th>\n",
       "      <th>v65</th>\n",
       "      <th>v66</th>\n",
       "      <th>v67</th>\n",
       "      <th>v68</th>\n",
       "      <th>v69</th>\n",
       "      <th>v7</th>\n",
       "      <th>v70</th>\n",
       "      <th>v71</th>\n",
       "      <th>v72</th>\n",
       "      <th>v73</th>\n",
       "      <th>v74</th>\n",
       "      <th>v75</th>\n",
       "      <th>v76</th>\n",
       "      <th>v77</th>\n",
       "      <th>v78</th>\n",
       "      <th>v79</th>\n",
       "      <th>v8</th>\n",
       "      <th>v80</th>\n",
       "      <th>v81</th>\n",
       "      <th>v82</th>\n",
       "      <th>v83</th>\n",
       "      <th>v84</th>\n",
       "      <th>v85</th>\n",
       "      <th>v86</th>\n",
       "      <th>v87</th>\n",
       "      <th>v88</th>\n",
       "      <th>v89</th>\n",
       "      <th>v9</th>\n",
       "      <th>v90</th>\n",
       "      <th>v91</th>\n",
       "      <th>v92</th>\n",
       "      <th>v93</th>\n",
       "      <th>v94</th>\n",
       "      <th>v95</th>\n",
       "      <th>v96</th>\n",
       "      <th>v97</th>\n",
       "      <th>v98</th>\n",
       "      <th>v99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.335739</td>\n",
       "      <td>0.503281</td>\n",
       "      <td>19.470199</td>\n",
       "      <td>8.389237</td>\n",
       "      <td>2.757375</td>\n",
       "      <td>4.374296</td>\n",
       "      <td>1.574039</td>\n",
       "      <td>0.007294</td>\n",
       "      <td>12.579184</td>\n",
       "      <td>E</td>\n",
       "      <td>2.382692</td>\n",
       "      <td>3.930922</td>\n",
       "      <td>16.434108</td>\n",
       "      <td>B</td>\n",
       "      <td>0.433213</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.634907</td>\n",
       "      <td>2.857144</td>\n",
       "      <td>1.951220</td>\n",
       "      <td>6.592012</td>\n",
       "      <td>5.909091</td>\n",
       "      <td>-6.297423e-07</td>\n",
       "      <td>6.085711</td>\n",
       "      <td>1.059603</td>\n",
       "      <td>0.803572</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.989780</td>\n",
       "      <td>0.035754</td>\n",
       "      <td>AU</td>\n",
       "      <td>1.804126</td>\n",
       "      <td>3.113719</td>\n",
       "      <td>2.024285</td>\n",
       "      <td>0</td>\n",
       "      <td>2.866830</td>\n",
       "      <td>0.636365</td>\n",
       "      <td>2.857144</td>\n",
       "      <td>11.636387</td>\n",
       "      <td>1.355013</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>3.670350</td>\n",
       "      <td>0.106720</td>\n",
       "      <td>0.148883</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>18.869283</td>\n",
       "      <td>7.730923</td>\n",
       "      <td>XDX</td>\n",
       "      <td>-1.716131e-08</td>\n",
       "      <td>C</td>\n",
       "      <td>0.139412</td>\n",
       "      <td>1.720818</td>\n",
       "      <td>3.393503</td>\n",
       "      <td>0.590122</td>\n",
       "      <td>8.880867</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>1.083033</td>\n",
       "      <td>1.010829</td>\n",
       "      <td>7.270147</td>\n",
       "      <td>8.375452</td>\n",
       "      <td>11.326592</td>\n",
       "      <td>0.454546</td>\n",
       "      <td>0</td>\n",
       "      <td>4.012088</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.711453</td>\n",
       "      <td>7.653429</td>\n",
       "      <td>12.707581</td>\n",
       "      <td>2.015505</td>\n",
       "      <td>10.498338</td>\n",
       "      <td>9.848672</td>\n",
       "      <td>0.113561</td>\n",
       "      <td>C</td>\n",
       "      <td>12.171733</td>\n",
       "      <td>8.086643</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>0.899420</td>\n",
       "      <td>7.277792</td>\n",
       "      <td>G</td>\n",
       "      <td>16.747968</td>\n",
       "      <td>0.037096</td>\n",
       "      <td>1.299638</td>\n",
       "      <td>DI</td>\n",
       "      <td>3.971118</td>\n",
       "      <td>0.529802</td>\n",
       "      <td>10.890984</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>1.588448</td>\n",
       "      <td>15.858152</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153461</td>\n",
       "      <td>6.363189</td>\n",
       "      <td>18.303925</td>\n",
       "      <td>C</td>\n",
       "      <td>9.314079</td>\n",
       "      <td>15.231789</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>11.784549</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>1.614988</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>2.230940</td>\n",
       "      <td>7.292418</td>\n",
       "      <td>8.571429</td>\n",
       "      <td>E</td>\n",
       "      <td>0.012941</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.528326</td>\n",
       "      <td>8.861647</td>\n",
       "      <td>0.649820</td>\n",
       "      <td>1.299638</td>\n",
       "      <td>1.707317</td>\n",
       "      <td>0.866426</td>\n",
       "      <td>9.551836</td>\n",
       "      <td>3.321300</td>\n",
       "      <td>0.095678</td>\n",
       "      <td>9.999999</td>\n",
       "      <td>0.905342</td>\n",
       "      <td>A</td>\n",
       "      <td>0.442252</td>\n",
       "      <td>5.814018</td>\n",
       "      <td>3.517720</td>\n",
       "      <td>0.462019</td>\n",
       "      <td>7.436824</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>8.877414</td>\n",
       "      <td>1.191337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.312910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.505335</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>1.825361</td>\n",
       "      <td>4.247858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>G</td>\n",
       "      <td>10.308044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.595357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.507647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598896</td>\n",
       "      <td>AF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.957825</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.636386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.763110</td>\n",
       "      <td>GUV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>3.056144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.615077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.579479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.305766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.449959</td>\n",
       "      <td>E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.191265</td>\n",
       "      <td>1.379210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.129469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.544736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.053353</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "      <td>2.301630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.277655</td>\n",
       "      <td>3.430691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.848004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.678584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.303967</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.943877</td>\n",
       "      <td>0.765864</td>\n",
       "      <td>15.491329</td>\n",
       "      <td>5.879353</td>\n",
       "      <td>3.292788</td>\n",
       "      <td>5.924457</td>\n",
       "      <td>1.668401</td>\n",
       "      <td>0.008275</td>\n",
       "      <td>11.670572</td>\n",
       "      <td>C</td>\n",
       "      <td>1.375753</td>\n",
       "      <td>1.184211</td>\n",
       "      <td>14.756098</td>\n",
       "      <td>B</td>\n",
       "      <td>3.367348</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.205561</td>\n",
       "      <td>12.941177</td>\n",
       "      <td>3.129253</td>\n",
       "      <td>3.478911</td>\n",
       "      <td>6.233767</td>\n",
       "      <td>-2.792745e-07</td>\n",
       "      <td>6.384670</td>\n",
       "      <td>2.138728</td>\n",
       "      <td>2.238806</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>2.477596</td>\n",
       "      <td>0.013452</td>\n",
       "      <td>AE</td>\n",
       "      <td>1.773709</td>\n",
       "      <td>3.922193</td>\n",
       "      <td>1.120468</td>\n",
       "      <td>2</td>\n",
       "      <td>2.505589</td>\n",
       "      <td>0.883118</td>\n",
       "      <td>1.176472</td>\n",
       "      <td>9.603542</td>\n",
       "      <td>1.984127</td>\n",
       "      <td>5.882353</td>\n",
       "      <td>3.170847</td>\n",
       "      <td>0.244541</td>\n",
       "      <td>0.144258</td>\n",
       "      <td>5.310079</td>\n",
       "      <td>17.952332</td>\n",
       "      <td>5.245035</td>\n",
       "      <td>FQ</td>\n",
       "      <td>-2.785053e-07</td>\n",
       "      <td>E</td>\n",
       "      <td>0.113997</td>\n",
       "      <td>2.244897</td>\n",
       "      <td>5.306122</td>\n",
       "      <td>0.836005</td>\n",
       "      <td>7.499999</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>1.454082</td>\n",
       "      <td>1.734693</td>\n",
       "      <td>4.043864</td>\n",
       "      <td>7.959184</td>\n",
       "      <td>12.730517</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>0</td>\n",
       "      <td>7.378964</td>\n",
       "      <td>4.410969</td>\n",
       "      <td>13.077201</td>\n",
       "      <td>6.173469</td>\n",
       "      <td>12.346939</td>\n",
       "      <td>2.926830</td>\n",
       "      <td>8.897561</td>\n",
       "      <td>5.343819</td>\n",
       "      <td>0.126035</td>\n",
       "      <td>C</td>\n",
       "      <td>12.711328</td>\n",
       "      <td>6.836734</td>\n",
       "      <td>5.326159</td>\n",
       "      <td>0.604504</td>\n",
       "      <td>9.637627</td>\n",
       "      <td>F</td>\n",
       "      <td>15.102041</td>\n",
       "      <td>0.085573</td>\n",
       "      <td>0.765305</td>\n",
       "      <td>AS</td>\n",
       "      <td>4.030613</td>\n",
       "      <td>4.277456</td>\n",
       "      <td>9.105481</td>\n",
       "      <td>3.979592</td>\n",
       "      <td>2.151361</td>\n",
       "      <td>16.075602</td>\n",
       "      <td>1</td>\n",
       "      <td>0.123643</td>\n",
       "      <td>5.517949</td>\n",
       "      <td>16.377205</td>\n",
       "      <td>A</td>\n",
       "      <td>8.367347</td>\n",
       "      <td>11.040463</td>\n",
       "      <td>5.882353</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>8.460654</td>\n",
       "      <td>B</td>\n",
       "      <td>3</td>\n",
       "      <td>2.413618</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>1.963971</td>\n",
       "      <td>5.918368</td>\n",
       "      <td>11.764705</td>\n",
       "      <td>E</td>\n",
       "      <td>0.019645</td>\n",
       "      <td>3.333334</td>\n",
       "      <td>10.194433</td>\n",
       "      <td>8.266200</td>\n",
       "      <td>1.530611</td>\n",
       "      <td>1.530613</td>\n",
       "      <td>2.429906</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>8.447465</td>\n",
       "      <td>3.367346</td>\n",
       "      <td>0.111388</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>0.811447</td>\n",
       "      <td>G</td>\n",
       "      <td>0.271480</td>\n",
       "      <td>5.156559</td>\n",
       "      <td>4.214944</td>\n",
       "      <td>0.309657</td>\n",
       "      <td>5.663265</td>\n",
       "      <td>5.974026</td>\n",
       "      <td>11.588858</td>\n",
       "      <td>0.841837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.797415</td>\n",
       "      <td>6.542669</td>\n",
       "      <td>18.256352</td>\n",
       "      <td>8.507281</td>\n",
       "      <td>2.503055</td>\n",
       "      <td>4.872157</td>\n",
       "      <td>2.573664</td>\n",
       "      <td>0.113967</td>\n",
       "      <td>12.554274</td>\n",
       "      <td>B</td>\n",
       "      <td>2.230754</td>\n",
       "      <td>1.990131</td>\n",
       "      <td>16.347483</td>\n",
       "      <td>B</td>\n",
       "      <td>2.643678</td>\n",
       "      <td>J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.777666</td>\n",
       "      <td>10.574713</td>\n",
       "      <td>1.511063</td>\n",
       "      <td>4.949609</td>\n",
       "      <td>7.180722</td>\n",
       "      <td>5.655086e-01</td>\n",
       "      <td>9.646653</td>\n",
       "      <td>1.166281</td>\n",
       "      <td>1.956521</td>\n",
       "      <td>7.018256</td>\n",
       "      <td>1.812795</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>CJ</td>\n",
       "      <td>1.415230</td>\n",
       "      <td>2.954381</td>\n",
       "      <td>1.990847</td>\n",
       "      <td>1</td>\n",
       "      <td>3.903302</td>\n",
       "      <td>1.677108</td>\n",
       "      <td>1.034483</td>\n",
       "      <td>14.094723</td>\n",
       "      <td>1.945044</td>\n",
       "      <td>5.517242</td>\n",
       "      <td>3.610789</td>\n",
       "      <td>1.224114</td>\n",
       "      <td>0.231630</td>\n",
       "      <td>8.304757</td>\n",
       "      <td>18.376407</td>\n",
       "      <td>7.517125</td>\n",
       "      <td>ACUE</td>\n",
       "      <td>-4.805344e-07</td>\n",
       "      <td>D</td>\n",
       "      <td>0.148843</td>\n",
       "      <td>1.308269</td>\n",
       "      <td>2.303640</td>\n",
       "      <td>8.926662</td>\n",
       "      <td>8.874521</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>1.587644</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>8.703550</td>\n",
       "      <td>8.898468</td>\n",
       "      <td>11.302795</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0</td>\n",
       "      <td>0.287322</td>\n",
       "      <td>4.225930</td>\n",
       "      <td>11.523045</td>\n",
       "      <td>7.931035</td>\n",
       "      <td>12.935823</td>\n",
       "      <td>1.470878</td>\n",
       "      <td>12.708574</td>\n",
       "      <td>9.670823</td>\n",
       "      <td>0.108387</td>\n",
       "      <td>C</td>\n",
       "      <td>12.194855</td>\n",
       "      <td>8.591954</td>\n",
       "      <td>11.627438</td>\n",
       "      <td>3.329176</td>\n",
       "      <td>4.780357</td>\n",
       "      <td>H</td>\n",
       "      <td>16.621695</td>\n",
       "      <td>0.139721</td>\n",
       "      <td>1.178161</td>\n",
       "      <td>BW</td>\n",
       "      <td>3.965517</td>\n",
       "      <td>1.732102</td>\n",
       "      <td>11.777912</td>\n",
       "      <td>2.097700</td>\n",
       "      <td>1.229246</td>\n",
       "      <td>15.927390</td>\n",
       "      <td>1</td>\n",
       "      <td>0.140260</td>\n",
       "      <td>6.292979</td>\n",
       "      <td>17.011645</td>\n",
       "      <td>A</td>\n",
       "      <td>9.703065</td>\n",
       "      <td>18.568129</td>\n",
       "      <td>9.425288</td>\n",
       "      <td>1.987549</td>\n",
       "      <td>13.594728</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>2.272541</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>2.188198</td>\n",
       "      <td>8.213602</td>\n",
       "      <td>13.448277</td>\n",
       "      <td>B</td>\n",
       "      <td>0.171947</td>\n",
       "      <td>1.947261</td>\n",
       "      <td>4.797873</td>\n",
       "      <td>13.315819</td>\n",
       "      <td>1.681034</td>\n",
       "      <td>1.379310</td>\n",
       "      <td>1.587045</td>\n",
       "      <td>1.242817</td>\n",
       "      <td>10.747144</td>\n",
       "      <td>1.408046</td>\n",
       "      <td>0.039051</td>\n",
       "      <td>8.965516</td>\n",
       "      <td>1.042425</td>\n",
       "      <td>B</td>\n",
       "      <td>0.763925</td>\n",
       "      <td>5.498902</td>\n",
       "      <td>3.423944</td>\n",
       "      <td>0.832518</td>\n",
       "      <td>7.375480</td>\n",
       "      <td>6.746988</td>\n",
       "      <td>6.942002</td>\n",
       "      <td>1.334611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.050328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>14.097099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.320087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.991098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.414567</td>\n",
       "      <td>HIT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.083151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.138920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.364536</td>\n",
       "      <td>NaN</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  target        v1       v10       v100      v101      v102      v103  \\\n",
       "0   3     1.0  1.335739  0.503281  19.470199  8.389237  2.757375  4.374296   \n",
       "1   4     1.0       NaN  1.312910        NaN       NaN       NaN       NaN   \n",
       "2   5     1.0  0.943877  0.765864  15.491329  5.879353  3.292788  5.924457   \n",
       "3   6     1.0  0.797415  6.542669  18.256352  8.507281  2.503055  4.872157   \n",
       "4   8     1.0       NaN  1.050328        NaN       NaN       NaN       NaN   \n",
       "\n",
       "       v104      v105       v106 v107      v108      v109        v11 v110  \\\n",
       "0  1.574039  0.007294  12.579184    E  2.382692  3.930922  16.434108    B   \n",
       "1       NaN  1.505335        NaN    B  1.825361  4.247858        NaN    A   \n",
       "2  1.668401  0.008275  11.670572    C  1.375753  1.184211  14.756098    B   \n",
       "3  2.573664  0.113967  12.554274    B  2.230754  1.990131  16.347483    B   \n",
       "4       NaN       NaN        NaN    C       NaN       NaN        NaN    A   \n",
       "\n",
       "       v111 v112 v113       v114       v115      v116       v117      v118  \\\n",
       "0  0.433213    O  NaN  15.634907   2.857144  1.951220   6.592012  5.909091   \n",
       "1       NaN    U    G  10.308044        NaN       NaN  10.595357       NaN   \n",
       "2  3.367348    S  NaN  11.205561  12.941177  3.129253   3.478911  6.233767   \n",
       "3  2.643678    J  NaN  13.777666  10.574713  1.511063   4.949609  7.180722   \n",
       "4       NaN    T    G  14.097099        NaN       NaN        NaN       NaN   \n",
       "\n",
       "           v119       v12      v120      v121      v122      v123      v124  \\\n",
       "0 -6.297423e-07  6.085711  1.059603  0.803572  8.000000  1.989780  0.035754   \n",
       "1           NaN  6.507647       NaN       NaN       NaN       NaN  0.598896   \n",
       "2 -2.792745e-07  6.384670  2.138728  2.238806  9.333333  2.477596  0.013452   \n",
       "3  5.655086e-01  9.646653  1.166281  1.956521  7.018256  1.812795  0.002267   \n",
       "4           NaN  6.320087       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "  v125      v126      v127      v128  v129       v13      v130      v131  \\\n",
       "0   AU  1.804126  3.113719  2.024285     0  2.866830  0.636365  2.857144   \n",
       "1   AF       NaN       NaN  1.957825     0       NaN       NaN       NaN   \n",
       "2   AE  1.773709  3.922193  1.120468     2  2.505589  0.883118  1.176472   \n",
       "3   CJ  1.415230  2.954381  1.990847     1  3.903302  1.677108  1.034483   \n",
       "4    Z       NaN       NaN       NaN     0       NaN       NaN       NaN   \n",
       "\n",
       "         v14       v15       v16       v17       v18       v19        v2  \\\n",
       "0  11.636387  1.355013  8.571429  3.670350  0.106720  0.148883  8.727474   \n",
       "1  11.636386       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2   9.603542  1.984127  5.882353  3.170847  0.244541  0.144258  5.310079   \n",
       "3  14.094723  1.945044  5.517242  3.610789  1.224114  0.231630  8.304757   \n",
       "4  10.991098       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "         v20       v21   v22           v23 v24       v25       v26       v27  \\\n",
       "0  18.869283  7.730923   XDX -1.716131e-08   C  0.139412  1.720818  3.393503   \n",
       "1        NaN  6.763110   GUV           NaN   C  3.056144       NaN       NaN   \n",
       "2  17.952332  5.245035    FQ -2.785053e-07   E  0.113997  2.244897  5.306122   \n",
       "3  18.376407  7.517125  ACUE -4.805344e-07   D  0.148843  1.308269  2.303640   \n",
       "4        NaN  6.414567   HIT           NaN   E       NaN       NaN       NaN   \n",
       "\n",
       "        v28       v29 v3  v30 v31       v32       v33       v34       v35  \\\n",
       "0  0.590122  8.880867  C    C   A  1.083033  1.010829  7.270147  8.375452   \n",
       "1       NaN       NaN  C    C   A       NaN       NaN  3.615077       NaN   \n",
       "2  0.836005  7.499999  C  NaN   A  1.454082  1.734693  4.043864  7.959184   \n",
       "3  8.926662  8.874521  C    C   B  1.587644  1.666667  8.703550  8.898468   \n",
       "4       NaN       NaN  C  NaN   A       NaN       NaN  6.083151       NaN   \n",
       "\n",
       "         v36       v37  v38       v39        v4        v40       v41  \\\n",
       "0  11.326592  0.454546    0  4.012088  3.921026   7.711453  7.653429   \n",
       "1  14.579479       NaN    0       NaN       NaN  14.305766       NaN   \n",
       "2  12.730517  0.259740    0  7.378964  4.410969  13.077201  6.173469   \n",
       "3  11.302795  0.433735    0  0.287322  4.225930  11.523045  7.931035   \n",
       "4        NaN       NaN    0       NaN       NaN  10.138920       NaN   \n",
       "\n",
       "         v42       v43        v44       v45       v46 v47        v48  \\\n",
       "0  12.707581  2.015505  10.498338  9.848672  0.113561   C  12.171733   \n",
       "1        NaN       NaN        NaN       NaN  2.449959   E        NaN   \n",
       "2  12.346939  2.926830   8.897561  5.343819  0.126035   C  12.711328   \n",
       "3  12.935823  1.470878  12.708574  9.670823  0.108387   C  12.194855   \n",
       "4        NaN       NaN        NaN       NaN       NaN   I        NaN   \n",
       "\n",
       "        v49         v5       v50       v51 v52        v53       v54       v55  \\\n",
       "0  8.086643   7.915266  0.899420  7.277792   G  16.747968  0.037096  1.299638   \n",
       "1       NaN   9.191265  1.379210       NaN   G        NaN  1.129469       NaN   \n",
       "2  6.836734   5.326159  0.604504  9.637627   F  15.102041  0.085573  0.765305   \n",
       "3  8.591954  11.627438  3.329176  4.780357   H  16.621695  0.139721  1.178161   \n",
       "4       NaN        NaN  1.364536       NaN   H        NaN       NaN       NaN   \n",
       "\n",
       "   v56       v57       v58        v59        v6       v60        v61  v62  \\\n",
       "0   DI  3.971118  0.529802  10.890984  2.599278  1.588448  15.858152    1   \n",
       "1   DY       NaN       NaN        NaN       NaN       NaN        NaN    2   \n",
       "2   AS  4.030613  4.277456   9.105481  3.979592  2.151361  16.075602    1   \n",
       "3   BW  3.965517  1.732102  11.777912  2.097700  1.229246  15.927390    1   \n",
       "4  NaN       NaN       NaN        NaN       NaN       NaN        NaN    1   \n",
       "\n",
       "        v63       v64        v65 v66       v67        v68        v69  \\\n",
       "0  0.153461  6.363189  18.303925   C  9.314079  15.231789  17.142857   \n",
       "1  2.544736       NaN        NaN   A       NaN        NaN        NaN   \n",
       "2  0.123643  5.517949  16.377205   A  8.367347  11.040463   5.882353   \n",
       "3  0.140260  6.292979  17.011645   A  9.703065  18.568129   9.425288   \n",
       "4       NaN       NaN        NaN   C       NaN        NaN        NaN   \n",
       "\n",
       "         v7        v70 v71  v72       v73 v74 v75       v76       v77  \\\n",
       "0  3.176895  11.784549   F    1  1.614988   B   D  2.230940  7.292418   \n",
       "1       NaN  12.053353   F    2       NaN   B   D       NaN       NaN   \n",
       "2  3.928571   8.460654   B    3  2.413618   B   B  1.963971  5.918368   \n",
       "3  1.987549  13.594728   F    2  2.272541   B   D  2.188198  8.213602   \n",
       "4       NaN        NaN   F    1       NaN   B   D       NaN       NaN   \n",
       "\n",
       "         v78 v79        v8       v80        v81        v82       v83  \\\n",
       "0   8.571429   E  0.012941  3.000000   7.528326   8.861647  0.649820   \n",
       "1        NaN   D  2.301630       NaN   7.277655   3.430691       NaN   \n",
       "2  11.764705   E  0.019645  3.333334  10.194433   8.266200  1.530611   \n",
       "3  13.448277   B  0.171947  1.947261   4.797873  13.315819  1.681034   \n",
       "4        NaN   C       NaN       NaN        NaN        NaN       NaN   \n",
       "\n",
       "        v84       v85       v86        v87       v88       v89         v9  \\\n",
       "0  1.299638  1.707317  0.866426   9.551836  3.321300  0.095678   9.999999   \n",
       "1       NaN       NaN       NaN   9.848004       NaN  2.678584        NaN   \n",
       "2  1.530613  2.429906  1.071429   8.447465  3.367346  0.111388  12.666667   \n",
       "3  1.379310  1.587045  1.242817  10.747144  1.408046  0.039051   8.965516   \n",
       "4       NaN       NaN       NaN        NaN       NaN       NaN        NaN   \n",
       "\n",
       "        v90 v91       v92       v93       v94       v95       v96       v97  \\\n",
       "0  0.905342   A  0.442252  5.814018  3.517720  0.462019  7.436824  5.454545   \n",
       "1       NaN   B       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2  0.811447   G  0.271480  5.156559  4.214944  0.309657  5.663265  5.974026   \n",
       "3  1.042425   B  0.763925  5.498902  3.423944  0.832518  7.375480  6.746988   \n",
       "4       NaN   G       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "         v98       v99  \n",
       "0   8.877414  1.191337  \n",
       "1   8.303967       NaN  \n",
       "2  11.588858  0.841837  \n",
       "3   6.942002  1.334611  \n",
       "4        NaN       NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Numerical features:  114\n",
      "Number of Categorical features:  19\n"
     ]
    }
   ],
   "source": [
    "# Verificar a quantidade de features numericas e categoricas\n",
    "\n",
    "numerical_feats = df.dtypes[df.dtypes != \"object\"]\n",
    "print(\"Number of Numerical features: \", len(numerical_feats))\n",
    "\n",
    "categorical_feats = df.dtypes[df.dtypes == \"object\"].index\n",
    "print(\"Number of Categorical features: \", len(categorical_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando as features categorias com LabelEncoder\n",
    "le = LabelEncoder()\n",
    "for i, col in enumerate(df):\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = le.fit_transform(np.array(df[col].astype(str)).reshape((-1,)))\n",
    "        \n",
    "        \n",
    "# Realizando tratamento de missing value\n",
    "for c in df.columns:\n",
    "    if c != 'ID' and c != 'target':\n",
    "        df[c].fillna(df[c].mean(),inplace=True)\n",
    "        \n",
    "scaler = StandardScaler()\n",
    "for c in df.columns:\n",
    "    col_type = df[c].dtype\n",
    "    #if col_type == 'float64' and c != 'target':\n",
    "    if c != 'ID' and c != 'target':\n",
    "        df[c] = scaler.fit_transform(df[c].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# Ajustando o skewness de algumas features com logaritmo\n",
    "df[\"v19\"]  = np.log1p(df[\"v19\"])\n",
    "df[\"v105\"] = np.log1p(df[\"v105\"])\n",
    "df[\"v119\"] = np.log1p(df[\"v119\"])\n",
    "df[\"v124\"] = np.log1p(df[\"v124\"])\n",
    "df[\"v23\"]  = np.log1p(df[\"v23\"])\n",
    "df[\"v39\"]  = np.log1p(df[\"v39\"])\n",
    "df[\"v68\"]  = np.log1p(df[\"v68\"])\n",
    "\n",
    "# Transformando as features categorias com LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "for i, col in enumerate(df):\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = le.fit_transform(np.array(df[col].astype(str)).reshape((-1,)))\n",
    "        \n",
    "scaler = StandardScaler()\n",
    "for c in df.columns:\n",
    "    col_type = df[c].dtype\n",
    "    #if col_type == 'float64' and c != 'target':\n",
    "    if c != 'ID' and c != 'target':\n",
    "        df[c] = scaler.fit_transform(df[c].values.reshape(-1, 1))\n",
    "        \n",
    "# Realizando tratamento de missing value\n",
    "for c in df.columns:\n",
    "    if c != 'ID' and c != 'target':\n",
    "        df[c].fillna(df[c].mean(),inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Add decomposed components: PCA / ICA etc.\n",
    "n_comp = 12\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "tsvd_results_df = tsvd.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=42)\n",
    "pca2_results_df = pca.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=42)\n",
    "ica2_results_df = ica.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=42)\n",
    "grp_results_df = grp.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n",
    "\n",
    "# SRP\n",
    "srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=42)\n",
    "srp_results_df = srp.fit_transform(df.drop(columns = ['ID','target'], axis = 1))\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp+1):\n",
    "    df['pca_' + str(i)]  = pca2_results_df[:,i-1]\n",
    "    df['ica_' + str(i)]  = ica2_results_df[:,i-1]\n",
    "    df['tsvd_' + str(i)] = tsvd_results_df[:,i-1]\n",
    "    df['grp_' + str(i)]  = grp_results_df[:,i-1]\n",
    "    df['srp_' + str(i)]  = srp_results_df[:,i-1] \n",
    "    \n",
    "\n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree = 3)\n",
    "poly_features = df[['v62', 'v119', 'v123', 'v48', 'v85']]\n",
    "\n",
    "# imputer for handling missing values\n",
    "imputer = SimpleImputer(strategy = 'mean')\n",
    "\n",
    "# Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "\n",
    "# Train the polynomial features\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "\n",
    "print('Polynomial Features shape: ', poly_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "# Feature Extraction\n",
    "The next step is to extract features from the signals in order to develop a series of classifiers to classify the remaining data. We've looked at some features above (RMS, PSD), but there are a wealth of other features that I can pull out that are likely meaningful. The package TSFRESH was developed to automate feature extraction and selection from time series data. Their work is published in [this paper linked here](https://www.sciencedirect.com/science/article/pii/S0925231218304843?via%3Dihub), and the packgage documentation is publicly available [here](https://tsfresh.readthedocs.io/en/latest/).\n",
    "\n",
    "I will only be extracting features from the motion data, since its the only data that should have any predictive power. After all, the position of the robot shouldn't predict what surface it's on (it's not the case that robots only walk east on concrete, though that would make for an interesting story). The position data was exploited above since I knew that the training data was split from a series of longer runs (original groups) and suspected that the same was true for the test data. If I have time, I will analyze how well the model performs in the absence of knowledge from stitching the data. It makes sense to keep the data stitched, however, since it is reasonable to assume that the robot walks on the same surface for at least an extended period of time.\n",
    "\n",
    "In any case, I'll extract the relevant features below. Open up the collapsed code to see which features I chose to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_from = df[df.columns[2:10]]\n",
    "extract_from.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = tsfresh.extract_features(extract_from,\n",
    "                                              column_id='v10',\n",
    "                                              column_sort='v1',\n",
    "                                              n_jobs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "efc = extracted_features.copy()\n",
    "tsfresh.utilities.dataframe_functions.impute(efc)\n",
    "y = all_stitched.groupby('series_id').max()['surface']\n",
    "matched_y = y[y!='no_surface']\n",
    "matched_ef = efc.iloc[matched_y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_filtered = tsfresh.select_features(matched_ef, matched_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = features_filtered.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "# Classifier Exploration\n",
    "The purpose of this section is to develop a set of classifiers that I can leverage for a hybrid classifier. It's only worthwhile to keep classifiers that are relatively accurate, but I'll have to explore a series of them in order to choose the best ones. I'll make a train/test (validate) set first in order to assess the classifiers. Then I'll dive headlong into the different kinds of classifiers that are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will intentionally use the unclassified stitched data below to not introduce any extraneous noise into the classifiers. Once the classifiers are trained I can toss them the data that I have yet to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "metadata = all_stitched.groupby('series_id').max()[['surface','test','new_group_id', 'orig_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "filtered_extracted = metadata.join(extracted_features[feature_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "train_filtered = filtered_extracted[filtered_extracted.test == 0]\n",
    "test_filtered = filtered_extracted[filtered_extracted.test == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered.to_csv('data/train_filtered_v2_v7.csv',index=False)\n",
    "test_filtered.to_csv('data/test_filtered_v2_v7.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling missing NAs and infinite data ∞ by zeroes 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filtered.fillna(0,inplace=True)\n",
    "test_filtered.fillna(0,inplace=True)\n",
    "train_filtered.replace(-np.inf,0,inplace=True)\n",
    "train_filtered.replace(np.inf,0,inplace=True)\n",
    "test_filtered.replace(-np.inf,0,inplace=True)\n",
    "test_filtered.replace(np.inf,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "treino = df[df['target'].notnull()]\n",
    "teste = df[df['target'].isnull()]\n",
    "\n",
    "X_filtered = treino.drop(labels = ['ID','target'], axis = 1)\n",
    "y_filtered = treino.target\n",
    "\n",
    "X_f_train, X_f_test, y_f_train, y_f_test = train_test_split(X_filtered, y_filtered, test_size=0.1, random_state=42, stratify = y_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102888, 131)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_f_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll also scale the data since this is helpful for some classifiers. I will have to keep track of whether or not the data that I'm using is scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier I'll test is the Random Forest Classifier. **I'll use the filtered extracted dataset without scaling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.21      0.31      2730\n",
      "         1.0       0.80      0.96      0.87      8703\n",
      "\n",
      "    accuracy                           0.78     11433\n",
      "   macro avg       0.72      0.59      0.59     11433\n",
      "weighted avg       0.76      0.78      0.74     11433\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RFC = RandomForestClassifier(n_estimators=1000, random_state = 42, n_jobs=-1)\n",
    "RFC.fit(X_f_train, y_f_train)\n",
    "y_f_test_RFC = RFC.predict(X_f_test)\n",
    "print(classification_report(y_f_test, y_f_test_RFC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll check Naive Bayes. **I'll use the filtered and scaled data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.75      0.43      2730\n",
      "         1.0       0.86      0.47      0.61      8703\n",
      "\n",
      "    accuracy                           0.54     11433\n",
      "   macro avg       0.58      0.61      0.52     11433\n",
      "weighted avg       0.72      0.54      0.56     11433\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB = GaussianNB()\n",
    "NB.fit(X_f_train, y_f_train)\n",
    "y_test_NB = NB.predict(X_f_test)\n",
    "print(classification_report(y_f_test, y_test_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How interesting. This one does better at predicting hard_tiles at the expense of predicting carpet. The scores aren't awful, but they're certainly not great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform linear SVC using a grid search. This grid search is small, so it runs fairly quickly. **I will use the filtered and scaled dataset for this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "C_range = np.logspace(-4, 0, 4)\n",
    "param_grid = dict(C=C_range)\n",
    "SVM = GridSearchCV(SVC(kernel = 'linear'), \n",
    "                    param_grid=param_grid, \n",
    "                    n_jobs=-1, \n",
    "                    pre_dispatch=2, \n",
    "                    cv = 5)\n",
    "SVM.fit(X_f_train, y_f_train)\n",
    "y_test_svc = SVM.predict(X_f_test)\n",
    "print(classification_report(y_f_test, y_test_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform KNN **using the filtered and scaled data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.19      0.26      2730\n",
      "         1.0       0.78      0.90      0.84      8703\n",
      "\n",
      "    accuracy                           0.73     11433\n",
      "   macro avg       0.58      0.55      0.55     11433\n",
      "weighted avg       0.69      0.73      0.70     11433\n",
      "\n"
     ]
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(10)\n",
    "KNN.fit(X_f_train,y_f_train)\n",
    "y_test_knn = KNN.predict(X_f_test)\n",
    "print(classification_report(y_f_test, y_test_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check a regular decision tree **using the filtered, scaled data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.38      0.37      2730\n",
      "         1.0       0.80      0.78      0.79      8703\n",
      "\n",
      "    accuracy                           0.69     11433\n",
      "   macro avg       0.58      0.58      0.58     11433\n",
      "weighted avg       0.69      0.69      0.69     11433\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X_f_train,y_f_train)\n",
    "y_test_dt = DT.predict(X_f_test)\n",
    "print(classification_report(y_f_test, y_test_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, perform XGBoost **on the filtered and scaled data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3,4,5,6],  # the maximum depth of each tree\n",
    "    'min_child_weight':np.linspace(0.8,1.2,4),\n",
    "    'gamma': np.linspace(0,0.2,4),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test_svc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6c71f42d233c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mXGB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_f_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_f_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0my_test_XGB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_f_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_f_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_svc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test_svc' is not defined"
     ]
    }
   ],
   "source": [
    "# The below parameters were found using GridSearchCV, using the parameter search basis above.\n",
    "XGB = xgb.sklearn.XGBClassifier(learning_rate = 0.02,\n",
    "                                objective = 'binary:logistic',\n",
    "                                n_estimators = 200,\n",
    "                                max_depth = 5,\n",
    "                                min_child_weight = 1.2,\n",
    "                                subsample=0.8,\n",
    "                                colsample_bytree = 0.8,\n",
    "                                gamma = 0.066,\n",
    "                                n_jobs = 4,\n",
    "                                nthreads = 1,\n",
    "                                silent = True,\n",
    "                                seed = 42)\n",
    "XGB.fit(X_f_train,y_f_train)\n",
    "y_test_XGB = XGB.predict(X_f_test)\n",
    "print(classification_report(y_f_test, y_test_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "# Meta Ensemble Classifier\n",
    "The real utility in stitching groups together is the ability to classify the entire group as one surface based on the results of classifying its constituent series. The easiest way to think of this is that the series vote on the classification of the group. The surface with the most \"votes\" is used to classify the entire group, similar to how RFC methods work. Furthermore, the results of each classifier above can be used in order to determine the votes for a group! Since the voting occurs as a sum of the number of instances that each surface appears in a group, each classifier can be assigned a relative weight in the voting schema. I'll employ this method to classify the remaining data.\n",
    "\n",
    "First I'll make a function that generates and joins predictions from the classifier instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(filtered_data, classifiers, scaler):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    df = filtered_data[['surface','test','new_group_id', 'orig_id']]\n",
    "    X = filtered_data.drop(labels = ['surface','test','new_group_id','orig_id'], \n",
    "                                 axis = 1).values\n",
    "    X_scaled = scaler.transform(X)\n",
    "    for name, classifier_dict in classifiers.items():\n",
    "        clf = classifier_dict['classifier']\n",
    "        if classifier_dict['scale']:\n",
    "            prediction = clf.predict(X_scaled)\n",
    "        else:\n",
    "            prediction = clf.predict(X)\n",
    "        df[name] = prediction\n",
    "    warnings.resetwarnings()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {'RFC':{'classifier':RFC,'scale': False},\n",
    "               'NB':{'classifier':NB,'scale':True},\n",
    "               'SVM':{'classifier':SVM,'scale':True},\n",
    "               'KNN':{'classifier':KNN,'scale':True},\n",
    "               'DT':{'classifier':DT,'scale':True},\n",
    "               'XGB':{'classifier':XGB,'scale':True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_predictions = get_predictions(filtered_data=train_filtered,\n",
    "                              classifiers=classifiers, \n",
    "                              scaler=scaler_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier-specific predictions can be used in an ensemble classifier function that is written below. The function accepts the predictions and a dictionary that includes the classifiers to use and their weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_ensemble(predictions, classifier_weights, groupby = 'new_group_id'):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    df = predictions[['test','new_group_id','orig_id']]\n",
    "    df['surface'] = 'None'\n",
    "    g = predictions.groupby(groupby)\n",
    "    for name, group in g:\n",
    "        index = group.index\n",
    "        weighted_value_counts = []\n",
    "        for classifier,weight in classifier_weights.items():\n",
    "            value_counts = group[classifier].value_counts()\n",
    "            weighted_value_counts.append(value_counts*weight)\n",
    "        all_counts = pd.DataFrame(weighted_value_counts).fillna(value=0)\n",
    "        surface = all_counts.sum().sort_values(ascending=False).index[0]\n",
    "        df.loc[index,'surface'] = surface\n",
    "    warnings.resetwarnings()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_weights = {classifier:1 for classifier in classifiers.keys()}\n",
    "del classifier_weights['NB'] # Naive Bayes classification wasn't great, so I won't include it\n",
    "ensemble_prediction = classifier_ensemble(predictions=multi_predictions,\n",
    "                                 classifier_weights=classifier_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_filtered.surface, ensemble_prediction.surface))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll have to perform my own parameter search on the classifier ensemble in order to find the weights that produce the best results. Using a grid approach might not be the best course of action since there are 5 parameters to set. Instead, I'll use an evolutionary algorithm with a maximum number of generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CEParamSearch(y_true, multi_predictions, classifier_names, \n",
    "                  groupby = 'new_group_id', generations = 20, \n",
    "                  seed = 1, mutation_rate = 0.35):  \n",
    "    # Generate random weights first\n",
    "    np.random.seed(seed)\n",
    "    weights = np.random.rand(len(classifiers))\n",
    "    classifier_weights = {classifier:weight for classifier,weight in zip(classifier_names,weights)}\n",
    "    \n",
    "    # Classify the data and calculate a score with the weights\n",
    "    ensemble_prediction = classifier_ensemble(multi_predictions,classifier_weights,groupby=groupby)\n",
    "    y_pred = ensemble_prediction.surface\n",
    "    score = sum(y_pred == y_true)/len(y_pred)\n",
    "    \n",
    "    # Keep track of score and time\n",
    "    scores = [score]\n",
    "    start = time.time()\n",
    "    \n",
    "    \n",
    "    # Hunt for a better solution\n",
    "    gene = random.choice(classifier_names)\n",
    "    orig_mut_rate = mutation_rate\n",
    "    for generation in range(1,generations+1):\n",
    "        # Make new weights using small mutation\n",
    "        new_weights = classifier_weights\n",
    "        mutation = (np.random.rand(1)-0.5)*mutation_rate\n",
    "        \n",
    "        new_weights[gene] += mutation\n",
    "        #Constrain weights to [0,1]\n",
    "        if new_weights[gene] < 0:\n",
    "            new_weights[gene] = 0\n",
    "        if new_weights[gene] > 1:\n",
    "            new_weights[gene] = 1\n",
    "            \n",
    "        # Classify data and obtain new score\n",
    "        ensemble_prediction = classifier_ensemble(multi_predictions,new_weights,groupby=groupby)\n",
    "        y_pred = ensemble_prediction.surface\n",
    "        new_score = sum(y_pred == y_true)/len(y_pred)\n",
    "        change = new_score - score\n",
    "        # Select fittest weights\n",
    "        if change < 0: # If new score is worse change the gene, reestablish mutation rate\n",
    "            scores.append(score)\n",
    "            gene = random.choice(classifier_names)\n",
    "            mutation_rate = orig_mut_rate\n",
    "        elif change == 0: # Mutate, but don't change gene or the mutation rate\n",
    "            scores.append(score)\n",
    "            classifier_weights = new_weights\n",
    "        elif change > 0: # Mutate and change mutation rate, but don't change the gene\n",
    "            mutation_rate /= 0.75\n",
    "            score = new_score\n",
    "            scores.append(score)\n",
    "            classifier_weights = new_weights\n",
    "        \n",
    "        # Update User\n",
    "        progress = generation/generations\n",
    "        time_elapsed = (time.time()-start)\n",
    "        ETA = (time_elapsed/progress-time_elapsed)/60\n",
    "        print('Score:{0:1.5f}\\t|\\tChange:{1:1.5f}\\t|\\tTime Remaining: {2:1.2f} min'.format(score,change,ETA), end = '\\r')\n",
    "    \n",
    "    plt.plot(scores)\n",
    "    return classifier_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier_names = list(classifier_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classifier_weights = CEParamSearch(train_filtered.surface,\n",
    "                                   multi_predictions,\n",
    "                                   classifier_names, \n",
    "                                   generations = 25,\n",
    "                                   mutation_rate = 0.5,\n",
    "                                   seed = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of CEParamSearch are random, though they do tend towards optimization as you can see in the graph above. The classifier weights below were used for a late submission whose public and private score I reveal below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier_weights = {'RFC': 1,\n",
    " 'SVM': 1,\n",
    " 'KNN': 0,\n",
    " 'DT': 0.22006725,\n",
    " 'XGB': 0.38377276}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have an optimized set of classifier weights I will run the unclassified dataset through the ensemble classifier to check its performance against Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_predictions_test = get_predictions(test_filtered,classifiers,scaler_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_prediction_test = classifier_ensemble(predictions=multi_predictions_test,\n",
    "                                 classifier_weights=classifier_weights)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output = ensemble_prediction_test[['orig_id','surface']].reset_index().drop('series_id', axis =1)\n",
    "output.to_csv(path_or_buf = 'submissions/submission_ensemble_classified_v7.csv', \n",
    "              header = ['series_id','surface'], \n",
    "              index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A private score of 0.8280 and a public score of 0.4656 is good given that I'm not leveraging all of the position data. The public score mostly benefits from the position matched data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "# Hybrid Classification\n",
    "It's unfair to say that the previous ensemble classification was not a hybrid classification already. Hybrid classifiers are those that classify over multiple iterations, rather than classifying by comparing the output of different processes. The above ensemble classification was built on the existing classification that was done to select features, and the classification to find groups. This following hybrid classification leverages all of that information to its extreme. I will accept the classification that was applied after finding groups, use the classified features to obtain outputs from individual classifiers, classify the remaining unclassified data using the ensemble classifier I generated, and finally stitch the position matched classification with the ensemble matched classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = train_filtered.groupby('series_id').max()[['surface','test','new_group_id', 'orig_id']]\n",
    "filtered_extracted_matched = metadata.join(extracted_features[feature_columns])\n",
    "unmatched_filtered = filtered_extracted_matched[filtered_extracted_matched.surface == 'no_surface']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_filtered.fillna(0,inplace=True)\n",
    "unmatched_filtered.replace(-np.inf,0,inplace=True)\n",
    "unmatched_filtered.replace(-np.inf,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_predictions_unmatched = get_predictions(unmatched_filtered, classifiers, scaler_f)\n",
    "ensemble_prediction_unmatched = classifier_ensemble(multi_predictions_unmatched, classifier_weights)\n",
    "unmatched_output = ensemble_prediction_unmatched[['orig_id','surface']].reset_index().drop('series_id', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_output = metadata[(metadata.test == 1)&(metadata.surface != 'no_surface')][['orig_id','surface']].reset_index().drop('series_id', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([matched_output,unmatched_output])\n",
    "output.to_csv('submissions/submission_hybrid_classified_v7.csv', \n",
    "              header = ['series_id','surface'], \n",
    "              index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the Kaggle submission are wonderful! **A private score of 0.8777 and a public score of 0.9876 yields a combined accuracy of 0.9272 on the whole dataset.** We already know that ~4% of the data was misclassified after the groups were stitched, so only about ~3% of the remaining unclassified data was misclassified by the meta ensemble classifier. Overall, I'm thrilled with this score! To improve these scores I could tune all of the individual classifiers, search for more features to extract, and improve and the grouping mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hybrid classifier that I developed was clearly better than using classical or ensemble classification schemes by themselves. Even a meta-ensemble of the standard classification schemes significantly outperforms a single classification scheme, which is helpful knowledge for future classification problems. Building this meta-classifier is simple enough, and tuning its performance even for a large number of parameters can be done using an evolutionary approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
